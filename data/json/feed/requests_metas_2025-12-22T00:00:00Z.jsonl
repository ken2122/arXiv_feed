{"custom_id": "2512.16965v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: AutoDFBench 1.0: A Benchmarking Framework for Digital Forensic Tool Testing and Generated Code Evaluation\nsummary: The National Institute of Standards and Technology (NIST) Computer Forensic Tool Testing (CFTT) programme has become the de facto standard for providing digital forensic tool testing and validation. However to date, no comprehensive framework exists to automate benchmarking across the diverse forensic tasks included in the programme. This gap results in inconsistent validation, challenges in comparing tools, and limited validation reproducibility. This paper introduces AutoDFBench 1.0, a modular benchmarking framework that supports the evaluation of both conventional DF tools and scripts, as well as AI-generated code and agentic approaches. The framework integrates five areas defined by the CFTT programme: string search, deleted file recovery, file carving, Windows registry recovery, and SQLite data recovery. AutoDFBench 1.0 includes ground truth data comprising of 63 test cases and 10,968 unique test scenarios, and execute evaluations through a RESTful API that produces structured JSON outputs with standardised metrics, including precision, recall, and F1~score for each test case, and the average of these F1~scores becomes the AutoDFBench Score. The benchmarking framework is validated against CFTT's datasets. The framework enables fair and reproducible comparison across tools and forensic scripts, establishing the first unified, automated, and extensible benchmarking framework for digital forensic tool testing and validation. AutoDFBench 1.0 supports tool vendors, researchers, practitioners, and standardisation bodies by facilitating transparent, reproducible, and comparable assessments of DF technologies.\nlink: https://arxiv.org/abs/2512.16965v1\n"}}
{"custom_id": "2512.16962v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval\nsummary: Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.\nlink: https://arxiv.org/abs/2512.16962v1\n"}}
{"custom_id": "2512.16146v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Analysis of Design Patterns and Benchmark Practices in Apache Kafka Event-Streaming Systems\nsummary: Apache Kafka has become a foundational platform for high throughput event streaming, enabling real time analytics, financial transaction processing, industrial telemetry, and large scale data driven systems. Despite its maturity and widespread adoption, consolidated research on reusable architectural design patterns and reproducible benchmarking methodologies remains fragmented across academic and industrial publications. This paper presents a structured synthesis of forty two peer reviewed studies published between 2015 and 2025, identifying nine recurring Kafka design patterns including log compaction, CQRS bus, exactly once pipelines, change data capture, stream table joins, saga orchestration, tiered storage, multi tenant topics, and event sourcing replay. The analysis examines co usage trends, domain specific deployments, and empirical benchmarking practices using standard suites such as TPCx Kafka and the Yahoo Streaming Benchmark, as well as custom workloads. The study highlights significant inconsistencies in configuration disclosure, evaluation rigor, and reproducibility that limit cross study comparison and practical replication. By providing a unified taxonomy, pattern benchmark matrix, and actionable decision heuristics, this work offers practical guidance for architects and researchers designing reproducible, high performance, and fault tolerant Kafka based event streaming systems.\nlink: https://arxiv.org/abs/2512.16146v1\n"}}
{"custom_id": "2512.16957v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: CAPIO: Safe Kernel-Bypass of Commodity Devices using Capabilities\nsummary: Securing low-latency I/O in commodity systems forces a fundamental trade-off: rely on the kernel's high overhead mediated interface, or bypass it entirely, exposing sensitive hardware resources to userspace and creating new vulnerabilities. This dilemma stems from a hardware granularity mismatch: standard MMUs operate at page boundaries, making it impossible to selectively expose safe device registers without also exposing the sensitive control registers colocated on the same page. Existing solutions to driver isolation enforce an isolation model that cannot protect sub-page device resources.\n  This paper presents CAPIO, the first architecture to leverage hardware capabilities to enforce fine-grained access control on memory-mapped I/O. Unlike prior page-based protections, CAPIO utilizes unforgeable capabilities to create precise, sub-page \"slices\" of device memory. This mechanism enables the kernel to delegate latency-critical hardware access to userspace applications while strictly preventing interaction with co-located privileged registers.\n  We implement CAPIO based on CHERI on the ARM Morello platform and demonstrate a proof-of-concept safe-access driver for a commodity network card which was not originally designed for kernel bypass. We demonstrate that CAPIO achieves the latency improvements of kernel bypass while enforcing byte-level access control of privileged resources.\nlink: https://arxiv.org/abs/2512.16957v1\n"}}
{"custom_id": "2512.15834v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Optimizing Agentic Language Model Inference via Speculative Tool Calls\nsummary: Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new \"tool cache\" API endpoint to enable LM providers to easily adopt these optimizations.\nlink: https://arxiv.org/abs/2512.15834v1\n"}}
{"custom_id": "2512.15823v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Secure AI-Driven Super-Resolution for Real-Time Mixed Reality Applications\nsummary: Immersive formats such as 360\u00b0 and 6DoF point cloud videos require high bandwidth and low latency, posing challenges for real-time AR/VR streaming. This work focuses on reducing bandwidth consumption and encryption/decryption delay, two key contributors to overall latency. We design a system that downsamples point cloud content at the origin server and applies partial encryption. At the client, the content is decrypted and upscaled using an ML-based super-resolution model. Our evaluation demonstrates a nearly linear reduction in bandwidth/latency, and encryption/decryption overhead with lower downsampling resolutions, while the super-resolution model effectively reconstructs the original full-resolution point clouds with minimal error and modest inference time.\nlink: https://arxiv.org/abs/2512.15823v1\n"}}
{"custom_id": "2512.15818v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Unveiling the Attribute Misbinding Threat in Identity-Preserving Models\nsummary: Identity-preserving models have led to notable progress in generating personalized content. Unfortunately, such models also exacerbate risks when misused, for instance, by generating threatening content targeting specific individuals. This paper introduces the \\textbf{Attribute Misbinding Attack}, a novel method that poses a threat to identity-preserving models by inducing them to produce Not-Safe-For-Work (NSFW) content. The attack's core idea involves crafting benign-looking textual prompts to circumvent text-filter safeguards and leverage a key model vulnerability: flawed attribute binding that stems from its internal attention bias. This results in misattributing harmful descriptions to a target identity and generating NSFW outputs. To facilitate the study of this attack, we present the \\textbf{Misbinding Prompt} evaluation set, which examines the content generation risks of current state-of-the-art identity-preserving models across four risk dimensions: pornography, violence, discrimination, and illegality. Additionally, we introduce the \\textbf{Attribute Binding Safety Score (ABSS)}, a metric for concurrently assessing both content fidelity and safety compliance. Experimental results show that our Misbinding Prompt evaluation set achieves a \\textbf{5.28}\\% higher success rate in bypassing five leading text filters (including GPT-4o) compared to existing main-stream evaluation sets, while also demonstrating the highest proportion of NSFW content generation. The proposed ABSS metric enables a more comprehensive evaluation of identity-preserving models by concurrently assessing both content fidelity and safety compliance.\nlink: https://arxiv.org/abs/2512.15818v1\n"}}
{"custom_id": "2512.15815v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Implementing a Scalable, Redeployable and Multitiered Repository for FAIR and Secure Scientific Data Sharing: The BIG-MAP Archive\nsummary: Data sharing in large consortia, such as research collaborations or industry partnerships, requires addressing both organizational and technical challenges. A common platform is essential to promote collaboration, facilitate exchange of findings, and ensure secure access to sensitive data. Key technical challenges include creating a scalable architecture, a user-friendly interface, and robust security and access control. The BIG-MAP Archive is a cloud-based, disciplinary, private repository designed to address these challenges. Built on InvenioRDM, it leverages platform functionalities to meet consortium-specific needs, providing a tailored solution compared to general repositories. Access can be restricted to members of specific communities or open to the entire consortium, such as the BATTERY 2030+, a consortium accelerating advanced battery technologies. Uploaded data and metadata are controlled via fine grained permissions, allowing access to individual project members or the full initiative. The formalized upload process ensures data are formatted and ready for publication in open repositories when needed. This paper reviews the repository's key features, showing how the BIG-MAP Archive enables secure, controlled data sharing within large consortia. It ensures data confidentiality while supporting flexible, permissions-based access and can be easily redeployed for other consortia, including MaterialsCommons4.eu and RAISE (Resource for AI Science in Europe).\nlink: https://arxiv.org/abs/2512.15815v1\n"}}
{"custom_id": "2512.15803v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: An empirical analysis of zero-day vulnerabilities disclosed by the zero day initiative\nsummary: Zero-day vulnerabilities represent some of the most critical threats in cybersecurity, as they correspond to previously unknown flaws in software or hardware that are actively exploited before vendors can develop and deploy patches. During this exposure window, affected systems remain defenseless, making zero-day attacks particularly damaging and difficult to mitigate. This study analyzes the Zero Day Initiative (ZDI) vulnerability disclosures reported between January and April 2024, Cole [2025] comprising a total of 415 vulnerabilities. The dataset includes vulnerability identifiers, Common Vulnerability Scoring System (CVSS) v3.0 scores, publication dates, and short textual descriptions. The primary objectives of this work are to identify trends in zero-day vulnerability disclosures, examine severity distributions across vendors, and investigate which vulnerability characteristics are most indicative of high severity. In addition, this study explores predictive modeling approaches for severity classification, comparing classical machine learning techniques with deep learning models using both structured metadata and unstructured textual descriptions. The findings aim to support improved patch prioritization strategies, more effective vulnerability management, and enhanced organizational preparedness against emerging zero-day threats.\nlink: https://arxiv.org/abs/2512.15803v1\n"}}
{"custom_id": "2512.15799v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Cybercrime and Computer Forensics in Epoch of Artificial Intelligence in India\nsummary: The integration of generative Artificial Intelligence into the digital ecosystem necessitates a critical re-evaluation of Indian criminal jurisprudence regarding computational forensics integrity. While algorithmic efficiency enhances evidence extraction, a research gap exists regarding the Digital Personal Data Protection Act, 2023's compatibility with adversarial AI threats, specifically anti-forensics and deepfakes. This study scrutinizes the AI \"dual-use\" dilemma, functioning as both a cyber-threat vector and forensic automation mechanism, to delineate privacy boundaries in high-stakes investigations. Employing a doctrinal legal methodology, the research synthesizes statutory analysis of the DPDP Act with global ethical frameworks (IEEE, EU) to evaluate regulatory efficacy. Preliminary results indicate that while Machine Learning offers high accuracy in pattern recognition, it introduces vulnerabilities regarding data poisoning and algorithmic bias. Findings highlight a critical tension between the Act's data minimization principles and forensic data retention requirements. Furthermore, the paper identifies that existing legal definitions inadequately encompass AI-driven \"tool crimes\" and \"target crimes.\" Consequently, the research proposes a \"human-centric\" forensic model prioritizing explainable AI (XAI) to ensure evidence admissibility. These implications suggest that synchronizing Indian privacy statutes with international forensic standards is imperative to mitigate synthetic media risks, establishing a roadmap for future legislative amendments and technical standardization.\nlink: https://arxiv.org/abs/2512.15799v1\n"}}
{"custom_id": "2512.14778v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Cybersecurity skills in new graduates: a Philippine perspective\nsummary: This study investigates the key skills and competencies needed by new cybersecurity graduates in the Philippines for entry-level positions. Using a descriptive cross-sectional research design, it combines analysis of job listings from Philippine online platforms with surveys of students, teachers, and professionals. The aim is to identify required skills and areas needing improvement, highlighting the balance between technical skills and other competencies like ethical conduct, suggesting a shift away from traditional cybersecurity skills towards a more diverse skillset. Furthermore, the results revealed common agreement on the importance of communication, critical thinking, problem-solving, and adaptability skills, albeit with slight variations in their prioritization. It recommends that aspiring cybersecurity professionals develop an inclusive skill set encompassing technical knowledge, soft skills, and personal competencies, with a focus on adaptability, continuous learning, and ethics. Skills such as business acumen are considered less vital for entry-level roles, proposing a preparation strategy that aligns with the changing demands of the cybersecurity industry.\nlink: https://arxiv.org/abs/2512.14778v1\n"}}
{"custom_id": "2512.15794v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Data Protection and Corporate Reputation Management in the Digital Era\nsummary: This paper analyzes the relationship between cybersecurity management, data protection, and corporate reputation in the context of digital transformation. The study examines how organizations implement strategies and tools to mitigate cyber risks, comply with regulatory requirements, and maintain stakeholder trust. A quantitative research design was applied using an online diagnostic survey conducted among enterprises from various industries operating in Poland. The analysis covered formal cybersecurity strategies, technical and procedural safeguards, employee awareness, incident response practices, and the adoption of international standards such as ISO/IEC 27001 and ISO/IEC 27032. The findings indicate that most organizations have formalized cybersecurity frameworks, conduct regular audits, and invest in employee awareness programs. Despite this high level of preparedness, 75 percent of surveyed firms experienced cybersecurity incidents within the previous twelve months. The most frequently reported consequences were reputational damage and loss of customer trust, followed by operational disruptions and financial or regulatory impacts. The results show that cybersecurity is increasingly perceived as a strategic investment supporting long-term organizational stability rather than merely a compliance cost. The study highlights the importance of integrating cybersecurity governance with corporate communication and reputation management, emphasizing data protection as a key determinant of digital trust and organizational resilience.\nlink: https://arxiv.org/abs/2512.15794v1\n"}}
{"custom_id": "2512.14767v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Privacy-Preserving Feature Valuation in Vertical Federated Learning Using Shapley-CMI and PSI Permutation\nsummary: Federated Learning (FL) is an emerging machine learning paradigm that enables multiple parties to collaboratively train models without sharing raw data, ensuring data privacy. In Vertical FL (VFL), where each party holds different features for the same users, a key challenge is to evaluate the feature contribution of each party before any model is trained, particularly in the early stages when no model exists. To address this, the Shapley-CMI method was recently proposed as a model-free, information-theoretic approach to feature valuation using Conditional Mutual Information (CMI). However, its original formulation did not provide a practical implementation capable of computing the required permutations and intersections securely. This paper presents a novel privacy-preserving implementation of Shapley-CMI for VFL. Our system introduces a private set intersection (PSI) server that performs all necessary feature permutations and computes encrypted intersection sizes across discretized and encrypted ID groups, without the need for raw data exchange. Each party then uses these intersection results to compute Shapley-CMI values, computing the marginal utility of their features. Initial experiments confirm the correctness and privacy of the proposed system, demonstrating its viability for secure and efficient feature contribution estimation in VFL. This approach ensures data confidentiality, scales across multiple parties, and enables fair data valuation without requiring the sharing of raw data or training models.\nlink: https://arxiv.org/abs/2512.14767v1\n"}}
{"custom_id": "2512.15641v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: ComMark: Covert and Robust Black-Box Model Watermarking with Compressed Samples\nsummary: The rapid advancement of deep learning has turned models into highly valuable assets due to their reliance on massive data and costly training processes. However, these models are increasingly vulnerable to leakage and theft, highlighting the critical need for robust intellectual property protection. Model watermarking has emerged as an effective solution, with black-box watermarking gaining significant attention for its practicality and flexibility. Nonetheless, existing black-box methods often fail to better balance covertness (hiding the watermark to prevent detection and forgery) and robustness (ensuring the watermark resists removal)-two essential properties for real-world copyright verification. In this paper, we propose ComMark, a novel black-box model watermarking framework that leverages frequency-domain transformations to generate compressed, covert, and attack-resistant watermark samples by filtering out high-frequency information. To further enhance watermark robustness, our method incorporates simulated attack scenarios and a similarity loss during training. Comprehensive evaluations across diverse datasets and architectures demonstrate that ComMark achieves state-of-the-art performance in both covertness and robustness. Furthermore, we extend its applicability beyond image recognition to tasks including speech recognition, sentiment analysis, image generation, image captioning, and video recognition, underscoring its versatility and broad applicability.\nlink: https://arxiv.org/abs/2512.15641v1\n"}}
{"custom_id": "2512.15790v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Bilevel Optimization for Covert Memory Tampering in Heterogeneous Multi-Agent Architectures (XAMT)\nsummary: The increasing operational reliance on complex Multi-Agent Systems (MAS) across safety-critical domains necessitates rigorous adversarial robustness assessment. Modern MAS are inherently heterogeneous, integrating conventional Multi-Agent Reinforcement Learning (MARL) with emerging Large Language Model (LLM) agent architectures utilizing Retrieval-Augmented Generation (RAG). A critical shared vulnerability is reliance on centralized memory components: the shared Experience Replay (ER) buffer in MARL and the external Knowledge Base (K) in RAG agents. This paper proposes XAMT (Bilevel Optimization for Covert Memory Tampering in Heterogeneous Multi-Agent Architectures), a novel framework that formalizes attack generation as a bilevel optimization problem. The Upper Level minimizes perturbation magnitude (delta) to enforce covertness while maximizing system behavior divergence toward an adversary-defined target (Lower Level). We provide rigorous mathematical instantiations for CTDE MARL algorithms and RAG-based LLM agents, demonstrating that bilevel optimization uniquely crafts stealthy, minimal-perturbation poisons evading detection heuristics. Comprehensive experimental protocols utilize SMAC and SafeRAG benchmarks to quantify effectiveness at sub-percent poison rates (less than or equal to 1 percent in MARL, less than or equal to 0.1 percent in RAG). XAMT defines a new unified class of training-time threats essential for developing intrinsically secure MAS, with implications for trust, formal verification, and defensive strategies prioritizing intrinsic safety over perimeter-based detection.\nlink: https://arxiv.org/abs/2512.15790v1\n"}}
{"custom_id": "2512.14759v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Quantum Resource Analysis of Low-Round Keccak/SHA-3 Preimage Attack: From Classical 2^57.8 to Quantum 2^28.9 using Qiskit Modeling\nsummary: This paper presents a hardware-conscious analysis of the quantum acceleration of the classical 3-round Keccak-256 preimage attack using Grover's Algorithm. While the theoretical quantum speed-up from T_cl=2^{57.8} (classical) to T_qu = 2^{28.9} (quantum) is mathematically sound, the practical implementation overhead is so extreme that attacks remain wholly infeasible in both resource and runtime dimensions. Using Qiskit-based circuit synthesis, we derive that a 3-round Keccak quantum oracle requires: 9,600 Toffoli gates (with uncomputation for reversibility); 3,200 logical qubits (1,600 state + 1,600 auxiliary); 7.47 * 10^{13} total 2-qubit gates (full Grover search); 3.2 million physical qubits (with quantum error correction)PROHIBITIVE; 0.12 years (43 days) to 2,365+ years execution time, depending on machine assumptions. These barriers -- particularly the physical qubit requirements, circuit depth, and error accumulation -- render the quantum attack infeasible for any foreseeable quantum computer. Consequently, SHA-3 security is not threatened by quantum computers for preimage attacks. We emphasize the critical importance of hardware-aware complexity analysis in quantum cryptanalysis: the elegant asymptotic theory of Grover's Algorithm hides an engineering overhead so prohibitive that the quantum approach becomes infeasible from both resource and implementation perspectives.\nlink: https://arxiv.org/abs/2512.14759v1\n"}}
{"custom_id": "2512.13767v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Stability-Drift Early Warning for Cyber-Physical Systems Under Degradation Attacks\nsummary: Cyber-physical systems (CPS) such as unmanned aerial vehicles are vulnerable to slow degradation that develops without causing immediate or obvious failures. Small sensor biases or timing irregularities can accumulate over time, gradually reducing stability while standard monitoring mechanisms continue to report normal operation. Detecting this early phase of degradation remains a challenge, as most existing approaches focus on abrupt faults or visible trajectory deviations. This paper introduces an early warning method based on stability drift, which measures the divergence between predicted and observed state transitions over short horizons. By tracking the gradual growth of this divergence, the proposed approach identifies emerging instability before it becomes visible in the flight trajectory or estimator residuals. The method operates externally to the flight stack and relies only on standard telemetry, making it suitable for deployment without modifying autopilot firmware. The approach was evaluated on a PX4 x500 platform in a software in the loop environment under two realistic degradation scenarios, gradual IMU bias drift and timing irregularities in the control loop. In both cases, the stability drift metric provided a consistent early warning signal several seconds before visible instability appeared, while remaining stable during nominal and aggressive but non degraded flight. The results demonstrate that stability drift can serve as a practical indicator of early degradation in UAV control systems. By providing advance notice during a pre instability phase, the proposed method complements existing safety mechanisms and offers additional time for mitigation or safe mode transitions under slow and subtle attacks.\nlink: https://arxiv.org/abs/2512.13767v1\n"}}
{"custom_id": "2512.15782v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Auto-Tuning Safety Guardrails for Black-Box Large Language Models\nsummary: Large language models (LLMs) are increasingly deployed behind safety guardrails such as system prompts and content filters, especially in settings where product teams cannot modify model weights. In practice these guardrails are typically hand-tuned, brittle, and difficult to reproduce. This paper studies a simple but practical alternative: treat safety guardrail design itself as a hyperparameter optimization problem over a frozen base model. Concretely, I wrap Mistral-7B-Instruct with modular jailbreak and malware system prompts plus a ModernBERT-based harmfulness classifier, then evaluate candidate configurations on three public benchmarks covering malware generation, classic jailbreak prompts, and benign user queries. Each configuration is scored using malware and jailbreak attack success rate, benign harmful-response rate, and end-to-end latency. A 48-point grid search over prompt combinations and filter modes establishes a baseline. I then run a black-box Optuna study over the same space and show that it reliably rediscovers the best grid configurations while requiring an order of magnitude fewer evaluations and roughly 8x less wall-clock time. The results suggest that viewing safety guardrails as tunable hyperparameters is a feasible way to harden black-box LLM deployments under compute and time constraints.\nlink: https://arxiv.org/abs/2512.15782v1\n"}}
{"custom_id": "2512.15781v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Detecting Malicious Entra OAuth Apps with LLM-Based Permission Risk Scoring\nsummary: This project presents a unified detection framework that constructs a complete corpus of Microsoft Graph permissions, generates consistent LLM-based risk scores, and integrates them into a real-time detection engine to identify malicious OAuth consent activity.\nlink: https://arxiv.org/abs/2512.15781v1\n"}}
{"custom_id": "2512.15780v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact, and Governance Evidence\nsummary: We evaluate adversarial robustness in tabular machine learning models used in financial decision making. Using credit scoring and fraud detection data, we apply gradient based attacks and measure impacts on discrimination, calibration, and financial risk metrics. Results show notable performance degradation under small perturbations and partial recovery through adversarial training.\nlink: https://arxiv.org/abs/2512.15780v1\n"}}
{"custom_id": "2512.14753v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: CODE ACROSTIC: Robust Watermarking for Code Generation\nsummary: Watermarking large language models (LLMs) is vital for preventing their misuse, including the fabrication of fake news, plagiarism, and spam. It is especially important to watermark LLM-generated code, as it often contains intellectual property.However, we found that existing methods for watermarking LLM-generated code fail to address comment removal attack.In such cases, an attacker can simply remove the comments from the generated code without affecting its functionality, significantly reducing the effectiveness of current code-watermarking techniques.On the other hand, injecting a watermark into code is challenging because, as previous works have noted, most code represents a low-entropy scenario compared to natural language. Our approach to addressing this issue involves leveraging prior knowledge to distinguish between low-entropy and high-entropy parts of the code, as indicated by a Cue List of words.We then inject the watermark guided by this Cue List, achieving higher detectability and usability than existing methods.We evaluated our proposed method on HumanEvaland compared our method with three state-of-the-art code watermarking techniques. The results demonstrate the effectiveness of our approach.\nlink: https://arxiv.org/abs/2512.14753v1\n"}}
{"custom_id": "2512.15779v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Hyperparameter Tuning-Based Optimized Performance Analysis of Machine Learning Algorithms for Network Intrusion Detection\nsummary: Network Intrusion Detection Systems (NIDS) are essential for securing networks by identifying and mitigating unauthorized activities indicative of cyberattacks. As cyber threats grow increasingly sophisticated, NIDS must evolve to detect both emerging threats and deviations from normal behavior. This study explores the application of machine learning (ML) methods to improve the NIDS accuracy through analyzing intricate structures in deep-featured network traffic records. Leveraging the 1999 KDD CUP intrusion dataset as a benchmark, this research evaluates and optimizes several ML algorithms, including Support Vector Machines (SVM), Na\u00efve Bayes variants (MNB, BNB), Random Forest (RF), k-Nearest Neighbors (k-NN), Decision Trees (DT), AdaBoost, XGBoost, Logistic Regression (LR), Ridge Classifier, Passive-Aggressive (PA) Classifier, Rocchio Classifier, Artificial Neural Networks (ANN), and Perceptron (PPN). Initial evaluations without hyper-parameter optimization demonstrated suboptimal performance, highlighting the importance of tuning to enhance classification accuracy. After hyper-parameter optimization using grid and random search techniques, the SVM classifier achieved 99.12% accuracy with a 0.0091 False Alarm Rate (FAR), outperforming its default configuration (98.08% accuracy, 0.0123 FAR) and all other classifiers. This result confirms that SVM accomplishes the highest accuracy among the evaluated classifiers. We validated the effectiveness of all classifiers using a tenfold cross-validation approach, incorporating Recursive Feature Elimination (RFE) for feature selection to enhance the classifiers accuracy and efficiency. Our outcomes indicate that ML classifiers are both adaptable and reliable, contributing to enhanced accuracy in systems for detecting network intrusions.\nlink: https://arxiv.org/abs/2512.15779v1\n"}}
{"custom_id": "2512.15778v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: RAMBO: Reliability Analysis for Mamba through Bit-flip attack Optimization\nsummary: State-space models (SSMs), exemplified by the Mamba architecture, have recently emerged as state-of-the-art sequence-modeling frameworks, offering linear-time scalability together with strong performance in long-context settings. Owing to their unique combination of efficiency, scalability, and expressive capacity, SSMs have become compelling alternatives to transformer-based models, which suffer from the quadratic computational and memory costs of attention mechanisms. As SSMs are increasingly deployed in real-world applications, it is critical to assess their susceptibility to both software- and hardware-level threats to ensure secure and reliable operation. Among such threats, hardware-induced bit-flip attacks (BFAs) pose a particularly severe risk by corrupting model parameters through memory faults, thereby undermining model accuracy and functional integrity. To investigate this vulnerability, we introduce RAMBO, the first BFA framework specifically designed to target Mamba-based architectures. Through experiments on the Mamba-1.4b model with LAMBADA benchmark, a cloze-style word-prediction task, we demonstrate that flipping merely a single critical bit can catastrophically reduce accuracy from 74.64% to 0% and increase perplexity from 18.94 to 3.75 x 10^6. These results demonstrate the pronounced fragility of SSMs to adversarial perturbations.\nlink: https://arxiv.org/abs/2512.15778v1\n"}}
{"custom_id": "2512.14751v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: One Leak Away: How Pretrained Model Exposure Amplifies Jailbreak Risks in Finetuned LLMs\nsummary: Finetuning pretrained large language models (LLMs) has become the standard paradigm for developing downstream applications. However, its security implications remain unclear, particularly regarding whether finetuned LLMs inherit jailbreak vulnerabilities from their pretrained sources. We investigate this question in a realistic pretrain-to-finetune threat model, where the attacker has white-box access to the pretrained LLM and only black-box access to its finetuned derivatives. Empirical analysis shows that adversarial prompts optimized on the pretrained model transfer most effectively to its finetuned variants, revealing inherited vulnerabilities from pretrained to finetuned LLMs. To further examine this inheritance, we conduct representation-level probing, which shows that transferable prompts are linearly separable within the pretrained hidden states, suggesting that universal transferability is encoded in pretrained representations. Building on this insight, we propose the Probe-Guided Projection (PGP) attack, which steers optimization toward transferability-relevant directions. Experiments across multiple LLM families and diverse finetuned tasks confirm PGP's strong transfer success, underscoring the security risks inherent in the pretrain-to-finetune paradigm.\nlink: https://arxiv.org/abs/2512.14751v1\n"}}
{"custom_id": "2512.15777v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Variable Record Table: A Unified Hardware-Assisted Framework for Runtime Security\nsummary: Modern computing systems face security threats, including memory corruption attacks, speculative execution vulnerabilities, and control-flow hijacking. Although existing solutions address these threats individually, they frequently introduce performance overhead and leave security gaps. This paper presents a Variable Record Table (VRT) with a unified hardware-assisted framework that simultaneously enforces spatial memory safety against buffer overflows, back-edge control-flow integrity (CFI), and speculative execution attack detection. The VRT dynamically constructs a protection table by instrumenting run-time instructions to extract memory addresses, bounds metadata, and control-flow signatures. Our evaluation across MiBench and SPEC benchmarks shows that VRT successfully detects all attack variants tested with zero additional instruction overhead. Furthermore, it maintains memory requirements below 25KB (for 512 entries) and maintains area / power overhead under 8% and 11.65 \u03bcW, respectively. By consolidating three essential security mechanisms into a single hardware structure, VRT provides comprehensive protection while minimizing performance impact.\nlink: https://arxiv.org/abs/2512.15777v1\n"}}
{"custom_id": "2512.14748v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Modeling the Interdependent Coupling of Safety and Security for Connected and Automated Vehicles: A Copula-Based Integrated Risk Analysis Approach\nsummary: Safety and security are critical to the reliable operation of connected and automated vehicles (CAVs). While existing research has identified correlations between the two domains, a theoretical framework to analyze their interaction mechanisms and guide co-design remains lacking. To address this gap, this paper proposes a copula-based joint safety-security analysis method to quantify their coupling effects. First, we formulate time-varying cyberattacks using dynamic risk functions derived from survival analysis, while modeling random hardware failures with the Weibull distribution, as per the automotive industry standard ISO 26262. Second, to capture the dependence between functional safety failures and cyber threats, we introduce a joint failure model based on copula theory, employing both elliptical (e.g., Gaussian) and Archimedean (e.g., Frank) copula families to construct a system-level failure function. Furthermore, we provide formal theoretical analysis of the dependence structure in the safety-security coupling, yielding three key insights: (1) a monotonic relationship between joint failure probability and dependence parameters, (2) the mechanisms of defensive response mechanisms (such as patch deployment) in mitigating joint failures, and (3) quantifying the dynamic coupling strength between safety and security under dependence structures. Through comprehensive simulations, we evaluate the sensitivity of the joint failure behavior to three critical factors: copula dependence parameters, security patch deployment timing, and Weibull distribution parameters. Our dynamic failure model further illustrates how cyberattacks affect safety failures and, conversely, how functional faults affect security failures under dependencies structures. This study provides a quantifiable theoretical foundation for the co-design of safety and security in CAVs.\nlink: https://arxiv.org/abs/2512.14748v1\n"}}
{"custom_id": "2512.14746v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: BLINDSPOT: Enabling Bystander-Controlled Privacy Signaling for Camera-Enabled Devices\nsummary: Camera-equipped mobile devices, such as phones, smart glasses, and AR headsets, pose a privacy challenge for bystanders, who currently lack effective real-time mechanisms to control the capture of their picture, video, including their face. We present BlindSpot, an on-device system that enables bystanders to manage their own privacy by signaling their privacy preferences in real-time without previously sharing any sensitive information. Our main contribution is the design and comparative evaluation of three distinct signaling modalities: a hand gesture mechanism, a significantly improved visible light communication (VLC) protocol, and a novel ultra-wideband (UWB) communication protocol. For all these modalities, we also design a validation mechanism that uses geometric consistency checks to verify the origin of a signal relative to the sending bystander, and defend against impersonation attacks. We implement the complete system (BlindSpot) on a commodity smartphone and conduct a comprehensive evaluation of each modality's accuracy and latency across various distances, lighting conditions, and user movements. Our results demonstrate the feasibility of these novel bystander signaling techniques and their trade-offs in terms of system performance and convenience.\nlink: https://arxiv.org/abs/2512.14746v1\n"}}
{"custom_id": "2512.14745v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Factor(U,T): Controlling Untrusted AI by Monitoring their Plans\nsummary: As AI capabilities advance, we increasingly rely on powerful models to decompose complex tasks $\\unicode{x2013}$ but what if the decomposer itself is malicious? Factored cognition protocols decompose complex tasks into simpler child tasks: one model creates the decomposition, while other models implement the child tasks in isolation. Prior work uses trusted (weaker but reliable) models for decomposition, which limits usefulness for tasks where decomposition itself is challenging. We introduce Factor($U$,$T$), in which an untrusted (stronger but potentially malicious) model decomposes while trusted models implement child tasks. Can monitors detect malicious activity when observing only natural language task instructions, rather than complete solutions? We baseline and red team Factor($U$,$T$) in control evaluations on BigCodeBench, a dataset of Python coding tasks. Monitors distinguishing malicious from honest decompositions perform poorly (AUROC 0.52) compared to monitors evaluating complete Python solutions (AUROC 0.96). Furthermore, Factor($D$,$U$), which uses a trusted decomposer and monitors concrete child solutions, achieves excellent discrimination (AUROC 0.96) and strong safety (1.2% ASR), demonstrating that implementation-context monitoring succeeds where decomposition-only monitoring fails.\nlink: https://arxiv.org/abs/2512.14745v1\n"}}
{"custom_id": "2512.15769v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Data-Chain Backdoor: Do You Trust Diffusion Models as Generative Data Supplier?\nsummary: The increasing use of generative models such as diffusion models for synthetic data augmentation has greatly reduced the cost of data collection and labeling in downstream perception tasks. However, this new data source paradigm may introduce important security concerns. This work investigates backdoor propagation in such emerging generative data supply chains, namely Data-Chain Backdoor (DCB). Specifically, we find that open-source diffusion models can become hidden carriers of backdoors. Their strong distribution-fitting ability causes them to memorize and reproduce backdoor triggers during generation, which are subsequently inherited by downstream models, resulting in severe security risks. This threat is particularly concerning under clean-label attack scenarios, as it remains effective while having negligible impact on the utility of the synthetic data. Furthermore, we discover an Early-Stage Trigger Manifestation (ESTM) phenomenon: backdoor trigger patterns tend to surface more explicitly in the early, high-noise stages of the diffusion model's reverse generation process before being subtly integrated into the final samples. Overall, this work reveals a previously underexplored threat in generative data pipelines and provides initial insights toward mitigating backdoor risks in synthetic data generation.\nlink: https://arxiv.org/abs/2512.15769v1\n"}}
{"custom_id": "2512.15768v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: PHANTOM: Progressive High-fidelity Adversarial Network for Threat Object Modeling\nsummary: The scarcity of cyberattack data hinders the development of robust intrusion detection systems. This paper introduces PHANTOM, a novel adversarial variational framework for generating high-fidelity synthetic attack data. Its innovations include progressive training, a dual-path VAE-GAN architecture, and domain-specific feature matching to preserve the semantics of attacks. Evaluated on 100,000 network traffic samples, models trained on PHANTOM data achieve 98% weighted accuracy on real attacks. Statistical analyses confirm that the synthetic data preserves authentic distributions and diversity. Limitations in generating rare attack types are noted, highlighting challenges with severe class imbalance. This work advances the generation of synthetic data for training robust, privacy-preserving detection systems.\nlink: https://arxiv.org/abs/2512.15768v1\n"}}
{"custom_id": "2512.14742v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Quantum-Augmented AI/ML for O-RAN: Hierarchical Threat Detection with Synergistic Intelligence and Interpretability (Technical Report)\nsummary: Open Radio Access Networks (O-RAN) enhance modularity and telemetry granularity but also widen the cybersecurity attack surface across disaggregated control, user and management planes. We propose a hierarchical defense framework with three coordinated layers-anomaly detection, intrusion confirmation, and multiattack classification-each aligned with O-RAN's telemetry stack. Our approach integrates hybrid quantum computing and machine learning, leveraging amplitude- and entanglement-based feature encodings with deep and ensemble classifiers. We conduct extensive benchmarking across synthetic and real-world telemetry, evaluating encoding depth, architectural variants, and diagnostic fidelity. The framework consistently achieves near-perfect accuracy, high recall, and strong class separability. Multi-faceted evaluation across decision boundaries, probabilistic margins, and latent space geometry confirms its interpretability, robustness, and readiness for slice-aware diagnostics and scalable deployment in near-RT and non-RT RIC domains.\nlink: https://arxiv.org/abs/2512.14742v1\n"}}
{"custom_id": "2512.14741v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Persistent Backdoor Attacks under Continual Fine-Tuning of LLMs\nsummary: Backdoor attacks embed malicious behaviors into Large Language Models (LLMs), enabling adversaries to trigger harmful outputs or bypass safety controls. However, the persistence of the implanted backdoors under user-driven post-deployment continual fine-tuning has been rarely examined. Most prior works evaluate the effectiveness and generalization of implanted backdoors only at releasing and empirical evidence shows that naively injected backdoor persistence degrades after updates. In this work, we study whether and how implanted backdoors persist through a multi-stage post-deployment fine-tuning. We propose P-Trojan, a trigger-based attack algorithm that explicitly optimizes for backdoor persistence across repeated updates. By aligning poisoned gradients with those of clean tasks on token embeddings, the implanted backdoor mapping is less likely to be suppressed or forgotten during subsequent updates. Theoretical analysis shows the feasibility of such persistent backdoor attacks after continual fine-tuning. And experiments conducted on the Qwen2.5 and LLaMA3 families of LLMs, as well as diverse task sequences, demonstrate that P-Trojan achieves over 99% persistence while preserving clean-task accuracy. Our findings highlight the need for persistence-aware evaluation and stronger defenses in realistic model adaptation pipelines.\nlink: https://arxiv.org/abs/2512.14741v1\n"}}
{"custom_id": "2512.15766v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models\nsummary: Loop transformations are semantics-preserving optimization techniques, widely used to maximize objectives such as parallelism. Despite decades of research, applying the optimal composition of loop transformations remains challenging due to inherent complexities, including cost modeling for optimization objectives. Recent studies have explored the potential of Large Language Models (LLMs) for code optimization. However, our key observation is that LLMs often struggle with effective loop transformation optimization, frequently leading to errors or suboptimal optimization, thereby missing opportunities for performance improvements. To bridge this gap, we propose LOOPRAG, a novel retrieval-augmented generation framework designed to guide LLMs in performing effective loop optimization on Static Control Part. We introduce a parameter-driven method to harness loop properties, which trigger various loop transformations, and generate diverse yet legal example codes serving as a demonstration source. To effectively obtain the most informative demonstrations, we propose a loop-aware algorithm based on loop features, which balances similarity and diversity for code retrieval. To enhance correct and efficient code generation, we introduce a feedback-based iterative mechanism that incorporates compilation, testing and performance results as feedback to guide LLMs. Each optimized code undergoes mutation, coverage and differential testing for equivalence checking. We evaluate LOOPRAG on PolyBench, TSVC and LORE benchmark suites, and compare it against compilers (GCC-Graphite, Clang-Polly, Perspective and ICX) and representative LLMs (DeepSeek and GPT-4). The results demonstrate average speedups over base compilers of up to 11.20$\\times$, 14.34$\\times$, and 9.29$\\times$ for PolyBench, TSVC, and LORE, respectively, and speedups over base LLMs of up to 11.97$\\times$, 5.61$\\times$, and 11.59$\\times$.\nlink: https://arxiv.org/abs/2512.15766v1\n"}}
{"custom_id": "2512.15754v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: A Survey on Reconfigurable Intelligent Surfaces in Practical Systems: Security and Privacy Perspectives\nsummary: Reconfigurable Intelligent Surfaces (RIS) have emerged as a transformative technology capable of reshaping wireless environments through dynamic manipulation of electromagnetic waves. While extensive research has explored their theoretical benefits for communication and sensing, practical deployments in smart environments such as homes, vehicles, and industrial settings remain limited and under-examined, particularly from security and privacy perspectives. This survey provides a comprehensive examination of RIS applications in real-world systems, with a focus on the security and privacy threats, vulnerabilities, and defensive strategies relevant to practical use. We analyze scenarios with two types of systems (with and without legitimate RIS) and two types of attackers (with and without malicious RIS), and demonstrate how RIS may introduce new attacks to practical systems, including eavesdropping, jamming, and spoofing attacks. In response, we review defenses against RIS-related attacks in these systems, such as applying additional security algorithms, disrupting attackers, and early detection of unauthorized RIS. We also discuss scenarios in which the legitimate user applies an additional RIS to defend against attacks. To support future research, we also provide a collection of open-source tools, datasets, demos, and papers at: https://awesome-ris-security.github.io/. By highlighting RIS's functionality and its security/privacy challenges and opportunities, this survey aims to guide researchers and engineers toward the development of secure, resilient, and privacy-preserving RIS-enabled practical wireless systems and environments.\nlink: https://arxiv.org/abs/2512.15754v1\n"}}
{"custom_id": "2512.14737v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Zero-Knowledge Audit for Internet of Agents: Privacy-Preserving Communication Verification with Model Context Protocol\nsummary: Existing agent communication frameworks face critical limitations in providing verifiable audit trails without compromising the privacy and confidentiality of agent interactions. The protection of agent communication privacy while ensuring auditability emerges as a fundamental challenge for applications requiring accurate billing, compliance verification, and accountability in regulated environments.\n  We introduce a framework for auditing agent communications that keeps messages private while still checking they follow expected rules. It pairs zero-knowledge proofs with the existing Model Context Protocol (MCP) so messages can be verified without revealing their contents. The approach runs in lightweight networks, stays compatible with standard MCP exchanges, and adds asynchronous audit verification to confirm format and general message types without exposing specifics.\n  The framework enables mutual audits between agents: one side can check communication content and quality while the other verifies usage metrics, all without revealing sensitive information. We formalize security goals and show that zk-MCP provides data authenticity and communication privacy, achieving efficient verification with negligible latency overhead. We fully implement the framework, including Circom-based zero-knowledge proof generation and an audit protocol integrated with MCP's bidirectional channel, and, to our knowledge, this is the first privacy-preserving audit system for agent communications that offers verifiable mutual auditing without exposing message content or compromising agent privacy.\nlink: https://arxiv.org/abs/2512.14737v1\n"}}
{"custom_id": "2512.11004v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Enhancing the Practical Reliability of Shor's Quantum Algorithm via Generalized Period Decomposition: Theory and Large-Scale Empirical Validation\nsummary: This work presents a generalized period decomposition approach, significantly improving the practical reliability of Shor's quantum factoring algorithm. Although Shor's algorithm theoretically enables polynomial-time integer factorization, its real-world performance heavily depends on stringent conditions related to the period obtained via quantum phase estimation. Our generalized decomposition method relaxes these conditions by systematically exploiting arbitrary divisors of the obtained period, effectively broadening the applicability of each quantum execution. Extensive classical simulations were performed to empirically validate our approach, involving over one million test cases across integers ranging from 2 to 8 digits. The proposed method achieved near-perfect success rates, exceeding 99.998% for 7-digit numbers and 99.999% for 8-digit numbers, significantly surpassing traditional and recently improved variants of Shor's algorithm. Crucially, this improvement is achieved without compromising the algorithm's polynomial-time complexity and integrates seamlessly with existing quantum computational frameworks. Moreover, our method enhances the efficiency of quantum resource usage by minimizing unnecessary repetitions, making it particularly relevant for quantum cryptanalysis with noisy intermediate-scale quantum (NISQ) devices. This study thus provides both theoretical advancements and substantial practical benefits, contributing meaningfully to the field of quantum algorithm research and the broader field of quantum information processing.\nlink: https://arxiv.org/abs/2512.11004v1\n"}}
{"custom_id": "2512.06364v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: JEEVHITAA -- An End-to-End HCAI System to Support Collective Care\nsummary: Current mobile health platforms are predominantly individual-centric and lack the necessary primitives for coordinated, auditable, multi-actor workflows. However, in many settings worldwide, health decisions are enacted by multi-actor care networks rather than single users. We present JEEVHITAA, an Android/Flutter system that provides context-sensitive, role-aware sharing and verifiable information flows for care circles. JEEVHITAA ingests platform and device data (via Google Health Connect and BLE connectors), constructs multi-layer user profiles from sensor streams and tiered onboarding, and enforces fine-grained, time-bounded access control across permissioned care graphs. Data are end-to-end encrypted in local stores and during peer sync (Firebase), and provisions are made for document capture by camera or upload as PDF. An integrated retrieval-augmented LLM pipeline (i) produces structured, role-targeted summaries and action plans, (ii) enables users to gather advanced insights on health reports, and (iii) performs evidence-grounded user-relevant verification of arbitrary health content, returning provenance, confidence scores, and source citations. We describe the system architecture, connector abstractions, and security primitives, and evaluate robustness and compatibility using synthetic, ontology-driven simulations and vendor compatibility tests. Finally, we outline plans for longitudinal in-the-wild deployments to measure system performance, the correctness of access control, and the real-world effectiveness of relationship-aware credibility support.\nlink: https://arxiv.org/abs/2512.06364v2\n"}}
{"custom_id": "2512.10998v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: SCOUT: A Defense Against Data Poisoning Attacks in Fine-Tuned Language Models\nsummary: Backdoor attacks create significant security threats to language models by embedding hidden triggers that manipulate model behavior during inference, presenting critical risks for AI systems deployed in healthcare and other sensitive domains. While existing defenses effectively counter obvious threats such as out-of-context trigger words and safety alignment violations, they fail against sophisticated attacks using contextually-appropriate triggers that blend seamlessly into natural language. This paper introduces three novel contextually-aware attack scenarios that exploit domain-specific knowledge and semantic plausibility: the ViralApp attack targeting social media addiction classification, the Fever attack manipulating medical diagnosis toward hypertension, and the Referral attack steering clinical recommendations. These attacks represent realistic threats where malicious actors exploit domain-specific vocabulary while maintaining semantic coherence, demonstrating how adversaries can weaponize contextual appropriateness to evade conventional detection methods. To counter both traditional and these sophisticated attacks, we present \\textbf{SCOUT (Saliency-based Classification Of Untrusted Tokens)}, a novel defense framework that identifies backdoor triggers through token-level saliency analysis rather than traditional context-based detection methods. SCOUT constructs a saliency map by measuring how the removal of individual tokens affects the model's output logits for the target label, enabling detection of both conspicuous and subtle manipulation attempts. We evaluate SCOUT on established benchmark datasets (SST-2, IMDB, AG News) against conventional attacks (BadNet, AddSent, SynBkd, StyleBkd) and our novel attacks, demonstrating that SCOUT successfully detects these sophisticated threats while preserving accuracy on clean inputs.\nlink: https://arxiv.org/abs/2512.10998v1\n"}}
{"custom_id": "2512.15742v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: SHARe-KAN: Holographic Vector Quantization for Memory-Bound Inference\nsummary: Kolmogorov-Arnold Networks (KANs) face a fundamental memory wall: their learned basis functions create parameter counts that impose extreme bandwidth demands, hindering deployment in memory-constrained environments. We show that Vision KANs exhibit a holographic topology, where information is distributed across the interference of splines rather than localized to specific edges. Consequently, traditional pruning fails (10% sparsity degrades mAP from 85.23% to 45%, a $\\sim$40-point drop). To address this, we present SHARe-KAN, a framework utilizing Gain-Shape-Bias Vector Quantization to exploit functional redundancy while preserving the dense topology. Coupled with LUTHAM, a hardware-aware compiler with static memory planning, we achieve $88\\times$ runtime memory reduction (1.13 GB $\\to$ 12.91 MB) and match uncompressed baseline accuracy on PASCAL VOC. Profiling on NVIDIA Ampere architecture confirms $>90\\%$ L2 cache residency, demonstrating that the workload is decoupled from DRAM bandwidth constraints inherent to spline-based architectures.\nlink: https://arxiv.org/abs/2512.15742v1\n"}}
{"custom_id": "2512.09961v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: TDC-Cache: A Trustworthy Decentralized Cooperative Caching Framework for Web3.0\nsummary: The rapid growth of Web3.0 is transforming the Internet from a centralized structure to decentralized, which empowers users with unprecedented self-sovereignty over their own data. However, in the context of decentralized data access within Web3.0, it is imperative to cope with efficiency concerns caused by the replication of redundant data, as well as security vulnerabilities caused by data inconsistency. To address these challenges, we develop a Trustworthy Decentralized Cooperative Caching (TDC-Cache) framework for Web3.0 to ensure efficient caching and enhance system resilience against adversarial threats. This framework features a two-layer architecture, wherein the Decentralized Oracle Network (DON) layer serves as a trusted intermediary platform for decentralized caching, bridging the contents from decentralized storage and the content requests from users. In light of the complexity of Web3.0 network topologies and data flows, we propose a Deep Reinforcement Learning-Based Decentralized Caching (DRL-DC) for TDC-Cache to dynamically optimize caching strategies of distributed oracles. Furthermore, we develop a Proof of Cooperative Learning (PoCL) consensus to maintain the consistency of decentralized caching decisions within DON. Experimental results show that, compared with existing approaches, the proposed framework reduces average access latency by 20%, increases the cache hit rate by at most 18%, and improves the average success consensus rate by 10%. Overall, this paper serves as a first foray into the investigation of decentralized caching framework and strategy for Web3.0.\nlink: https://arxiv.org/abs/2512.09961v1\n"}}
{"custom_id": "2512.09959v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: TRUCE: TRUsted Compliance Enforcement Service for Secure Health Data Exchange\nsummary: Organizations are increasingly sharing large volumes of sensitive Personally Identifiable Information (PII), like health records, with each other to better manage their services. Protecting PII data has become increasingly important in today's digital age, and several regulations have been formulated to ensure the secure exchange and management of sensitive personal data. However, at times some of these regulations are at loggerheads with each other, like the Health Insurance Portability and Accountability Act (HIPAA) and Cures Act; and this adds complexity to the already challenging task of Health Data compliance. As public concern regarding sensitive data breaches grows, finding solutions that streamline compliance processes and enhance individual privacy is crucial. We have developed a novel TRUsted Compliance Enforcement (TRUCE) framework for secure data exchange which aims to automate compliance procedures and enhance trusted data management within organizations. The TRUCE framework reasons over contexts of data exchange and assesses the trust score of users and the veracity of data based on corresponding regulations. This framework, developed using approaches from AI/Knowledge representation and Semantic Web technologies, includes a trust management method that incorporates static ground truth, represented by regulations such as HIPAA, and dynamic ground truth, defined by an organization's policies. In this paper, we present our framework in detail along with the validation against the Health Insurance Portability and Accountability Act (HIPAA) Data Usage Agreement (DUA) on CDC Contact Tracing patient data, up to one million patient records. TRUCE service will streamline compliance efforts and ensure adherence to privacy regulations and can be used by organizations to manage compliance of large velocity data exchange in real time.\nlink: https://arxiv.org/abs/2512.09959v1\n"}}
{"custom_id": "2512.09958v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: When Quantum Federated Learning Meets Blockchain in 6G Networks\nsummary: Quantum federated learning (QFL) is emerging as a key enabler for intelligent, secure, and privacy-preserving model training in next-generation 6G networks. By leveraging the computational advantages of quantum devices, QFL offers significant improvements in learning efficiency and resilience against quantum-era threats. However, future 6G environments are expected to be highly dynamic, decentralized, and data-intensive, which necessitates moving beyond traditional centralized federated learning frameworks. To meet this demand, blockchain technology provides a decentralized, tamper-resistant infrastructure capable of enabling trustless collaboration among distributed quantum edge devices. This paper presents QFLchain, a novel framework that integrates QFL with blockchain to support scalable and secure 6G intelligence. In this work, we investigate four key pillars of \\textit{QFLchain} in the 6G context: (i) communication and consensus overhead, (ii) scalability and storage overhead, (iii) energy inefficiency, and (iv) security vulnerability. A case study is also presented, demonstrating potential advantages of QFLchain, based on simulation, over state-of-the-art approaches in terms of training performance.\nlink: https://arxiv.org/abs/2512.09958v1\n"}}
{"custom_id": "2512.09957v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: CloudFix: Automated Policy Repair for Cloud Access Control Policies Using Large Language Models\nsummary: Access control policies are vital for securing modern cloud computing, where organizations must manage access to sensitive data across thousands of users in distributed system settings. Cloud administrators typically write and update policies manually, which can be an error-prone and time-consuming process and can potentially lead to security vulnerabilities. Existing approaches based on symbolic analysis have demonstrated success in automated debugging and repairing access control policies; however, their generalizability is limited in the context of cloud-based access control. Conversely, Large Language Models (LLMs) have been utilized for automated program repair; however, their applicability to repairing cloud access control policies remains unexplored. In this work, we introduce CloudFix, the first automated policy repair framework for cloud access control that combines formal methods with LLMs. Given an access control policy and a specification of allowed and denied access requests, CloudFix employs Formal Methods-based Fault Localization to identify faulty statements in the policy and leverages LLMs to generate potential repairs, which are then verified using SMT solvers. To evaluate CloudFix, we curated a dataset of 282 real-world AWS access control policies extracted from forum posts and augmented them with synthetically generated request sets based on real scenarios. Our experimental results show that CloudFix improves repair accuracy over a Baseline implementation across varying request sizes. Our work is the first to leverage LLMs for policy repair, showcasing the effectiveness of LLMs for access control and enabling efficient and automated repair of cloud access control policies. We make our tool Cloudfix and AWS dataset publicly available.\nlink: https://arxiv.org/abs/2512.09957v1\n"}}
{"custom_id": "2512.09954v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Cross-Layer Isochronous Diffusion Protocol (CIDP): A Rigorous Information-Theoretic and Control-Theoretic Framework for Sovereign Tactical Anonymity\nsummary: Next-generation tactical networks face a critical Anonymity Trilemma: it is impossible to simultaneously achieve strong anonymity, low latency (isochrony), and low bandwidth overhead under a global passive adversary. CIDP breaks this deadlock by injecting physical-layer entropy via rapid antenna sidelobe modulation, enabling near-isochronous, low-overhead anonymous communication. CIDP jointly designs: (a) a Lyapunov drift-plus-penalty network controller that stabilizes queues and maximizes entropy injection; (b) a robust discrete-time Control Barrier Function (RaCBF) filter that provably enforces deterministic jitter bounds for real-time flows despite uncertainty; and (c) a convex Sidelobe Time Modulation (SLTM) optimization that spreads signals into the antenna null-space to mask transmissions. We explicitly augment the classical anonymity bound with a physical-layer equivocation term, showing that rapidly changing sidelobes contribute additional secrecy. Consequently, as the injected physical entropy grows, both latency and dummy overhead can approach zero for a fixed anonymity target. We provide full theoretical proofs of queue stability, barrier-set invariance, and SLTM convexity. Moreover, we quantitatively benchmark our SLTM design against recent LPI/LPD schemes, demonstrating significantly lower intercept probability for comparable overhead. High-fidelity MATLAB/NS-3 simulations and an FPGA prototype validate CIDP: results show approximately 40% larger anonymity sets and 100% compliance with sub-30 ms jitter (compared to a Tor-like baseline), with only about 5% throughput loss. We also outline a Modular Open Systems Approach (MOSA) and FOCI-compliant supply-chain strategy. CIDP is the first architecture that simultaneously addresses strong anonymity, strict isochrony, and spectral efficiency with provable guarantees, making it highly relevant for sovereign JADC2 deployments.\nlink: https://arxiv.org/abs/2512.09954v1\n"}}
{"custom_id": "2512.09953v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: ZK-APEX: Zero-Knowledge Approximate Personalized Unlearning with Executable Proofs\nsummary: Machine unlearning aims to remove the influence of specific data points from a trained model to satisfy privacy, copyright, and safety requirements. In real deployments, providers distribute a global model to many edge devices, where each client personalizes the model using private data. When a deletion request is issued, clients may ignore it or falsely claim compliance, and providers cannot check their parameters or data. This makes verification difficult, especially because personalized models must forget the targeted samples while preserving local utility, and verification must remain lightweight on edge devices.\n  We introduce ZK APEX, a zero-shot personalized unlearning method that operates directly on the personalized model without retraining. ZK APEX combines sparse masking on the provider side with a small Group OBS compensation step on the client side, using a blockwise empirical Fisher matrix to create a curvature-aware update designed for low overhead. Paired with Halo2 zero-knowledge proofs, it enables the provider to verify that the correct unlearning transformation was applied without revealing any private data or personalized parameters.\n  On Vision Transformer classification tasks, ZK APEX recovers nearly all personalization accuracy while effectively removing the targeted information. Applied to the OPT125M generative model trained on code data, it recovers around seventy percent of the original accuracy. Proof generation for the ViT case completes in about two hours, more than ten million times faster than retraining-based checks, with less than one gigabyte of memory use and proof sizes around four hundred megabytes. These results show the first practical framework for verifiable personalized unlearning on edge devices.\nlink: https://arxiv.org/abs/2512.09953v1\n"}}
{"custom_id": "2512.09006v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning\nsummary: The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: https://github.com/DynaSoumhaneOuchebara/Llama-based-vulnerability-detection.\nlink: https://arxiv.org/abs/2512.09006v1\n"}}
{"custom_id": "2512.10990v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Dora: QoE-Aware Hybrid Parallelism for Distributed Edge AI\nsummary: With the proliferation of edge AI applications, satisfying user quality of experience (QoE) requirements, such as model inference latency, has become a first class objective, as these models operate in resource constrained settings and directly interact with users. Yet, modern AI models routinely exceed the resource capacity of individual devices, necessitating distributed execution across heterogeneous devices over variable and contention prone networks. Existing planners for hybrid (e.g., data and pipeline) parallelism largely optimize for throughput or device utilization, overlooking QoE, leading to severe resource inefficiency (e.g., unnecessary energy drain) or QoE violations under runtime dynamics.\n  We present Dora, a framework for QoE aware hybrid parallelism in distributed edge AI training and inference. Dora jointly optimizes heterogeneous computation, contention prone networks, and multi dimensional QoE objectives via three key mechanisms: (i) a heterogeneity aware model partitioner that determines and assigns model partitions across devices, forming a compact set of QoE compliant plans; (ii) a contention aware network scheduler that further refines these candidate plans by maximizing compute communication overlap; and (iii) a runtime adapter that adaptively composes multiple plans to maximize global efficiency while respecting overall QoEs. Across representative edge deployments, including smart homes, traffic analytics, and small edge clusters, Dora achieves 1.1--6.3 times faster execution and, alternatively, reduces energy consumption by 21--82 percent, all while maintaining QoE under runtime dynamics.\nlink: https://arxiv.org/abs/2512.10990v1\n"}}
{"custom_id": "2512.07909v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Agentic Artificial Intelligence for Ethical Cybersecurity in Uganda: A Reinforcement Learning Framework for Threat Detection in Resource-Constrained Environments\nsummary: Uganda's rapid digital transformation, supported by national strategies such as Vision 2040 and the Digital Transformation Roadmap, has expanded reliance on networked services while simultaneously increasing exposure to sophisticated cyber threats. In resource-constrained settings, commonly deployed rule-based intrusion detection systems lack the adaptability and ethical safeguards needed to address evolving attack patterns, leading to undetected breaches and excessive blocking of legitimate traffic. This study proposes an Agentic Artificial Intelligence (AAI) framework that integrates reinforcement learning, an explicit ethical governance layer, and human oversight to deliver adaptive and trustworthy cybersecurity. A CPU-optimized simulation environment was developed using a five-node network topology that mirrors key elements of Uganda's critical digital infrastructure and generates both benign and malicious traffic, including phishing, ransomware, and distributed denial-of-service attacks. A Q-learning agent, operating within clearly defined ethical constraints and subject to human auditability, was trained and evaluated against a traditional rule-based baseline. The AAI framework achieved a 100 percent detection rate, zero false positives, and full ethical compliance, compared with 70 percent detection and 15 percent false positives for the baseline system. These results demonstrate that agentic, ethically governed reinforcement learning can substantially improve cybersecurity effectiveness and fairness in CPU-only, resource-constrained environments, offering a practical pathway for operationalizing responsible AI in Uganda's national cybersecurity strategy.\nlink: https://arxiv.org/abs/2512.07909v1\n"}}
{"custom_id": "2512.11878v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: A Technical Policy Blueprint for Trustworthy Decentralized AI\nsummary: Decentralized AI systems, such as federated learning, can play a critical role in further unlocking AI asset marketplaces (e.g., healthcare data marketplaces) thanks to increased asset privacy protection. Unlocking this big potential necessitates governance mechanisms that are transparent, scalable, and verifiable. However current governance approaches rely on bespoke, infrastructure-specific policies that hinder asset interoperability and trust among systems. We are proposing a Technical Policy Blueprint that encodes governance requirements as policy-as-code objects and separates asset policy verification from asset policy enforcement. In this architecture the Policy Engine verifies evidence (e.g., identities, signatures, payments, trusted-hardware attestations) and issues capability packages. Asset Guardians (e.g. data guardians, model guardians, computation guardians, etc.) enforce access or execution solely based on these capability packages. This core concept of decoupling policy processing from capabilities enables governance to evolve without reconfiguring AI infrastructure, thus creating an approach that is transparent, auditable, and resilient to change.\nlink: https://arxiv.org/abs/2512.11878v1\n"}}
{"custom_id": "2512.09946v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: ELANA: A Simple Energy and Latency Analyzer for LLMs\nsummary: The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \\textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.\nlink: https://arxiv.org/abs/2512.09946v1\n"}}
{"custom_id": "2512.13709v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Smart Surveillance: Identifying IoT Device Behaviours using ML-Powered Traffic Analysis\nsummary: The proliferation of Internet of Things (IoT) devices has grown exponentially in recent years, introducing significant security challenges. Accurate identification of the types of IoT devices and their associated actions through network traffic analysis is essential to mitigate potential threats. By monitoring and analysing packet flows between IoT devices and connected networks, anomalous or malicious behaviours can be detected. Existing research focuses primarily on device identification within local networks using methods such as protocol fingerprinting and wireless frequency scanning. However, these approaches are limited in their ability to monitor or classify IoT devices externally. To address this gap, we investigate the use of machine learning (ML) techniques, specifically Random Forest (RF), Multilayer Perceptron (MLP), and K-Nearest Neighbours (KNN), in conjunction with targeted network traffic monitoring to classify IoT device types and their actions. We constructed a testbed comprising an NPAT-enabled router and a diverse set of IoT devices, including smart cameras, controller hubs, home appliances, power controllers, and streaming devices. Experimental results demonstrate that IoT device and action recognition is feasible using our proposed ML-driven approach, with the RF classifier achieving the highest accuracy of 91%, while the MLP recorded the lowest accuracy at 56%. Notably, all device categories were successfully classified except for certain actions associated with security cameras, underscoring both the potential and the limitations of the proposed method.\nlink: https://arxiv.org/abs/2512.13709v1\n"}}
{"custom_id": "2512.10766v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Metaphor-based Jailbreaking Attacks on Text-to-Image Models\nsummary: Text-to-image~(T2I) models commonly incorporate defense mechanisms to prevent the generation of sensitive images. Unfortunately, recent jailbreaking attacks have shown that adversarial prompts can effectively bypass these mechanisms and induce T2I models to produce sensitive content, revealing critical safety vulnerabilities. However, existing attack methods implicitly assume that the attacker knows the type of deployed defenses, which limits their effectiveness against unknown or diverse defense mechanisms. In this work, we introduce \\textbf{MJA}, a \\textbf{m}etaphor-based \\textbf{j}ailbreaking \\textbf{a}ttack method inspired by the Taboo game, aiming to effectively and efficiently attack diverse defense mechanisms without prior knowledge of their type by generating metaphor-based adversarial prompts. Specifically, MJA consists of two modules: an LLM-based multi-agent generation module~(MLAG) and an adversarial prompt optimization module~(APO). MLAG decomposes the generation of metaphor-based adversarial prompts into three subtasks: metaphor retrieval, context matching, and adversarial prompt generation. Subsequently, MLAG coordinates three LLM-based agents to generate diverse adversarial prompts by exploring various metaphors and contexts. To enhance attack efficiency, APO first trains a surrogate model to predict the attack results of adversarial prompts and then designs an acquisition strategy to adaptively identify optimal adversarial prompts. Extensive experiments on T2I models with various external and internal defense mechanisms demonstrate that MJA outperforms six baseline methods, achieving stronger attack performance while using fewer queries. Code is available in https://github.com/datar001/metaphor-based-jailbreaking-attack.\nlink: https://arxiv.org/abs/2512.10766v1\n"}}
{"custom_id": "2512.09942v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: A study of the spectrum resource leasing method based on ERC4907 extension\nsummary: The ERC4907 standard enables rentable Non-Fungible Tokens (NFTs) but is limited to single-user, single-time-slot authorization, which severely limits its applicability and efficiency in decentralized multi-slot scheduling scenarios. To address this limitation, this paper proposes Multi-slot ERC4907 (M-ERC4907) extension method. The M-ERC4907 method introduces novel functionalities to support the batch configuration of multiple time slots and simultaneous authorization of multiple users, thereby effectively eliminating the rigid sequential authorization constraint of ERC4907. The experiment was conducted on the Remix development platform. Experimental results show that the M-ERC4907 method significantly reduces on-chain transactions and overall Gas consumption, leading to enhanced scalability and resource allocation efficiency.\nlink: https://arxiv.org/abs/2512.09942v1\n"}}
{"custom_id": "2512.09941v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Fourier Sparsity of Delta Functions and Matching Vector PIRs\nsummary: In this paper we study a basic and natural question about Fourier analysis of Boolean functions, which has applications to the study of Matching Vector based Private Information Retrieval (PIR) schemes. For integers m and r, define a delta function on {0,1}^r to be a function f: Z_m^r -> C with f(0) = 1 and f(x) = 0 for all nonzero Boolean x. The basic question we study is how small the Fourier sparsity of a delta function can be; namely how sparse such an f can be in the Fourier basis?\n  In addition to being intrinsically interesting and natural, such questions arise naturally when studying \"S-decoding polynomials\" for the known matching vector families. Finding S-decoding polynomials of reduced sparsity, which corresponds to finding delta functions with low Fourier sparsity, would improve the current best PIR schemes.\n  We show nontrivial upper and lower bounds on the Fourier sparsity of delta functions. Our proofs are elementary and clean. These results imply limitations on improving Matching Vector PIR schemes simply by finding better S-decoding polynomials. In particular, there are no S-decoding polynomials that can make Matching Vector PIRs based on the known matching vector families achieve polylogarithmic communication with a constant number of servers. Many interesting questions remain open.\nlink: https://arxiv.org/abs/2512.09941v1\n"}}
{"custom_id": "2512.10987v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems\nsummary: In recent years, the landscape of federated learning has witnessed significant advancements, particularly in decentralized methodologies. This research paper presents a comprehensive comparison of Centralized Hierarchical Federated Learning (HFL) with Decentralized Aggregated Federated Learning (AFL) and Decentralized Continual Federated Learning (CFL) architectures. While HFL, in its centralized approach, faces challenges such as communication bottlenecks and privacy concerns due to centralized data aggregation, AFL and CFL provide promising alternatives by distributing computation and aggregation processes across devices. Through evaluation of Fashion MNIST and MNIST datasets, this study demonstrates the advantages of decentralized methodologies, showcasing how AFL and CFL outperform HFL in precision, recall, F1 score, and balanced accuracy. The analysis highlights the importance of decentralized aggregation mechanisms in AFL and CFL, which effectively enables collaborative model training across distributed devices. This comparative study contributes valuable insights into the evolving landscape of federated learning, guiding researchers and practitioners towards decentralized methodologies for enhanced performance in collaborative model training scenarios.\nlink: https://arxiv.org/abs/2512.10987v1\n"}}
{"custom_id": "2512.06048v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: The Road of Adaptive AI for Precision in Cybersecurity\nsummary: Cybersecurity's evolving complexity presents unique challenges and opportunities for AI research and practice. This paper shares key lessons and insights from designing, building, and operating production-grade GenAI pipelines in cybersecurity, with a focus on the continual adaptation required to keep pace with ever-shifting knowledge bases, tooling, and threats. Our goal is to provide an actionable perspective for AI practitioners and industry stakeholders navigating the frontier of GenAI for cybersecurity, with particular attention to how different adaptation mechanisms complement each other in end-to-end systems. We present practical guidance derived from real-world deployments, propose best practices for leveraging retrieval- and model-level adaptation, and highlight open research directions for making GenAI more robust, precise, and auditable in cyber defense.\nlink: https://arxiv.org/abs/2512.06048v1\n"}}
{"custom_id": "2512.13703v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Safe2Harm: Semantic Isomorphism Attacks for Jailbreaking Large Language Models\nsummary: Large Language Models (LLMs) have demonstrated exceptional performance across various tasks, but their security vulnerabilities can be exploited by attackers to generate harmful content, causing adverse impacts across various societal domains. Most existing jailbreak methods revolve around Prompt Engineering or adversarial optimization, yet we identify a previously overlooked phenomenon: many harmful scenarios are highly consistent with legitimate ones in terms of underlying principles. Based on this finding, this paper proposes the Safe2Harm Semantic Isomorphism Attack method, which achieves efficient jailbreaking through four stages: first, rewrite the harmful question into a semantically safe question with similar underlying principles; second, extract the thematic mapping relationship between the two; third, let the LLM generate a detailed response targeting the safe question; finally, reversely rewrite the safe response based on the thematic mapping relationship to obtain harmful output. Experiments on 7 mainstream LLMs and three types of benchmark datasets show that Safe2Harm exhibits strong jailbreaking capability, and its overall performance is superior to existing methods. Additionally, we construct a challenging harmful content evaluation dataset containing 358 samples and evaluate the effectiveness of existing harmful detection methods, which can be deployed for LLM input-output filtering to enable defense.\nlink: https://arxiv.org/abs/2512.13703v1\n"}}
{"custom_id": "2512.06033v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Sell Data to AI Algorithms Without Revealing It: Secure Data Valuation and Sharing via Homomorphic Encryption\nsummary: The rapid expansion of Artificial Intelligence is hindered by a fundamental friction in data markets: the value-privacy dilemma, where buyers cannot verify a dataset's utility without inspection, yet inspection may expose the data (Arrow's Information Paradox). We resolve this challenge by introducing the Trustworthy Influence Protocol (TIP), a privacy-preserving framework that enables prospective buyers to quantify the utility of external data without ever decrypting the raw assets. By integrating Homomorphic Encryption with gradient-based influence functions, our approach allows for the precise, blinded scoring of data points against a buyer's specific AI model. To ensure scalability for Large Language Models (LLMs), we employ low-rank gradient projections that reduce computational overhead while maintaining near-perfect fidelity to plaintext baselines, as demonstrated across BERT and GPT-2 architectures. Empirical simulations in healthcare and generative AI domains validate the framework's economic potential: we show that encrypted valuation signals achieve a high correlation with realized clinical utility and reveal a heavy-tailed distribution of data value in pre-training corpora where a minority of texts drive capability while the majority degrades it. These findings challenge prevailing flat-rate compensation models and offer a scalable technical foundation for a meritocratic, secure data economy.\nlink: https://arxiv.org/abs/2512.06033v1\n"}}
{"custom_id": "2509.18874v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: When Ads Become Profiles: Uncovering the Invisible Risk of Web Advertising at Scale with LLMs\nsummary: Regulatory limits on explicit targeting have not eliminated algorithmic profiling on the Web, as optimisation systems still adapt ad delivery to users' private attributes. The widespread availability of powerful zero-shot multimodal Large Language Models (LLMs) has dramatically lowered the barrier for exploiting these latent signals for adversarial inference. We investigate this emerging societal risk, specifically how adversaries can now exploit these signals to reverse-engineer private attributes from ad exposure alone. We introduce a novel pipeline that leverages LLMs as adversarial inference engines to perform natural language profiling. Applying this method to a longitudinal dataset comprising over 435,000 ad impressions collected from 891 users, we conducted a large-scale study to assess the feasibility and precision of inferring private attributes from passive online ad observations. Our results demonstrate that off-the-shelf LLMs can accurately reconstruct complex user private attributes, including party preference, employment status, and education level, consistently outperforming strong census-based priors and matching or exceeding human social perception, while operating at only a fraction of the cost (223$\\times$ lower) and time (52$\\times$ faster) required by humans. Critically, actionable profiling is feasible even within short observation windows, indicating that prolonged tracking is not a prerequisite for a successful attack. These findings provide the first empirical evidence that ad streams serve as a high-fidelity digital footprint, enabling off-platform profiling that inherently bypasses current platform safeguards, highlighting a systemic vulnerability in the ad ecosystem and the urgent need for responsible web AI governance in the generative AI era. The code is available at https://github.com/Breezelled/when-ads-become-profiles.\nlink: https://arxiv.org/abs/2509.18874v2\n"}}
{"custom_id": "2512.10980v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling\nsummary: GPU clusters have become essential for training and deploying modern AI systems, yet real deployments continue to report average utilization near 50%. This inefficiency is largely caused by fragmentation, heterogeneous workloads, and the limitations of static scheduling policies. This work presents a systematic evaluation of these issues and introduces three specialized dynamic schedulers: Hybrid Priority (HPS), Predictive Backfill (PBS), and Smart Batch (SBS). These schedulers are designed to improve utilization, fairness, and overall throughput in multi-tenant GPU clusters. We evaluate all schedulers using a controlled simulation of 1,000 AI jobs on a 64-GPU, 8-node cluster that includes a realistic mix of training, inference, and research workloads. Static baselines (FIFO, SJF, Shortest, Shortest-GPU) achieve 45 to 67% GPU utilization and 12.5 to 18.3 jobs per hour and experience severe starvation, with as many as 156 jobs waiting longer than 30 minutes. The dynamic schedulers significantly outperform these policies. HPS achieves the highest utilization (78.2%), highest throughput (25.8 jobs per hour), and the lowest fairness variance among dynamic methods (457), reducing starvation to 12 jobs. PBS improves fragmentation handling and reaches 76.1% utilization, while SBS increases efficiency for structurally similar jobs and reaches 74.6% utilization. Across all key metrics, including throughput, job wait times, fairness variance, and starvation, dynamic multi-objective schedulers consistently outperform single-objective heuristics. These results show that targeted and transparent scheduling strategies can meaningfully increase GPU efficiency in heterogeneous AI clusters and provide a practical foundation for future production scheduling frameworks.\nlink: https://arxiv.org/abs/2512.10980v1\n"}}
{"custom_id": "2512.09938v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Blockchain-Anchored Audit Trail Model for Transparent Inter-Operator Settlement\nsummary: The telecommunications and financial services industries face substantial challenges in inter-operator settlement processes, characterized by extended reconciliation cycles, high transaction costs, and limited real-time transparency. Traditional settlement mechanisms rely on multiple intermediaries and manual procedures, resulting in settlement periods exceeding 120 days with operational costs consuming approximately 5 percent of total revenue. This research presents a blockchain-anchored audit trail model enabling transparent, immutable, and automated inter-operator settlement. The framework leverages distributed ledger technology, smart contract automation, and cryptographic verification to establish a unified, tamper-proof transaction record. Empirical evaluation demonstrates 87 percent reduction in transaction fees, settlement cycle compression from 120 days to 3 minutes, and 100 percent audit trail integrity. Smart contract automation reduces manual intervention by 92 percent and eliminates 88 percent of settlement disputes. Market analysis indicates institutional adoption accelerated from 8 percent in 2020 to 52 percent by April 2024, with projected industry investment reaching 9.2 billion USD annually. The framework addresses scalability (12,000 transactions per second), interoperability, and regulatory compliance across multiple jurisdictions.\nlink: https://arxiv.org/abs/2512.09938v1\n"}}
{"custom_id": "2512.10979v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Seamless Transitions: A Comprehensive Review of Live Migration Technologies\nsummary: Live migration, a technology enabling seamless transition of operational computational entities between various hosts while preserving continuous functionality and client connectivity, has been the subject of extensive research. However, existing reviews often overlook critical technical aspects and practical challenges integral to the usage of live migration techniques in real-world scenarios. This work bridges this gap by integrating the aspects explored in existing reviews together with a comprehensive analysis of live migration technologies across multiple dimensions, with focus on migration techniques, migration units, and infrastructure characteristics. Despite efforts to make live migration widely accessible, its reliance on multiple system factors can create challenges. In certain cases, the complexities and resource demands outweigh the benefits, making its implementation hard to justify. The focus of this work is mainly on container based and virtual machine-based migration technologies, examining the current state of the art and the disparity in adoption between these two approaches. Furthermore, this work explores the impact of migration objectives and operational constraints on the usability and efficacy of existing technologies. By outlining current technical challenges and providing guidelines for future research and development directions, this work serves a dual purpose: first, to equip enthusiasts with a valuable resource on live migration, and second, to contribute to the advancement of live migration technologies and their practical implementation across diverse computing environments.\nlink: https://arxiv.org/abs/2512.10979v1\n"}}
{"custom_id": "2512.04129v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Tipping the Dominos: Topology-Aware Multi-Hop Attacks on LLM-Based Multi-Agent Systems\nsummary: LLM-based multi-agent systems (MASs) have reshaped the digital landscape with their emergent coordination and problem-solving capabilities. However, current security evaluations of MASs are still confined to limited attack scenarios, leaving their security issues unclear and likely underestimated. To fill this gap, we propose TOMA, a topology-aware multi-hop attack scheme targeting MASs. By optimizing the propagation of contamination within the MAS topology and controlling the multi-hop diffusion of adversarial payloads originating from the environment, TOMA unveils new and effective attack vectors without requiring privileged access or direct agent manipulation. Experiments demonstrate attack success rates ranging from 40% to 78% across three state-of-the-art MAS architectures: \\textsc{Magentic-One}, \\textsc{LangManus}, and \\textsc{OWL}, and five representative topologies, revealing intrinsic MAS vulnerabilities that may be overlooked by existing research. Inspired by these findings, we propose a conceptual defense framework based on topology trust, and prototype experiments show its effectiveness in blocking 94.8% of adaptive and composite attacks.\nlink: https://arxiv.org/abs/2512.04129v1\n"}}
{"custom_id": "2512.10977v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Agentic Operator Generation for ML ASICs\nsummary: We present TritorX, an agentic AI system designed to generate functionally correct Triton PyTorch ATen kernels at scale for emerging accelerator platforms. TritorX integrates open-source large language models with a custom linter, JIT compilation, and a PyTorch OpInfo-based test harness. This pipeline is compatible with both real Meta Training and Inference Accelerator (MTIA) silicon and in hardware simulation environments for next-generation devices. In contrast to previous kernel-generation approaches that prioritize performance for a limited set of high-usage kernels, TritorX prioritizes coverage. Our system emphasizes correctness and generality across the entire operator set, including diverse data types, shapes, and argument patterns. In our experiments, TritorX successfully generated kernels and wrappers for 481 unique ATen operators that pass all corresponding PyTorch OpInfo tests (over 20,000 in total). TritorX paves the way for overnight generation of complete PyTorch ATen backends for new accelerator platforms.\nlink: https://arxiv.org/abs/2512.10977v1\n"}}
{"custom_id": "2512.10974v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: An Efficient Approach for Energy Conservation in Cloud Computing Environment\nsummary: Recent trends of technology have explored a numerous applications of cloud services, which require a significant amount of energy. In the present scenario, most of the energy sources are limited and have a greenhouse effect on the environment. Therefore, it is the need of the hour that the energy consumed by the cloud service providers must be reduced and it is a great challenge to the research community to develop energy-efficient algorithms. To design the same, some researchers tried to maximize the average resource utilization, whereas some researchers tried to minimize the makespan. However, they have not considered different types of resources that are present in the physical machines. In this paper, we propose a task scheduling algorithm, which tries to improve utilization of resources (like CPU, disk, I/O) explicitly, which in turn increases the utilization of active resources. For this, the proposed algorithm uses a fitness value, which is a function of CPU, disk and I/O utilization, and processing time of the task. To demonstrate the performance of the proposed algorithm, extensive simulations are performed on both proposed algorithm and existing algorithm MaxUtil using synthetic datasets. From the simulation results, it can be observed that the proposed algorithm is a better energy-efficient algorithm and consumes less energy than the MaxUtil algorithm.\nlink: https://arxiv.org/abs/2512.10974v1\n"}}
{"custom_id": "2505.02828v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review\nsummary: Explainable Artificial Intelligence (XAI) has emerged as a pillar of Trustworthy AI and aims to bring transparency in complex models that are opaque by nature. Despite the benefits of incorporating explanations in models, an urgent need is found in addressing the privacy concerns of providing this additional information to end users. In this article, we conduct a scoping review of existing literature to elicit details on the conflict between privacy and explainability. Using the standard methodology for scoping review, we extracted 57 articles from 1,943 studies published from January 2019 to December 2024. The review addresses 3 research questions to present readers with more understanding of the topic: (1) what are the privacy risks of releasing explanations in AI systems? (2) what current methods have researchers employed to achieve privacy preservation in XAI systems? (3) what constitutes a privacy preserving explanation? Based on the knowledge synthesized from the selected studies, we categorize the privacy risks and preservation methods in XAI and propose the characteristics of privacy preserving explanations to aid researchers and practitioners in understanding the requirements of XAI that is privacy compliant. Lastly, we identify the challenges in balancing privacy with other system desiderata and provide recommendations for achieving privacy preserving XAI. We expect that this review will shed light on the complex relationship of privacy and explainability, both being the fundamental principles of Trustworthy AI.\nlink: https://arxiv.org/abs/2505.02828v3\n"}}
{"custom_id": "2512.03121v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Lost in Modality: Evaluating the Effectiveness of Text-Based Membership Inference Attacks on Large Multimodal Models\nsummary: Large Multimodal Language Models (MLLMs) are emerging as one of the foundational tools in an expanding range of applications. Consequently, understanding training-data leakage in these systems is increasingly critical. Log-probability-based membership inference attacks (MIAs) have become a widely adopted approach for assessing data exposure in large language models (LLMs), yet their effect in MLLMs remains unclear. We present the first comprehensive evaluation of extending these text-based MIA methods to multimodal settings. Our experiments under vision-and-text (V+T) and text-only (T-only) conditions across the DeepSeek-VL and InternVL model families show that in in-distribution settings, logit-based MIAs perform comparably across configurations, with a slight V+T advantage. Conversely, in out-of-distribution settings, visual inputs act as regularizers, effectively masking membership signals.\nlink: https://arxiv.org/abs/2512.03121v1\n"}}
{"custom_id": "2512.04120v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Towards Contextual Sensitive Data Detection\nsummary: The emergence of open data portals necessitates more attention to protecting sensitive data before datasets get published and exchanged. While an abundance of methods for suppressing sensitive data exist, the conceptualization of sensitive data and methods to detect it, focus particularly on personal data that, if disclosed, may be harmful or violate privacy. We observe the need for refining and broadening our definitions of sensitive data, and argue that the sensitivity of data depends on its context. Based on this definition, we introduce two mechanisms for contextual sensitive data detection that consider the broader context of a dataset at hand. First, we introduce type contextualization, which first detects the semantic type of particular data values, then considers the overall context of the data values within the dataset or document. Second, we introduce domain contextualization which determines sensitivity of a given dataset in the broader context based on the retrieval of relevant rules from documents that specify data sensitivity (e.g., data topic and geographic origin). Experiments with these mechanisms, assisted by large language models (LLMs), confirm that: 1) type-contextualization significantly reduces the number of false positives for type-based sensitive data detection and reaches a recall of 94% compared to 63% with commercial tools, and 2) domain-contextualization leveraging sensitivity rule retrieval is effective for context-grounded sensitive data detection in non-standard data domains such as humanitarian datasets. Evaluation with humanitarian data experts also reveals that context-grounded LLM explanations provide useful guidance in manual data auditing processes, improving consistency. We open-source mechanisms and annotated datasets for contextual sensitive data detection at https://github.com/trl-lab/sensitive-data-detection.\nlink: https://arxiv.org/abs/2512.04120v1\n"}}
{"custom_id": "2512.03100v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Ensemble Privacy Defense for Knowledge-Intensive LLMs against Membership Inference Attacks\nsummary: Retrieval-Augmented Generation (RAG) and Supervised Finetuning (SFT) have become the predominant paradigms for equipping Large Language Models (LLMs) with external knowledge for diverse, knowledge-intensive tasks. However, while such knowledge injection improves performance, it also exposes new attack surfaces. Membership Inference Attacks (MIAs), which aim to determine whether a given data sample was included in a model's training set, pose serious threats to privacy and trust in sensitive domains. To this end, we first systematically evaluate the vulnerability of RAG- and SFT-based LLMs to various MIAs. Then, to address the privacy risk, we further introduce a novel, model-agnostic defense framework, Ensemble Privacy Defense (EPD), which aggregates and evaluates the outputs of a knowledge-injected LLM, a base LLM, and a dedicated judge model to enhance resistance against MIAs. Comprehensive experiments show that, on average, EPD reduces MIA success by up to 27.8\\% for SFT and 526.3\\% for RAG compared to inference-time baseline, while maintaining answer quality.\nlink: https://arxiv.org/abs/2512.03100v1\n"}}
{"custom_id": "2512.16928v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Dion2: A Simple Method to Shrink Matrix in Muon\nsummary: The Muon optimizer enjoys strong empirical performance and theoretical grounding. However, the super-linear cost of its orthonormalization step introduces increasing overhead with scale. To alleviate this cost, several works have attempted to reduce the size of the matrix entering the orthonormalization step. We introduce Dion2, a much simpler method for shrinking the matrix involved in Muon's computation compared to prior approaches. At a high level, Dion2 selects a fraction of rows or columns at each iteration and orthonormalizes only those. This sampling procedure makes the update sparse, reducing both computation and communication costs which in turn improves the scalability of Muon.\nlink: https://arxiv.org/abs/2512.16928v1\n"}}
{"custom_id": "2512.02087v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: A survey about Hidden Subgroup Problem from a mathematical and cryptographic perspective\nsummary: We provide a survey on the Hidden Subgroup Problem (HSP), which plays an important role in studying the security of public-key cryptosystems. We first review the abelian case, where Kitaev's algorithm yields an efficient quantum solution to the HSP, recalling how classical problems (such as order finding, integer factorization, and discrete logarithm) can be formulated as abelian HSP instances. We then examine the current state of the art for non-abelian HSP, where no general efficient quantum solution is known, focusing on some relevant groups including dihedral group (connected to the shortest vector problem), symmetric groups (connected to the graph isomorphism problem), and semidirect product constructions (connected, in a special case, to the code equivalence problem). We also describe the main techniques for addressing the HSP in non-abelian cases, namely Fourier sampling and the black-box approach. Throughout the paper, we highlight the mathematical notions required and exploited in this context, providing a cryptography-oriented perspective.\nlink: https://arxiv.org/abs/2512.02087v1\n"}}
{"custom_id": "2512.03097v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Many-to-One Adversarial Consensus: Exposing Multi-Agent Collusion Risks in AI-Based Healthcare\nsummary: The integration of large language models (LLMs) into healthcare IoT systems promises faster decisions and improved medical support. LLMs are also deployed as multi-agent teams to assist AI doctors by debating, voting, or advising on decisions. However, when multiple assistant agents interact, coordinated adversaries can collude to create false consensus, pushing an AI doctor toward harmful prescriptions. We develop an experimental framework with scripted and unscripted doctor agents, adversarial assistants, and a verifier agent that checks decisions against clinical guidelines. Using 50 representative clinical questions, we find that collusion drives the Attack Success Rate (ASR) and Harmful Recommendation Rates (HRR) up to 100% in unprotected systems. In contrast, the verifier agent restores 100% accuracy by blocking adversarial consensus. This work provides the first systematic evidence of collusion risk in AI healthcare and demonstrates a practical, lightweight defence that ensures guideline fidelity.\nlink: https://arxiv.org/abs/2512.03097v1\n"}}
{"custom_id": "2512.02082v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Deterministic Random Bit Generators Based on Ascon for Embedded Systems\nsummary: As the Deterministic Random Bit Generator (DRBG) serves as a fundamental component in random number generation and cryptographic applications, its performance and security are particularly critical in resource-constrained embedded systems, where memory capacity and computational efficiency are limited. Establishing a high-performance and secure DRBG is therefore an important issue for embedded system design. Furthermore, the National Institute of Standards and Technology (NIST) established the Ascon lightweight cryptographic standard in August 2025, which is suitable for use in resource-limited embedded environments. Therefore, this study revises the DRBG standard and proposes three Ascon-driven constructions: the Ascon-Driven Hash-Based DRBG, the Ascon-Driven keyed-Hash Message Authentication Code (HMAC) DRBG, and the Ascon-Driven Counter-mode (CTR) DRBG. In the experiments, these methods are implemented on a Raspberry Pi platform. The experimental results demonstrate that the proposed approaches achieve higher computational efficiency and lower memory usage compared with existing DRBG implementations, making them suitable for deployment in embedded systems.\nlink: https://arxiv.org/abs/2512.02082v1\n"}}
{"custom_id": "2512.03462v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: A Hybrid Deep Learning and Anomaly Detection Framework for Real-Time Malicious URL Classification\nsummary: Malicious URLs remain a primary vector for phishing, malware, and cyberthreats. This study proposes a hybrid deep learning framework combining \\texttt{HashingVectorizer} n-gram analysis, SMOTE balancing, Isolation Forest anomaly filtering, and a lightweight neural network classifier for real-time URL classification. The multi-stage pipeline processes URLs from open-source repositories with statistical features (length, dot count, entropy), achieving $O(NL + EBdh)$ training complexity and a 20\\,ms prediction latency. Empirical evaluation yields 96.4\\% accuracy, 95.4\\% F1-score, and 97.3\\% ROC-AUC, outperforming CNN (94.8\\%) and SVM baselines with a $50\\!\\times$--$100\\!\\times$ speedup (Table~\\ref{tab:comp-complexity}). A multilingual Tkinter GUI (Arabic/English/French) enables real-time threat assessment with clipboard integration. The framework demonstrates superior scalability and resilience against obfuscated URL patterns.\nlink: https://arxiv.org/abs/2512.03462v1\n"}}
{"custom_id": "2512.02069v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Large Language Model based Smart Contract Auditing with LLMBugScanner\nsummary: This paper presents LLMBugScanner, a large language model (LLM) based framework for smart contract vulnerability detection using fine-tuning and ensemble learning. Smart contract auditing presents several challenges for LLMs: different pretrained models exhibit varying reasoning abilities, and no single model performs consistently well across all vulnerability types or contract structures. These limitations persist even after fine-tuning individual LLMs.\n  To address these challenges, LLMBugScanner combines domain knowledge adaptation with ensemble reasoning to improve robustness and generalization. Through domain knowledge adaptation, we fine-tune LLMs on complementary datasets to capture both general code semantics and instruction-guided vulnerability reasoning, using parameter-efficient tuning to reduce computational cost. Through ensemble reasoning, we leverage the complementary strengths of multiple LLMs and apply a consensus-based conflict resolution strategy to produce more reliable vulnerability assessments.\n  We conduct extensive experiments across multiple popular LLMs and compare LLMBugScanner with both pretrained and fine-tuned individual models. Results show that LLMBugScanner achieves consistent accuracy improvements and stronger generalization, demonstrating that it provides a principled, cost-effective, and extensible framework for smart contract auditing.\nlink: https://arxiv.org/abs/2512.02069v1\n"}}
{"custom_id": "2512.03089v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Password-Activated Shutdown Protocols for Misaligned Frontier Agents\nsummary: Frontier AI developers may fail to align or control highly-capable AI agents. In many cases, it could be useful to have emergency shutdown mechanisms which effectively prevent misaligned agents from carrying out harmful actions in the world. We introduce password-activated shutdown protocols (PAS protocols) -- methods for designing frontier agents to implement a safe shutdown protocol when given a password. We motivate PAS protocols by describing intuitive use-cases in which they mitigate risks from misaligned systems that subvert other control efforts, for instance, by disabling automated monitors or self-exfiltrating to external data centres. PAS protocols supplement other safety efforts, such as alignment fine-tuning or monitoring, contributing to defence-in-depth against AI risk. We provide a concrete demonstration in SHADE-Arena, a benchmark for AI monitoring and subversion capabilities, in which PAS protocols supplement monitoring to increase safety with little cost to performance. Next, PAS protocols should be robust to malicious actors who want to bypass shutdown. Therefore, we conduct a red-team blue-team game between the developers (blue-team), who must implement a robust PAS protocol, and a red-team trying to subvert the protocol. We conduct experiments in a code-generation setting, finding that there are effective strategies for the red-team, such as using another model to filter inputs, or fine-tuning the model to prevent shutdown behaviour. We then outline key challenges to implementing PAS protocols in real-life systems, including: security considerations of the password and decisions regarding when, and in which systems, to use them. PAS protocols are an intuitive mechanism for increasing the safety of frontier AI. We encourage developers to consider implementing PAS protocols prior to internal deployment of particularly dangerous systems to reduce loss-of-control risks.\nlink: https://arxiv.org/abs/2512.03089v1\n"}}
{"custom_id": "2512.03088v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: From Oracle Choice to Oracle Lock-In: An Exploratory Study on Blockchain Oracles Supplier Selection\nsummary: As data is an essential asset for any Web3 application, selecting an oracle is a critical decision for its success. To date, academic research has mainly focused on improving oracle technology and internal economics, while the drivers of oracle choice on the client side remain largely unexplored. This study fills this gap by gathering insights from leading Web3 protocols, uncovering their rationale for oracle selection and their preferences when deciding whether to outsource or internalize data request mechanisms. The collected data covers more than 55% of the DeFi market cap and is obtained exclusively by protocol executives, board members, or delegates. Insights support the view that protocol choices are tied to technological dependencies, where immutability of smart contracts amplifies lock-in, preventing agile switching among data providers. Furthermore, when viable third-party solutions exist, protocols overwhelmingly prefer outsourcing rather than building and maintaining internal oracle mechanisms.\nlink: https://arxiv.org/abs/2512.03088v1\n"}}
{"custom_id": "2512.02062v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Superpixel Attack: Enhancing Black-box Adversarial Attack with Image-driven Division Areas\nsummary: Deep learning models are used in safety-critical tasks such as automated driving and face recognition. However, small perturbations in the model input can significantly change the predictions. Adversarial attacks are used to identify small perturbations that can lead to misclassifications. More powerful black-box adversarial attacks are required to develop more effective defenses. A promising approach to black-box adversarial attacks is to repeat the process of extracting a specific image area and changing the perturbations added to it. Existing attacks adopt simple rectangles as the areas where perturbations are changed in a single iteration. We propose applying superpixels instead, which achieve a good balance between color variance and compactness. We also propose a new search method, versatile search, and a novel attack method, Superpixel Attack, which applies superpixels and performs versatile search. Superpixel Attack improves attack success rates by an average of 2.10% compared with existing attacks. Most models used in this study are robust against adversarial attacks, and this improvement is significant for black-box adversarial attacks. The code is avilable at https://github.com/oe1307/SuperpixelAttack.git.\nlink: https://arxiv.org/abs/2512.02062v1\n"}}
{"custom_id": "2512.05134v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models\nsummary: Diffusion models deliver high-fidelity synthesis but remain slow due to iterative sampling. We empirically observe there exists feature invariance in deterministic sampling, and present InvarDiff, a training-free acceleration method that exploits the relative temporal invariance across timestep-scale and layer-scale. From a few deterministic runs, we compute a per-timestep, per-layer, per-module binary cache plan matrix and use a re-sampling correction to avoid drift when consecutive caches occur. Using quantile-based change metrics, this matrix specifies which module at which step is reused rather than recomputed. The same invariance criterion is applied at the step scale to enable cross-timestep caching, deciding whether an entire step can reuse cached results. During inference, InvarDiff performs step-first and layer-wise caching guided by this matrix. When applied to DiT and FLUX, our approach reduces redundant compute while preserving fidelity. Experiments show that InvarDiff achieves $2$-$3\\times$ end-to-end speed-ups with minimal impact on standard quality metrics. Qualitatively, we observe almost no degradation in visual quality compared with full computations.\nlink: https://arxiv.org/abs/2512.05134v1\n"}}
{"custom_id": "2512.16926v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Fixed-Priority and EDF Schedules for ROS2 Graphs on Uniprocessor\nsummary: This paper addresses limitations of current scheduling methods in the Robot Operating System (ROS)2, focusing on scheduling tasks beyond simple chains and analyzing arbitrary Directed Acyclic Graphs (DAGs). While previous research has focused mostly on chain-based scheduling with ad-hoc response time analyses, we propose a novel approach using the events executor to implement fixed-job-level-priority schedulers for arbitrary ROS2 graphs on uniprocessor systems. We demonstrate that ROS 2 applications can be abstracted as forests of trees, enabling the mapping of ROS 2 applications to traditional real-time DAG task models. Our usage of the events executor requires a special implementation of the events queue and a communication middleware that supports LIFO-ordered message delivery, features not yet standard in ROS2. We show that our implementation generates the same schedules as a conventional fixed-priority DAG task scheduler, in spite of lacking access to the precedence information that usually is required. This further closes the gap between established real-time systems theory and ROS2 scheduling analyses.\nlink: https://arxiv.org/abs/2512.16926v1\n"}}
{"custom_id": "2512.09934v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: IoTEdu: Access Control, Detection, and Automatic Incident Response in Academic IoT Networks\nsummary: The growing presence of IoT devices in academic environments has increased operational complexity and exposed security weaknesses, especially in academic institutions without unified policies for registration, monitoring, and incident response involving IoT. This work presents IoTEdu, an integrated platform that combines access control, incident detection, and automatic blocking of IoT devices. The solution was evaluated in a controlled environment with simulated attacks, achieving an average time of 28.6 seconds between detection and blocking. The results show a reduction in manual intervention, standardization of responses, and unification of the processes of registration, monitoring, and incident response.\nlink: https://arxiv.org/abs/2512.09934v1\n"}}
{"custom_id": "2512.04106v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Retrieval-Augmented Few-Shot Prompting Versus Fine-Tuning for Code Vulnerability Detection\nsummary: Few-shot prompting has emerged as a practical alternative to fine-tuning for leveraging the capabilities of large language models (LLMs) in specialized tasks. However, its effectiveness depends heavily on the selection and quality of in-context examples, particularly in complex domains. In this work, we examine retrieval-augmented prompting as a strategy to improve few-shot performance in code vulnerability detection, where the goal is to identify one or more security-relevant weaknesses present in a given code snippet from a predefined set of vulnerability categories. We perform a systematic evaluation using the Gemini-1.5-Flash model across three approaches: (1) standard few-shot prompting with randomly selected examples, (2) retrieval-augmented prompting using semantically similar examples, and (3) retrieval-based labeling, which assigns labels based on retrieved examples without model inference. Our results show that retrieval-augmented prompting consistently outperforms the other prompting strategies. At 20 shots, it achieves an F1 score of 74.05% and a partial match accuracy of 83.90%. We further compare this approach against zero-shot prompting and several fine-tuned models, including Gemini-1.5-Flash and smaller open-source models such as DistilBERT, DistilGPT2, and CodeBERT. Retrieval-augmented prompting outperforms both zero-shot (F1 score: 36.35%, partial match accuracy: 20.30%) and fine-tuned Gemini (F1 score: 59.31%, partial match accuracy: 53.10%), while avoiding the training time and cost associated with model fine-tuning. On the other hand, fine-tuning CodeBERT yields higher performance (F1 score: 91.22%, partial match accuracy: 91.30%) but requires additional training, maintenance effort, and resources.\nlink: https://arxiv.org/abs/2512.04106v1\n"}}
{"custom_id": "2512.03079v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Watermarks for Embeddings-as-a-Service Large Language Models\nsummary: Large Language Models (LLMs) have demonstrated exceptional capabilities in natural language understanding and generation. Based on these LLMs, businesses have started to provide Embeddings-as-a-Service (EaaS), offering feature extraction capabilities (in the form of text embeddings) that benefit downstream natural language processing tasks. However, prior research has demonstrated that EaaS is vulnerable to imitation attacks, where an attacker clones the service's model in a black-box manner without access to the model's internal workings. In response, watermarks have been added to the text embeddings to protect the intellectual property of EaaS providers by allowing them to check for model ownership. This thesis focuses on defending against imitation attacks by investigating EaaS watermarks. To achieve this goal, we unveil novel attacks and propose and validate new watermarking techniques.\n  Firstly, we show that existing EaaS watermarks can be removed through paraphrasing the input text when attackers clone the model during imitation attacks. Our study illustrates that paraphrasing can effectively bypass current state-of-the-art EaaS watermarks across various attack setups (including different paraphrasing techniques and models) and datasets in most instances. This demonstrates a new vulnerability in recent EaaS watermarking techniques.\n  Subsequently, as a countermeasure, we propose a novel watermarking technique, WET (Watermarking EaaS with Linear Transformation), which employs linear transformation of the embeddings. Watermark verification is conducted by applying a reverse transformation and comparing the similarity between recovered and original embeddings. We demonstrate its robustness against paraphrasing attacks with near-perfect verifiability. We conduct detailed ablation studies to assess the significance of each component and hyperparameter in WET.\nlink: https://arxiv.org/abs/2512.03079v1\n"}}
{"custom_id": "2512.07866v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Command & Control (C2) Traffic Detection Via Algorithm Generated Domain (Dga) Classification Using Deep Learning And Natural Language Processing\nsummary: The sophistication of modern malware, specifically regarding communication with Command and Control (C2) servers, has rendered static blacklist-based defenses obsolete. The use of Domain Generation Algorithms (DGA) allows attackers to generate thousands of dynamic addresses daily, hindering blocking by traditional firewalls. This paper aims to propose and evaluate a method for detecting DGA domains using Deep Learning and Natural Language Processing (NLP) techniques. The methodology consisted of collecting a hybrid database containing 50,000 legitimate and 50,000 malicious domains, followed by the extraction of lexical features and the training of a Recurrent Neural Network (LSTM). Results demonstrated that while statistical entropy analysis is effective for simple DGAs, the Neural Network approach presents superiority in detecting complex patterns, reaching 97.2% accuracy and reducing the false positive rate in ambiguous lawful traffic scenarios.\nlink: https://arxiv.org/abs/2512.07866v1\n"}}
