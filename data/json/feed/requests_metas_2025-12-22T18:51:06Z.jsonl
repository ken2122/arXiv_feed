{"custom_id": "2512.19606v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference\nsummary: RAPID-LLM is a unified performance modeling framework for large language model (LLM) training and inference on GPU clusters. It couples a DeepFlow-based frontend that generates hardware-aware, operator-level Chakra execution traces from an abstract LLM specification (model shape, batch/sequence settings, training vs. inference, and hybrid parallelism choices) with an extended Astra-Sim backend that executes those traces on explicit multi-dimensional network topologies with congestion-aware routing and support for degraded and faulty links. The frontend assigns per-operator latency using a tile-based model that accounts for SM under-utilization and multi-level memory traffic (SRAM/ L2/ HBM), and prunes memory-infeasible configurations using an activation-liveness traversal under recomputation, parallelism and ZeRO/FDSP sharding policies.\n  Across A100-based validation cases, RAPID-LLM predicts Llama inference step latency and GPT-scale training time per batch within 10.4\\% relative to published measurements, and matches ns-3 packet-level results within 8\\% on representative communication workloads. Case studies demonstrate how RAPID-LLM enables fast, exhaustive sweeps over hybrid-parallel configurations, quantifies sensitivity to soft link faults under realistic routing and congestion, and evaluates hypothetical GPU design variants including HBM bandwidth throttling effects.\nlink: https://arxiv.org/abs/2512.19606v1\n"}}
{"custom_id": "2506.05594v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: SoK: Are Watermarks in LLMs Ready for Deployment?\nsummary: Large Language Models (LLMs) have transformed natural language processing, demonstrating impressive capabilities across diverse tasks. However, deploying these models introduces critical risks related to intellectual property violations and potential misuse, particularly as adversaries can imitate these models to steal services or generate misleading outputs. We specifically focus on model stealing attacks, as they are highly relevant to proprietary LLMs and pose a serious threat to their security, revenue, and ethical deployment. While various watermarking techniques have emerged to mitigate these risks, it remains unclear how far the community and industry have progressed in developing and deploying watermarks in LLMs.\n  To bridge this gap, we aim to develop a comprehensive systematization for watermarks in LLMs by 1) presenting a detailed taxonomy for watermarks in LLMs, 2) proposing a novel intellectual property classifier to explore the effectiveness and impacts of watermarks on LLMs under both attack and attack-free environments, 3) analyzing the limitations of existing watermarks in LLMs, and 4) discussing practical challenges and potential future directions for watermarks in LLMs. Through extensive experiments, we show that despite promising research outcomes and significant attention from leading companies and community to deploy watermarks, these techniques have yet to reach their full potential in real-world applications due to their unfavorable impacts on model utility of LLMs and downstream tasks. Our findings provide an insightful understanding of watermarks in LLMs, highlighting the need for practical watermarks solutions tailored to LLM deployment.\nlink: https://arxiv.org/abs/2506.05594v2\n"}}
{"custom_id": "2504.06408v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Parallel GPU-Enabled Algorithms for SpGEMM on Arbitrary Semirings with Hybrid Communication\nsummary: Sparse General Matrix Multiply (SpGEMM) is key for various High-Performance Computing (HPC) applications such as genomics and graph analytics. Using the semiring abstraction, many algorithms can be formulated as SpGEMM, allowing redefinition of addition, multiplication, and numeric types. Today large input matrices require distributed memory parallelism to avoid disk I/O, and modern HPC machines with GPUs can greatly accelerate linear algebra computation. In this paper, we implement a GPU-based distributed-memory SpGEMM routine on top of the CombBLAS library. Our implementation achieves a speedup of over 2x compared to the CPU-only CombBLAS implementation and up to 3x compared to PETSc for large input matrices. Furthermore, we note that communication between processes can be optimized by either direct host-to-host or device-to-device communication, depending on the message size. To exploit this, we introduce a hybrid communication scheme that dynamically switches data paths depending on the message size, thus improving runtimes in communication-bound scenarios.\nlink: https://arxiv.org/abs/2504.06408v2\n"}}
{"custom_id": "2512.19414v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: From Retrieval to Reasoning: A Framework for Cyber Threat Intelligence NER with Explicit and Adaptive Instructions\nsummary: The automation of Cyber Threat Intelligence (CTI) relies heavily on Named Entity Recognition (NER) to extract critical entities from unstructured text. Currently, Large Language Models (LLMs) primarily address this task through retrieval-based In-Context Learning (ICL). This paper analyzes this mainstream paradigm, revealing a fundamental flaw: its success stems not from global semantic similarity but largely from the incidental overlap of entity types within retrieved examples. This exposes the limitations of relying on unreliable implicit induction. To address this, we propose TTPrompt, a framework shifting from implicit induction to explicit instruction. TTPrompt maps the core concepts of CTI's Tactics, Techniques, and Procedures (TTPs) into an instruction hierarchy: formulating task definitions as Tactics, guiding strategies as Techniques, and annotation guidelines as Procedures. Furthermore, to handle the adaptability challenge of static guidelines, we introduce Feedback-driven Instruction Refinement (FIR). FIR enables LLMs to self-refine guidelines by learning from errors on minimal labeled data, adapting to distinct annotation dialects. Experiments on five CTI NER benchmarks demonstrate that TTPrompt consistently surpasses retrieval-based baselines. Notably, with refinement on just 1% of training data, it rivals models fine-tuned on the full dataset. For instance, on LADDER, its Micro F1 of 71.96% approaches the fine-tuned baseline, and on the complex CTINexus, its Macro F1 exceeds the fine-tuned ACLM model by 10.91%.\nlink: https://arxiv.org/abs/2512.19414v1\n"}}
{"custom_id": "2501.10190v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Temporal Causal Reasoning with (Non-Recursive) Structural Equation Models\nsummary: Structural Equation Models (SEM) are the standard approach to representing causal dependencies between variables in causal models. In this paper we propose a new interpretation of SEMs when reasoning about Actual Causality, in which SEMs are viewed as mechanisms transforming the dynamics of exogenous variables into the dynamics of endogenous variables. This allows us to combine counterfactual causal reasoning with existing temporal logic formalisms, and to introduce a temporal logic, CPLTL, for causal reasoning about such structures. We show that the standard restriction to so-called \\textit{recursive} models (with no cycles in the dependency graph) is not necessary in our approach, allowing us to reason about mutually dependent processes and feedback loops. Finally, we introduce new notions of model equivalence for temporal causal models, and show that CPLTL has an efficient model-checking procedure.\nlink: https://arxiv.org/abs/2501.10190v2\n"}}
{"custom_id": "2406.05491v4", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: One Perturbation is Enough: On Generating Universal Adversarial Perturbations against Vision-Language Pre-training Models\nsummary: Vision-Language Pre-training (VLP) models have exhibited unprecedented capability in many applications by taking full advantage of the multimodal alignment. However, previous studies have shown they are vulnerable to maliciously crafted adversarial samples. Despite recent success, these methods are generally instance-specific and require generating perturbations for each input sample. In this paper, we reveal that VLP models are also vulnerable to the instance-agnostic universal adversarial perturbation (UAP). Specifically, we design a novel Contrastive-training Perturbation Generator with Cross-modal conditions (C-PGC) to achieve the attack. In light that the pivotal multimodal alignment is achieved through the advanced contrastive learning technique, we devise to turn this powerful weapon against themselves, i.e., employ a malicious version of contrastive learning to train the C-PGC based on our carefully crafted positive and negative image-text pairs for essentially destroying the alignment relationship learned by VLP models. Besides, C-PGC fully utilizes the characteristics of Vision-and-Language (V+L) scenarios by incorporating both unimodal and cross-modal information as effective guidance. Extensive experiments show that C-PGC successfully forces adversarial samples to move away from their original area in the VLP model's feature space, thus essentially enhancing attacks across various victim models and V+L tasks. The GitHub repository is available at https://github.com/ffhibnese/CPGC_VLP_Universal_Attacks.\nlink: https://arxiv.org/abs/2406.05491v4\n"}}
{"custom_id": "2506.17231v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs\nsummary: As the scale and complexity of jailbreaking attacks on large language models (LLMs) continue to escalate, their efficiency and practical applicability are constrained, posing a profound challenge to LLM security. Jailbreaking techniques have advanced from manual prompt engineering to automated methodologies. Recent advances have automated jailbreaking approaches that harness LLMs to generate jailbreak instructions and adversarial examples, delivering encouraging results. Nevertheless, these methods universally include an LLM generation phase, which, due to the complexities of deploying and reasoning with LLMs, impedes effective implementation and broader adoption. To mitigate this issue, we introduce \\textbf{Adversarial Prompt Distillation}, an innovative framework that integrates masked language modeling, reinforcement learning, and dynamic temperature control to distill LLM jailbreaking prowess into smaller language models (SLMs). This methodology enables efficient, robust jailbreak attacks while maintaining high success rates and accommodating a broader range of application contexts. Empirical evaluations affirm the approach's superiority in attack efficacy, resource optimization, and cross-model versatility. Our research underscores the practicality of transferring jailbreak capabilities to SLMs, reveals inherent vulnerabilities in LLMs, and provides novel insights to advance LLM security investigations. Our code is available at: https://github.com/lxgem/Efficient_and_Stealthy_Jailbreak_Attacks_via_Adversarial_Prompt.\nlink: https://arxiv.org/abs/2506.17231v2\n"}}
{"custom_id": "2512.19342v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives\nsummary: Recommender systems are enablers of personalized content delivery, and therefore revenue, for many large companies. In the last decade, deep learning recommender models (DLRMs) are the de-facto standard in this field. The main bottleneck in DLRM inference is the lookup of sparse features across huge embedding tables, which are usually partitioned across the aggregate RAM of many nodes. In state-of-the-art recommender systems, the distributed lookup is implemented via irregular all-to-all (alltoallv) communication, and often presents the main bottleneck. Today, most related work sees this operation as a given; in addition, every collective is synchronous in nature. In this work, we propose a novel bounded lag synchronous (BLS) version of the alltoallv operation. The bound can be a parameter allowing slower processes to lag behind entire iterations before the fastest processes block. In special applications such as inference-only DLRM, the accuracy of the application is fully preserved. We implement BLS alltoallv in a new PyTorch Distributed backend and evaluate it with a BLS version of the reference DLRM code. We show that for well balanced, homogeneous-access DLRM runs our BLS technique does not offer notable advantages. But for unbalanced runs, e.g. runs with strongly irregular embedding table accesses or with delays across different processes, our BLS technique improves both the latency and throughput of inference-only DLRM. In the best-case scenario, the proposed reduced synchronisation can mask the delays across processes altogether.\nlink: https://arxiv.org/abs/2512.19342v1\n"}}
{"custom_id": "2511.20944v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Semantic Superiority vs. Forensic Efficiency: A Comparative Analysis of Deep Learning and Psycholinguistics for Business Email Compromise Detection\nsummary: Business Email Compromise (BEC) is a sophisticated social engineering threat that manipulates organizational hierarchies, leading to significant financial damage. According to the 2024 FBI Internet Crime Report, BEC accounts for over $2.9 billion in annual losses, presenting a massive economic asymmetry: the financial cost of a False Negative (fraud loss) exceeds the operational cost of a False Positive (manual review) by a ratio of approximately 5,480:1. This paper contrasts two detection paradigms: a Forensic Psycholinguistic Stream (CatBoost), which analyzes linguistic cues like urgency and authority with high interpretability, and a Semantic Stream (DistilBERT), which utilizes deep learning for contextual understanding. We evaluated both streams on a hybrid dataset (N=7,990) containing human-legitimate and AI-synthesized adversarial fraud. Benchmarked on Tesla T4 infrastructure, DistilBERT achieved near-perfect detection on synthetic threats (AUC >0.99, F1 =0.998) with acceptable real-time latency (7.4 ms). CatBoost achieved competitive detection (AUC =0.991, F1 =0.949) at 8.4x lower latency (0.8 ms) with negligible resource consumption. We conclude that while DistilBERT offers maximum accuracy for GPU-equipped organizations, CatBoost provides a viable, cost-effective alternative for edge deployments. Both approaches demonstrate a theoretical ROI exceeding 99.9% when optimized via cost-sensitive learning.\nlink: https://arxiv.org/abs/2511.20944v3\n"}}
{"custom_id": "2512.19326v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Simulations between Strongly Sublinear MPC and Node-Capacitated Clique\nsummary: We study how the strongly sublinear MPC model relates to the classic, graph-centric distributed models, focusing on the Node-Capacitated Clique (NCC), a bandwidth-parametrized generalization of the Congested Clique. In MPC, $M$ machines with per-machine memory $S$ hold a partition of the input graph, in NCC, each node knows its full neighborhood but can send/receive only a bounded number of $C$ words per round. We are particularly interested in the strongly sublinear regime where $S=C=n^\u03b4$ for some constant $0 < \u03b4<1$.\n  Our goal is determine when round-preserving simulations between these models are possible and when they are not, when total memory and total bandwidth $SM=nC$ in both models are matched, for different problem families and graph classes. On the positive side, we provide techniques that allow us to replicate the specific behavior regarding input representation, number of machines and local memory from one model to the other to obtain simulations with only constant overhead. On the negative side, we prove simulation impossibility results, which show that the limitations of our simulations are necessary.\nlink: https://arxiv.org/abs/2512.19326v1\n"}}
{"custom_id": "2512.19314v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Protecting Quantum Circuits Through Compiler-Resistant Obfuscation\nsummary: Quantum circuit obfuscation is becoming increasingly important to prevent theft and reverse engineering of quantum algorithms. As quantum computing advances, the need to protect the intellectual property contained in quantum circuits continues to grow. Existing methods often provide limited defense against structural and statistical analysis or introduce considerable overhead. In this paper, we propose a novel quantum obfuscation method that uses randomized U3 transformations to conceal circuit structure while preserving functionality. We implement and assess our approach on QASM circuits using Qiskit AER, achieving over 93\\% semantic accuracy with minimal runtime overhead. The method demonstrates strong resistance to reverse engineering and structural inference, making it a practical and effective approach for quantum software protection.\nlink: https://arxiv.org/abs/2512.19314v1\n"}}
{"custom_id": "2503.04521v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market\nsummary: The convergence of edge computing and Artificial Intelligence (AI) gives rise to Edge-AI, which enables the deployment of real-time AI applications at the network edge. A key research challenge in Edge-AI is edge inference acceleration, which aims to realize low-latency high-accuracy Deep Neural Network (DNN) inference by offloading partitioned inference tasks from end devices to edge servers. However, existing research has yet to adopt a practical Edge-AI market perspective, which would explore the personalized inference needs of AI users (e.g., inference accuracy, latency, and task complexity), the revenue incentives for AI service providers that offer edge inference services, and multi-stakeholder governance within a market-oriented context. To bridge this gap, we propose an Auction-based Edge Inference Pricing Mechanism (AERIA) for revenue maximization to tackle the multi-dimensional optimization problem of DNN model partition, edge inference pricing, and resource allocation. We develop a multi-exit device-edge synergistic inference scheme for on-demand DNN inference acceleration, and theoretically analyze the auction dynamics amongst the AI service providers, AI users and edge infrastructure provider. Owing to the strategic mechanism design via randomized consensus estimate and cost sharing techniques, the Edge-AI market attains several desirable properties. These include competitiveness in revenue maximization, incentive compatibility, and envy-freeness, which are crucial to maintain the effectiveness, truthfulness, and fairness in auction outcomes. Extensive simulations based on four representative DNN inference workloads demonstrate that AERIA significantly outperforms several state-of-the-art approaches in revenue maximization. This validates the efficacy of AERIA for on-demand DNN inference in the Edge-AI market.\nlink: https://arxiv.org/abs/2503.04521v2\n"}}
{"custom_id": "2512.19297v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models\nsummary: Low-Rank Adaptation (LoRA) has emerged as an efficient method for fine-tuning large language models (LLMs) and is widely adopted within the open-source community. However, the decentralized dissemination of LoRA adapters through platforms such as Hugging Face introduces novel security vulnerabilities: malicious adapters can be easily distributed and evade conventional oversight mechanisms. Despite these risks, backdoor attacks targeting LoRA-based fine-tuning remain relatively underexplored. Existing backdoor attack strategies are ill-suited to this setting, as they often rely on inaccessible training data, fail to account for the structural properties unique to LoRA, or suffer from high false trigger rates (FTR), thereby compromising their stealth. To address these challenges, we propose Causal-Guided Detoxify Backdoor Attack (CBA), a novel backdoor attack framework specifically designed for open-weight LoRA models. CBA operates without access to original training data and achieves high stealth through two key innovations: (1) a coverage-guided data generation pipeline that synthesizes task-aligned inputs via behavioral exploration, and (2) a causal-guided detoxification strategy that merges poisoned and clean adapters by preserving task-critical neurons. Unlike prior approaches, CBA enables post-training control over attack intensity through causal influence-based weight allocation, eliminating the need for repeated retraining. Evaluated across six LoRA models, CBA achieves high attack success rates while reducing FTR by 50-70\\% compared to baseline methods. Furthermore, it demonstrates enhanced resistance to state-of-the-art backdoor defenses, highlighting its stealth and robustness.\nlink: https://arxiv.org/abs/2512.19297v1\n"}}
{"custom_id": "2512.19286v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: GShield: Mitigating Poisoning Attacks in Federated Learning\nsummary: Federated Learning (FL) has recently emerged as a revolutionary approach to collaborative training Machine Learning models. In particular, it enables decentralized model training while preserving data privacy, but its distributed nature makes it highly vulnerable to a severe attack known as Data Poisoning. In such scenarios, malicious clients inject manipulated data into the training process, thereby degrading global model performance or causing targeted misclassification. In this paper, we present a novel defense mechanism called GShield, designed to detect and mitigate malicious and low-quality updates, especially under non-independent and identically distributed (non-IID) data scenarios. GShield operates by learning the distribution of benign gradients through clustering and Gaussian modeling during an initial round, enabling it to establish a reliable baseline of trusted client behavior. With this benign profile, GShield selectively aggregates only those updates that align with the expected gradient patterns, effectively isolating adversarial clients and preserving the integrity of the global model. An extensive experimental campaign demonstrates that our proposed defense significantly improves model robustness compared to the state-of-the-art methods while maintaining a high accuracy of performance across both tabular and image datasets. Furthermore, GShield improves the accuracy of the targeted class by 43\\% to 65\\% after detecting malicious and low-quality clients.\nlink: https://arxiv.org/abs/2512.19286v1\n"}}
{"custom_id": "2512.16280v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams\nsummary: Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation.\n  We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion.\nlink: https://arxiv.org/abs/2512.16280v2\n"}}
{"custom_id": "2512.19251v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Institutional Backing and Crypto Volatility: A Hybrid Framework for DeFi Stabilization\nsummary: Decentralized finance (DeFi) lacks centralized oversight, often resulting in heightened volatility. In contrast, centralized finance (CeFi) offers a more stable environment with institutional safeguards. Institutional backing can play a stabilizing role in a hybrid structure (HyFi), enhancing transparency, governance, and market discipline. This study investigates whether HyFi-like cryptocurrencies, those backed by institutions, exhibit lower price risk than fully decentralized counterparts. Using daily data for 18 major cryptocurrencies from January 2020 to November 2024, we estimate panel EGLS models with fixed, random, and dynamic specifications. Results show that HyFi-like assets consistently experience lower price risk, with this effect intensifying during periods of elevated market volatility. The negative interaction between HyFi status and market-wide volatility confirms their stabilizing role. Conversely, greater decentralization is strongly associated with increased volatility, particularly during periods of market stress. Robustness checks using quantile regressions and pre-/post-Terra Luna subsamples reinforce these findings, with stronger effects observed in high-volatility quantiles and post-crisis conditions. These results highlight the importance of institutional architecture in enhancing the resilience of digital asset markets.\nlink: https://arxiv.org/abs/2512.19251v1\n"}}
{"custom_id": "2512.19203v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Evaluating MCC for Low-Frequency Cyberattack Detection in Imbalanced Intrusion Detection Data\nsummary: In many real-world network environments, several types of cyberattacks occur at very low rates compared to benign traffic, making them difficult for intrusion detection systems (IDS) to detect reliably. This imbalance causes traditional evaluation metrics, such as accuracy, to often overstate model performance in these conditions, masking failures on minority attack classes that are most important in practice. In this paper, we evaluate a set of base and meta classifiers on low-traffic attacks in the CSE-CIC-IDS2018 dataset and compare their reliability in terms of accuracy and Matthews Correlation Coefficient (MCC). The results show that accuracy consistently inflates performance, while MCC provides a more accurate assessment of a classifier's performance across both majority and minority classes. Meta-classification methods, such as LogitBoost and AdaBoost, demonstrate more effective minority class detection when measured by MCC, revealing trends that accuracy fails to capture. These findings establish the need for imbalance-aware evaluation and make MCC a more trustworthy metric for IDS research involving low-traffic cyberattacks.\nlink: https://arxiv.org/abs/2512.19203v1\n"}}
{"custom_id": "2402.13081v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: IT Intrusion Detection Using Statistical Learning and Testbed Measurements\nsummary: We study automated intrusion detection in an IT infrastructure, specifically the problem of identifying the start of an attack, the type of attack, and the sequence of actions an attacker takes, based on continuous measurements from the infrastructure. We apply statistical learning methods, including Hidden Markov Model (HMM), Long Short-Term Memory (LSTM), and Random Forest Classifier (RFC) to map sequences of observations to sequences of predicted attack actions. In contrast to most related research, we have abundant data to train the models and evaluate their predictive power. The data comes from traces we generate on an in-house testbed where we run attacks against an emulated IT infrastructure. Central to our work is a machine-learning pipeline that maps measurements from a high-dimensional observation space to a space of low dimensionality or to a small set of observation symbols. Investigating intrusions in offline as well as online scenarios, we find that both HMM and LSTM can be effective in predicting attack start time, attack type, and attack actions. If sufficient training data is available, LSTM achieves higher prediction accuracy than HMM. HMM, on the other hand, requires less computational resources and less training data for effective prediction. Also, we find that the methods we study benefit from data produced by traditional intrusion detection systems like SNORT.\nlink: https://arxiv.org/abs/2402.13081v2\n"}}
{"custom_id": "2512.19179v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling\nsummary: Efficiently harnessing GPU compute is critical to improving user experience and reducing operational costs in large language model (LLM) services. However, current inference engine schedulers overlook the attention backend's sensitivity to request-length heterogeneity within a batch. As state-of-the-art models now support context windows exceeding 128K tokens, this once-tolerable inefficiency has escalated into a primary system bottleneck, causing severe performance degradation through GPU underutilization and increased latency. We present L4, a runtime system that dynamically reschedules requests across multiple instances serving the same LLM to mitigate per-instance length heterogeneity. L4 partitions these instances into length-specialized groups, each handling requests within a designated length range, naturally forming a pipeline as requests flow through them. L4 devises a dynamic programming algorithm to efficiently find the stage partition with the best QoE, employs runtime range refinement together with decentralized load (re)balance both across and within groups, achieving a balanced and efficient multi-instance service. Our evaluation shows that, under the same configuration, L4 reduces end-to-end latency by up to 67% and tail latency by up to 69%, while improving overall system throughput by up to 2.89 times compared to the state-of-the-art multi-instance scheduling systems.\nlink: https://arxiv.org/abs/2512.19179v1\n"}}
{"custom_id": "2512.19177v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Semantic Communication for Rate-Limited Closed-Loop Distributed Communication-Sensing-Control Systems\nsummary: The growing integration of distributed integrated sensing and communication (ISAC) with closed-loop control in intelligent networks demands efficient information transmission under stringent bandwidth constraints. To address this challenge, this paper proposes a unified framework for goal-oriented semantic communication in distributed SCC systems. Building upon Weaver's three-level model, we establish a hierarchical semantic formulation with three error levels (L1: observation reconstruction, L2: state estimation, and L3: control) to jointly optimize their corresponding objectives. Based on this formulation, we propose a unified goal-oriented semantic compression and rate adaptation framework that is applicable to different semantic error levels and optimization goals across the SCC loop. A rate-limited multi-sensor LQR system is used as a case study to validate the proposed framework. We employ a GRU-based AE for semantic compression and a PPO-based rate adaptation algorithm that dynamically allocates transmission rates across sensors. Results show that the proposed framework effectively captures task-relevant semantics and adapts its resource allocation strategies across different semantic levels, thereby achieving level-specific performance gains under bandwidth constraints.\nlink: https://arxiv.org/abs/2512.19177v1\n"}}
{"custom_id": "2512.19131v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT\nsummary: Decentralized federated learning (DFL) enables collaborative model training across edge devices without centralized coordination, offering resilience against single points of failure. However, statistical heterogeneity arising from non-identically distributed local data creates a fundamental challenge: nodes must learn personalized models adapted to their local distributions while selectively collaborating with compatible peers. Existing approaches either enforce a single global model that fits no one well, or rely on heuristic peer selection mechanisms that cannot distinguish between peers with genuinely incompatible data distributions and those with valuable complementary knowledge. We present Murmura, a framework that leverages evidential deep learning to enable trust-aware model personalization in DFL. Our key insight is that epistemic uncertainty from Dirichlet-based evidential models directly indicates peer compatibility: high epistemic uncertainty when a peer's model evaluates local data reveals distributional mismatch, enabling nodes to exclude incompatible influence while maintaining personalized models through selective collaboration. Murmura introduces a trust-aware aggregation mechanism that computes peer compatibility scores through cross-evaluation on local validation samples and personalizes model aggregation based on evidential trust with adaptive thresholds. Evaluation on three wearable IoT datasets (UCI HAR, PAMAP2, PPG-DaLiA) demonstrates that Murmura reduces performance degradation from IID to non-IID conditions compared to baseline (0.9% vs. 19.3%), achieves 7.4$\\times$ faster convergence, and maintains stable accuracy across hyperparameter choices. These results establish evidential uncertainty as a principled foundation for compatibility-aware personalization in decentralized heterogeneous environments.\nlink: https://arxiv.org/abs/2512.19131v1\n"}}
{"custom_id": "2512.19124v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: ShadowBlock: Efficient Dynamic Anonymous Blocklisting and Its Cross-chain Application\nsummary: Online harassment, incitement to violence, racist behavior, and other harmful content on social media can damage social harmony and even break the law. Traditional blocklisting technologies can block malicious users, but this comes at the expense of identity privacy. The anonymous blocklisting has emerged as an effective mechanism to restrict the abuse of freedom of speech while protecting user identity privacy. However, the state-of-the-art anonymous blocklisting schemes suffer from either poor dynamism or low efficiency. In this paper, we propose $\\mathsf{ShadowBlock}$, an efficient dynamic anonymous blocklisting scheme. Specifically, we utilize the pseudorandom function and cryptographic accumulator to construct the public blocklisting, enabling users to prove they are not on the blocklisting in an anonymous manner. To improve verification efficiency, we design an aggregation zero-knowledge proof mechanism that converts multiple verification operations into a single one. In addition, we leverage the accumulator's property to achieve efficient updates of the blocklisting, i.e., the original proof can be reused with minimal updates rather than regenerating the entire proof. Experiments show that $\\mathsf{ShadowBlock}$ has better dynamics and efficiency than the existing schemes. Finally, the discussion on applications indicates that $\\mathsf{ShadowBlock}$ also holds significant value and has broad prospects in emerging fields such as cross-chain identity management.\nlink: https://arxiv.org/abs/2512.19124v1\n"}}
{"custom_id": "2512.17748v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Methods and Tools for Secure Quantum Clouds with a specific Case Study on Homomorphic Encryption\nsummary: The rise of quantum computing/technology potentially introduces significant security challenges to cloud computing, necessitating quantum-resistant encryption strategies as well as protection schemes and methods for cloud infrastructures offering quantum computing time and services (i.e. quantum clouds). This research explores various options for securing quantum clouds and ensuring privacy, especially focussing on the integration of homomorphic encryption (HE) into Eclipse Qrisp, a high-level quantum computing framework, to enhance the security of quantum cloud platforms. The study addresses the technical feasibility of integrating HE with Qrisp, evaluates performance trade-offs, and assesses the potential impact on future quantum cloud architectures. The successful implementation and Qrisp integration of three post-quantum cryptographic (PQC) algorithms demonstrates the feasibility of integrating HE with quantum computing frameworks. The findings indicate that while the Quantum One-Time Pad (QOTP) offers simplicity and low overhead, other algorithms like Chen and Gentry-Sahai-Waters (GSW) present performance trade-offs in terms of runtime and memory consumption. The study results in an overall set of recommendations for securing quantum clouds, e.g. implementing HE at data storage and processing levels, developing Quantum Key Distribution (QKD), and enforcing stringent access control and authentication mechanisms as well as participating in PQC standardization efforts.\nlink: https://arxiv.org/abs/2512.17748v2\n"}}
{"custom_id": "2512.19113v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: A Unified Framework and Comparative Study of Decentralized Finance Derivatives Protocols\nsummary: Decentralized Finance (DeFi) applications introduce novel financial instruments replicating and extending traditional ones through blockchain-based smart contracts. Among these, derivatives protocols enable the decentralized trading of cryptoassets that are the counterpart of derivative products available in traditional finance. Despite their growing significance, DeFi derivatives protocols remain relatively understudied compared to other DeFi instruments, such as lending protocols and decentralized exchanges with automated market makers. This paper systematically analyzes DeFi derivatives protocols - categorized into perpetual, options, and synthetics - in the field, highlighting similarities, differences, dynamics, and actors. As a result of our study, we provide a formal characterization of decentralized derivative products and introduce a unifying conceptual framework that captures the design principles and core architecture of such protocols. We complement our theoretical analysis with numerical simulations: we evaluate protocol dynamics under various economic conditions, including changes in underlying asset prices, volatility, protocol-specific fees, leverage, and their impact on liquidation and profitability.\nlink: https://arxiv.org/abs/2512.19113v1\n"}}
{"custom_id": "2512.19103v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Timely Parameter Updating in Over-the-Air Federated Learning\nsummary: Incorporating over-the-air computations (OAC) into the model training process of federated learning (FL) is an effective approach to alleviating the communication bottleneck in FL systems. Under OAC-FL, every client modulates its intermediate parameters, such as gradient, onto the same set of orthogonal waveforms and simultaneously transmits the radio signal to the edge server. By exploiting the superposition property of multiple-access channels, the edge server can obtain an automatically aggregated global gradient from the received signal. However, the limited number of orthogonal waveforms available in practical systems is fundamentally mismatched with the high dimensionality of modern deep learning models. To address this issue, we propose Freshness Freshness-mAgnItude awaRe top-k (FAIR-k), an algorithm that selects, in each communication round, the most impactful subset of gradients to be updated over the air. In essence, FAIR-k combines the complementary strengths of the Round-Robin and Top-k algorithms, striking a delicate balance between timeliness (freshness of parameter updates) and importance (gradient magnitude). Leveraging tools from Markov analysis, we characterize the distribution of parameter staleness under FAIR-k. Building on this, we establish the convergence rate of OAC-FL with FAIR-k, which discloses the joint effect of data heterogeneity, channel noise, and parameter staleness on the training efficiency. Notably, as opposed to conventional analyses that assume a universal Lipschitz constant across all the clients, our framework adopts a finer-grained model of the data heterogeneity. The analysis demonstrates that since FAIR-k promotes fresh (and fair) parameter updates, it not only accelerates convergence but also enhances communication efficiency by enabling an extended period of local training without significantly affecting overall training efficiency.\nlink: https://arxiv.org/abs/2512.19103v1\n"}}
{"custom_id": "2506.22714v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Libra: Unleashing GPU Heterogeneity for High-Performance Sparse Matrix Multiplication\nsummary: Sparse matrix multiplication operators (i.e., SpMM and SDDMM) are widely used in deep learning and scientific computing. Modern accelerators are commonly equipped with Tensor Core Units (TCUs) and CUDA cores to accelerate sparse operators. The former excels at structured matrix computations, whereas the latter offers greater programming flexibility. However, how to combine these two resources to maximize sparse-operator performance remains unclear. In this work, we first identify the source of performance gains in hybrid computation and systematically analyze their complementary strengths. Motivated by this, we propose Libra, a holistic framework that efficiently leverages heterogeneous computing resources to accelerate both SpMM and SDDMM operators. Specifically, Libra introduces a 2D-aware (locality and utilization) workload distribution method to precisely identify the optimal task mapping, simultaneously leveraging the data reuse capabilities of TCUs and the flexibility of CUDA cores to minimize computational redundancy. Libra further incorporates hybrid load balancing, occupancy-aware task scheduling, and efficient kernel implementations to maximize execution efficiency. Extensive experiments on H100 and RTX 4090 GPUs demonstrate that Libra surpasses all the 12 up-to-date baselines significantly, e.g., on average 1.77x speedup over FlashSparse, 1.73x over RoDe, and 2.9x over DGL for end-to-end GNN applications. Libra opens up a new perspective for sparse operator acceleration by fully unleashing the power of heterogeneous GPU resources.\nlink: https://arxiv.org/abs/2506.22714v2\n"}}
{"custom_id": "2503.11185v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Bleeding Pathways: Vanishing Discriminability in LLM Hidden States Fuels Jailbreak Attacks\nsummary: LLMs remain vulnerable to jailbreak attacks that exploit adversarial prompts to circumvent safety measures. Current safety fine-tuning approaches face two critical limitations. First, they often fail to strike a balance between security and utility, where stronger safety measures tend to over-reject harmless user requests. Second, they frequently miss malicious intent concealed within seemingly benign tasks, leaving models exposed to exploitation. Our work identifies a fundamental cause of these issues: during response generation, an LLM's capacity to differentiate harmful from safe outputs deteriorates. Experimental evidence confirms this, revealing that the separability between hidden states for safe and harmful responses diminishes as generation progresses. This weakening discrimination forces models to make compliance judgments earlier in the generation process, restricting their ability to recognize developing harmful intent and contributing to both aforementioned failures. To mitigate this vulnerability, we introduce DEEPALIGN - an inherent defense framework that enhances the safety of LLMs. By applying contrastive hidden-state steering at the midpoint of response generation, DEEPALIGN amplifies the separation between harmful and benign hidden states, enabling continuous intrinsic toxicity detection and intervention throughout the generation process. Across diverse LLMs spanning varying architectures and scales, it reduced attack success rates of nine distinct jailbreak attacks to near-zero or minimal. Crucially, it preserved model capability while reducing over-refusal. Models equipped with DEEPALIGN exhibited up to 3.5% lower error rates in rejecting challenging benign queries and maintained standard task performance with less than 1% decline. This marks a substantial advance in the safety-utility Pareto frontier.\nlink: https://arxiv.org/abs/2503.11185v2\n"}}
{"custom_id": "2512.19037v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Elevating Intrusion Detection and Security Fortification in Intelligent Networks through Cutting-Edge Machine Learning Paradigms\nsummary: The proliferation of IoT devices and their reliance on Wi-Fi networks have introduced significant security vulnerabilities, particularly the KRACK and Kr00k attacks, which exploit weaknesses in WPA2 encryption to intercept and manipulate sensitive data. Traditional IDS using classifiers face challenges such as model overfitting, incomplete feature extraction, and high false positive rates, limiting their effectiveness in real-world deployments. To address these challenges, this study proposes a robust multiclass machine learning based intrusion detection framework. The methodology integrates advanced feature selection techniques to identify critical attributes, mitigating redundancy and enhancing detection accuracy. Two distinct ML architectures are implemented: a baseline classifier pipeline and a stacked ensemble model combining noise injection, Principal Component Analysis (PCA), and meta learning to improve generalization and reduce false positives. Evaluated on the AWID3 data set, the proposed ensemble architecture achieves superior performance, with an accuracy of 98%, precision of 98%, recall of 98%, and a false positive rate of just 2%, outperforming existing state-of-the-art methods. This work demonstrates the efficacy of combining preprocessing strategies with ensemble learning to fortify network security against sophisticated Wi-Fi attacks, offering a scalable and reliable solution for IoT environments. Future directions include real-time deployment and adversarial resilience testing to further enhance the model's adaptability.\nlink: https://arxiv.org/abs/2512.19037v1\n"}}
{"custom_id": "2502.01110v7", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: The Nonlinear Filter Model of Stream Cipher Redivivus\nsummary: The nonlinear filter model is an old and well understood approach to the design of secure stream ciphers. Extensive research over several decades has shown how to attack stream ciphers based on this model and has identified the security properties required of the Boolean function used as the filtering function to resist such attacks. This led to the problem of constructing Boolean functions which provide adequate security \\textit{and} at the same time are efficient to implement. Unfortunately, over the last two decades no fully satisfactory solutions to this problem appeared in the literature. The lack of good solutions has effectively led to the nonlinear filter model becoming more or less obsolete. This is a big loss to the cryptographic design toolkit, since the great advantages of the nonlinear filter model are its simplicity, well understood security and the potential to provide low cost solutions for hardware oriented stream ciphers. In this paper, we revive the nonlinear filter model by constructing appropriate Boolean functions which provide required security and are also efficient to implement. We put forward concrete suggestions of stream ciphers which are $\u03ba$-bit secure against known types of attacks for $\u03ba=80$, 128, 160, 192, 224 and 256. For the 80-bit and the 128-bit security levels, the gate count estimates of our proposals compare quite well to the famous ciphers Trivium and Grain-128a respectively, while for the 256-bit security level, we do not know of any other stream cipher design which has such a low gate count.\nlink: https://arxiv.org/abs/2502.01110v7\n"}}
{"custom_id": "2311.02757v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Certified Defense on the Fairness of Graph Neural Networks\nsummary: Graph Neural Networks (GNNs) have emerged as a prominent graph learning model in various graph-based tasks over the years. Nevertheless, due to the vulnerabilities of GNNs, it has been empirically shown that malicious attackers could easily corrupt the fairness level of their predictions by adding perturbations to the input graph data. In this paper, we take crucial steps to study a novel problem of certifiable defense on the fairness level of GNNs. Specifically, we propose a principled framework named ELEGANT and present a detailed theoretical certification analysis for the fairness of GNNs. ELEGANT takes {\\em any} GNN as its backbone, and the fairness level of such a backbone is theoretically impossible to be corrupted under certain perturbation budgets for attackers. Notably, ELEGANT does not make any assumptions over the GNN structure or parameters, and does not require re-training the GNNs to realize certification. Hence it can serve as a plug-and-play framework for any optimized GNNs ready to be deployed. We verify the satisfactory effectiveness of ELEGANT in practice through extensive experiments on real-world datasets across different backbones of GNNs and parameter settings.\nlink: https://arxiv.org/abs/2311.02757v3\n"}}
{"custom_id": "2512.19025v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation\nsummary: Machine unlearning aims to remove specific data influences from trained models, a capability essential for adhering to copyright laws and ensuring AI safety. Current unlearning metrics typically measure success by monitoring the model's performance degradation on the specific unlearning dataset ($D_u$). We argue that for Large Language Models (LLMs), this evaluation paradigm is insufficient and potentially misleading. Many real-world uses of unlearning--motivated by copyright or safety--implicitly target not only verbatim content in $D_u$, but also behaviors influenced by the broader generalizations the model derived from it. We demonstrate that LLMs can pass standard unlearning evaluation and appear to have ``forgotten'' the target knowledge, while simultaneously retaining strong capabilities on content that is semantically adjacent to $D_u$. This phenomenon indicates that erasing exact sentences does not necessarily equate to removing the underlying knowledge. To address this gap, we propose \\name, an automated stress-testing framework that generates a surrogate dataset, $\\tilde{D}_u$. This surrogate set is constructed to be semantically derived from $D_u$ yet sufficiently distinct in embedding space. By comparing unlearning metric scores between $D_u$ and $\\tilde{D}_u$, we can stress-test the reliability of the metric itself. Our extensive evaluation across three LLM families (Llama-3-8B, Qwen2.5-7B, and Zephyr-7B-$\u03b2$), three distinct datasets, and seven standard metrics reveals widespread inconsistencies. We find that current metrics frequently overestimate unlearning success, failing to detect retained knowledge exposed by our stress-test datasets.\nlink: https://arxiv.org/abs/2512.19025v1\n"}}
{"custom_id": "2512.19016v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: DREAM: Dynamic Red-teaming across Environments for AI Models\nsummary: Large Language Models (LLMs) are increasingly used in agentic systems, where their interactions with diverse tools and environments create complex, multi-stage safety challenges. However, existing benchmarks mostly rely on static, single-turn assessments that miss vulnerabilities from adaptive, long-chain attacks. To fill this gap, we introduce DREAM, a framework for systematic evaluation of LLM agents against dynamic, multi-stage attacks. At its core, DREAM uses a Cross-Environment Adversarial Knowledge Graph (CE-AKG) to maintain stateful, cross-domain understanding of vulnerabilities. This graph guides a Contextualized Guided Policy Search (C-GPS) algorithm that dynamically constructs attack chains from a knowledge base of 1,986 atomic actions across 349 distinct digital environments. Our evaluation of 12 leading LLM agents reveals a critical vulnerability: these attack chains succeed in over 70% of cases for most models, showing the power of stateful, cross-environment exploits. Through analysis of these failures, we identify two key weaknesses in current agents: contextual fragility, where safety behaviors fail to transfer across environments, and an inability to track long-term malicious intent. Our findings also show that traditional safety measures, such as initial defense prompts, are largely ineffective against attacks that build context over multiple interactions. To advance agent safety research, we release DREAM as a tool for evaluating vulnerabilities and developing more robust defenses.\nlink: https://arxiv.org/abs/2512.19016v1\n"}}
{"custom_id": "2512.19011v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline\nsummary: Prompt injection and jailbreaking attacks pose persistent security challenges to large language model (LLM)-based systems. We present an efficient and systematically evaluated defense architecture that mitigates these threats through a lightweight, multi-stage pipeline. Its core component is a semantic filter based on text normalization, TF-IDF representations, and a Linear SVM classifier. Despite its simplicity, this module achieves 93.4% accuracy and 96.5% specificity on held-out data, substantially reducing attack throughput while incurring negligible computational overhead.\n  Building on this efficient foundation, the full pipeline integrates complementary detection and mitigation mechanisms that operate at successive stages, providing strong robustness with minimal latency. In comparative experiments, our SVM-based configuration improves overall accuracy from 35.1% to 93.4% while reducing average time to completion from approximately 450s to 47s, yielding over 10 times lower latency than ShieldGemma. These results demonstrate that the proposed design simultaneously advances defensive precision and efficiency, addressing a core limitation of current model-based moderators.\n  Evaluation across a curated corpus of over 30,000 labeled prompts, including benign, jailbreak, and application-layer injections, confirms that staged, resource-efficient defenses can robustly secure modern LLM-driven applications.\nlink: https://arxiv.org/abs/2512.19011v1\n"}}
{"custom_id": "2512.19005v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Quantum-Resistant Cryptographic Models for Next-Gen Cybersecurity\nsummary: Another threat is the development of large quantum computers, which have a high likelihood of breaking the high popular security protocols because it can use both Shor and Grover algorithms. In order to fix this looming threat, quantum-resistant cryptographic systems, otherwise known as post-quantum cryptography (PQC), are being formulated to protect cybersecurity systems of the future. The current paper presents the state of the art in designing, realizing, and testing the security of robust quantum-resistant algorithms, paying attention to lattice-based, code-based, multivariate polynomial and hash-based cryptography. We discuss their resistance to classical and quantum attackers, distributed system scalability properties, and their deployment in practice (secure communications, blockchain, cloud computing infrastructures). Also, we study a hybrid cryptographic model that integrates the classical efficient cryptography scheme and a quantum-resilient cryptographic scheme to achieve a backward-compatible solution and simultaneously improving the forward security properties. With the experimental findings, it is evident that performance with reasonable computational footprint of the proposed framework succeeds to install amplified security fortitude which successfully harbours prolific cybersecurity systems of the future.\nlink: https://arxiv.org/abs/2512.19005v1\n"}}
{"custom_id": "2403.15676v5", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: AC4: Algebraic Computation Checker for Circuit Constraints in ZKPs\nsummary: Zero-knowledge proof (ZKP) systems have surged attention and held a fundamental role in contemporary cryptography. Zero-knowledge succinct non-interactive argument of knowledge (zk-SNARK) protocols dominate the ZKP usage, implemented through arithmetic circuit programming paradigm. However, underconstrained or overconstrained circuits may lead to bugs. The former refers to circuits that lack the necessary constraints, resulting in unexpected solutions and causing the verifier to accept a bogus witness, and the latter refers to circuits that are constrained excessively, resulting in lacking necessary solutions and causing the verifier to accept no witness. This paper introduces a novel approach for pinpointing two distinct types of bugs in ZKP circuits. The method involves encoding the arithmetic circuit constraints to polynomial equation systems and solving them over finite fields by the computer algebra system. The classification of verification results is refined, greatly enhancing the expressive power of the system. A tool, AC4, is proposed to represent the implementation of the method. Experiments show that AC4 demonstrates a increase in the solved rate, showing a 29% improvement over Picus and CIVER, and a slight improvement over halo2-analyzer, a checker for halo2 circuits. Within a solvable range, the checking time has also exhibited noticeable improvement, demonstrating a magnitude increase compared to previous efforts.\nlink: https://arxiv.org/abs/2403.15676v5\n"}}
{"custom_id": "2512.15778v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: COBRA: Catastrophic Bit-flip Reliability Analysis of State-Space Models\nsummary: State-space models (SSMs), exemplified by the Mamba architecture, have recently emerged as state-of-the-art sequence-modeling frameworks, offering linear-time scalability together with strong performance in long-context settings. Owing to their unique combination of efficiency, scalability, and expressive capacity, SSMs have become compelling alternatives to transformer-based models, which suffer from the quadratic computational and memory costs of attention mechanisms. As SSMs are increasingly deployed in real-world applications, it is critical to assess their susceptibility to both software- and hardware-level threats to ensure secure and reliable operation. Among such threats, hardware-induced bit-flip attacks (BFAs) pose a particularly severe risk by corrupting model parameters through memory faults, thereby undermining model accuracy and functional integrity. To investigate this vulnerability, we introduce RAMBO, the first BFA framework specifically designed to target Mamba-based architectures. Through experiments on the Mamba-1.4b model with LAMBADA benchmark, a cloze-style word-prediction task, we demonstrate that flipping merely a single critical bit can catastrophically reduce accuracy from 74.64% to 0% and increase perplexity from 18.94 to 3.75 x 10^6. These results demonstrate the pronounced fragility of SSMs to adversarial perturbations.\nlink: https://arxiv.org/abs/2512.15778v2\n"}}
{"custom_id": "2504.20068v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: JITServe: SLO-aware LLM Serving with Imprecise Request Information\nsummary: The integration of Large Language Models (LLMs) into applications ranging from interactive chatbots to multi-agent systems has introduced a wide spectrum of service-level objectives (SLOs) for responsiveness. These include latency-sensitive requests emphasizing per-token latency in streaming chat, deadline-sensitive requests requiring rapid full responses to trigger external tools, and compound requests with evolving dependencies across multiple LLM calls. Despite-or perhaps, because of-this workload diversity and unpredictable request information (e.g., response lengths and dependencies), existing request schedulers have focused on aggregate performance, unable to ensure application-level SLO needs.\n  This paper presents JITServe, the first SLO-aware LLM serving system designed to maximize service goodput (e.g., the number of tokens meeting request SLOs) across diverse workloads. JITServe novelly schedules requests using imprecise request information and gradually relaxes this conservatism by refining request information estimates as generation progresses. It applies a grouped margin goodput maximization algorithm to allocate just enough serving bandwidth to satisfy each request's SLO just-in-time (JIT), maximizing residual capacity for others, while deciding the composition of requests in a batch to maximize efficiency and goodput with provable guarantees. Our evaluation across diverse realistic workloads, including chat, deep research, and agentic pipelines, shows that JITServe improves service goodput by 1.4x-6.3x, alternatively achieving 28.5%-83.2% resource savings, compared to state-of-the-art designs.\nlink: https://arxiv.org/abs/2504.20068v3\n"}}
{"custom_id": "2512.18932v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: DPSR: Differentially Private Sparse Reconstruction via Multi-Stage Denoising for Recommender Systems\nsummary: Differential privacy (DP) has emerged as the gold standard for protecting user data in recommender systems, but existing privacy-preserving mechanisms face a fundamental challenge: the privacy-utility tradeoff inevitably degrades recommendation quality as privacy budgets tighten. We introduce DPSR (Differentially Private Sparse Reconstruction), a novel three-stage denoising framework that fundamentally addresses this limitation by exploiting the inherent structure of rating matrices -- sparsity, low-rank properties, and collaborative patterns.\n  DPSR consists of three synergistic stages: (1) \\textit{information-theoretic noise calibration} that adaptively reduces noise for high-information ratings, (2) \\textit{collaborative filtering-based denoising} that leverages item-item similarities to remove privacy noise, and (3) \\textit{low-rank matrix completion} that exploits latent structure for signal recovery. Critically, all denoising operations occur \\textit{after} noise injection, preserving differential privacy through the post-processing immunity theorem while removing both privacy-induced and inherent data noise.\n  Through extensive experiments on synthetic datasets with controlled ground truth, we demonstrate that DPSR achieves 5.57\\% to 9.23\\% RMSE improvement over state-of-the-art Laplace and Gaussian mechanisms across privacy budgets ranging from $\\varepsilon=0.1$ to $\\varepsilon=10.0$ (all improvements statistically significant with $p < 0.05$, most $p < 0.001$). Remarkably, at $\\varepsilon=1.0$, DPSR achieves RMSE of 0.9823, \\textit{outperforming even the non-private baseline} (1.0983), demonstrating that our denoising pipeline acts as an effective regularizer that removes data noise in addition to privacy noise.\nlink: https://arxiv.org/abs/2512.18932v1\n"}}
{"custom_id": "2512.18915v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits\nsummary: As computation shifts from the cloud to the edge to reduce processing latency and network traffic, the resulting Computing Continuum (CC) creates a dynamic environment where it is challenging to meet strict Quality of Service (QoS) requirements and avoid service instance overload. Existing methods often prioritize global metrics, overlooking per-client QoS, which is crucial for latency-sensitive and reliability-critical applications. We propose QEdgeProxy, a decentralized QoS-aware load balancer that acts as a proxy between IoT devices and service instances in CC. We formulate the load balancing problem as a Multi-Player Multi-Armed Bandit (MP-MAB) with heterogeneous rewards, where each load balancer autonomously selects service instances that maximize the probability of meeting its clients' QoS targets by using Kernel Density Estimation (KDE) to estimate QoS success probabilities. It also incorporates an adaptive exploration mechanism to recover rapidly from performance shifts and non-stationary conditions. We present a Kubernetes-native QEdgeProxy implementation and evaluate it on an emulated CC testbed deployed on a K3s cluster with realistic network conditions and a latency-sensitive edge-AI workload. Results show that QEdgeProxy significantly outperforms proximity-based and reinforcement-learning baselines in per-client QoS satisfaction, while adapting effectively to load surges and instance availability changes.\nlink: https://arxiv.org/abs/2512.18915v1\n"}}
{"custom_id": "2511.02376v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models\nsummary: Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where adversarial prompts elicit harmful outputs. Yet most evaluations focus on single-turn interactions while real-world attacks unfold through adaptive multi-turn conversations. We present AutoAdv, a training-free framework for automated multi-turn jailbreaking that achieves an attack success rate of up to 95% on Llama-3.1-8B within six turns, a 24% improvement over single-turn baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern manager that learns from successful attacks to enhance future prompts, a temperature manager that dynamically adjusts sampling parameters based on failure modes, and a two-phase rewriting strategy that disguises harmful requests and then iteratively refines them. Extensive evaluation across commercial and open-source models (Llama-3.1-8B, GPT-4o mini, Qwen3-235B, Mistral-7B) reveals persistent vulnerabilities in current safety mechanisms, with multi-turn attacks consistently outperforming single-turn approaches. These findings demonstrate that alignment strategies optimized for single-turn interactions fail to maintain robustness across extended conversations, highlighting an urgent need for multi-turn-aware defenses.\nlink: https://arxiv.org/abs/2511.02376v3\n"}}
{"custom_id": "2512.18894v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: A Real-Time Digital Twin for Adaptive Scheduling\nsummary: High-performance computing (HPC) workloads are becoming increasingly diverse, exhibiting wide variability in job characteristics, yet cluster scheduling has long relied on static, heuristic-based policies. In this work we present SchedTwin, a real-time digital twin designed to adaptively guide scheduling decisions using predictive simulation. SchedTwin periodically ingests runtime events from the physical scheduler, performs rapid what-if evaluations of multiple policies using a high-fidelity discrete-event simulator, and dynamically selects the one satisfying the administrator configured optimization goal. We implement SchedTwin as an open-source software and integrate it with the production PBS scheduler. Preliminary results show that SchedTwin consistently outperforms widely used static scheduling policies, while maintaining low overhead (a few seconds per scheduling cycle). These results demonstrate that real-time digital twins offer a practical and effective path toward adaptive HPC scheduling.\nlink: https://arxiv.org/abs/2512.18894v1\n"}}
{"custom_id": "2512.16292v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: In-Context Probing for Membership Inference in Fine-Tuned Language Models\nsummary: Membership inference attacks (MIAs) pose a critical privacy threat to fine-tuned large language models (LLMs), especially when models are adapted to domain-specific tasks using sensitive data. While prior black-box MIA techniques rely on confidence scores or token likelihoods, these signals are often entangled with a sample's intrinsic properties - such as content difficulty or rarity - leading to poor generalization and low signal-to-noise ratios. In this paper, we propose ICP-MIA, a novel MIA framework grounded in the theory of training dynamics, particularly the phenomenon of diminishing returns during optimization. We introduce the Optimization Gap as a fundamental signal of membership: at convergence, member samples exhibit minimal remaining loss-reduction potential, while non-members retain significant potential for further optimization. To estimate this gap in a black-box setting, we propose In-Context Probing (ICP), a training-free method that simulates fine-tuning-like behavior via strategically constructed input contexts. We propose two probing strategies: reference-data-based (using semantically similar public samples) and self-perturbation (via masking or generation). Experiments on three tasks and multiple LLMs show that ICP-MIA significantly outperforms prior black-box MIAs, particularly at low false positive rates. We further analyze how reference data alignment, model type, PEFT configurations, and training schedules affect attack effectiveness. Our findings establish ICP-MIA as a practical and theoretically grounded framework for auditing privacy risks in deployed LLMs.\nlink: https://arxiv.org/abs/2512.16292v2\n"}}
{"custom_id": "2512.18883v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: EuroHPC SPACE CoE: Redesigning Scalable Parallel Astrophysical Codes for Exascale\nsummary: High Performance Computing (HPC) based simulations are crucial in Astrophysics and Cosmology (A&C), helping scientists investigate and understand complex astrophysical phenomena. Taking advantage of exascale computing capabilities is essential for these efforts. However, the unprecedented architectural complexity of exascale systems impacts legacy codes. The SPACE Centre of Excellence (CoE) aims to re-engineer key astrophysical codes to tackle new computational challenges by adopting innovative programming paradigms and software (SW) solutions. SPACE brings together scientists, code developers, HPC experts, hardware (HW) manufacturers, and SW developers. This collaboration enhances exascale A&C applications, promoting the use of exascale and post-exascale computing capabilities. Additionally, SPACE addresses high-performance data analysis for the massive data outputs from exascale simulations and modern observations, using machine learning (ML) and visualisation tools. The project facilitates application deployment across platforms by focusing on code repositories and data sharing, integrating European astrophysical communities around exascale computing with standardised SW and data protocols.\nlink: https://arxiv.org/abs/2512.18883v1\n"}}
{"custom_id": "2503.05136v23", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: The Beginner's Textbook for Fully Homomorphic Encryption\nsummary: Fully Homomorphic Encryption (FHE) is a cryptographic scheme that enables computations to be performed directly on encrypted data, as if the data were in plaintext. After all computations are performed on the encrypted data, it can be decrypted to reveal the result. The decrypted value matches the result that would have been obtained if the same computations were applied to the plaintext data.\n  FHE supports basic operations such as addition and multiplication on encrypted numbers. Using these fundamental operations, more complex computations can be constructed, including subtraction, division, logic gates (e.g., AND, OR, XOR, NAND, MUX), and even advanced mathematical functions such as ReLU, sigmoid, and trigonometric functions (e.g., sin, cos). These functions can be implemented either as exact formulas or as approximations, depending on the trade-off between computational efficiency and accuracy.\n  FHE enables privacy-preserving machine learning by allowing a server to process the client's data in its encrypted form through an ML model. With FHE, the server learns neither the plaintext version of the input features nor the inference results. Only the client, using their secret key, can decrypt and access the results at the end of the service protocol. FHE can also be applied to confidential blockchain services, ensuring that sensitive data in smart contracts remains encrypted and confidential while maintaining the transparency and integrity of the execution process. Other applications of FHE include secure outsourcing of data analytics, encrypted database queries, privacy-preserving searches, efficient multi-party computation for digital signatures, and more.\n  A dynamic website version is available at (https://fhetextbook.github.io). Please report any bugs or errors to the Github issues board.\nlink: https://arxiv.org/abs/2503.05136v23\n"}}
{"custom_id": "2512.18791v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Smark: A Watermark for Text-to-Speech Diffusion Models via Discrete Wavelet Transform\nsummary: Text-to-Speech (TTS) diffusion models generate high-quality speech, which raises challenges for the model intellectual property protection and speech tracing for legal use. Audio watermarking is a promising solution. However, due to the structural differences among various TTS diffusion models, existing watermarking methods are often designed for a specific model and degrade audio quality, which limits their practical applicability. To address this dilemma, this paper proposes a universal watermarking scheme for TTS diffusion models, termed Smark. This is achieved by designing a lightweight watermark embedding framework that operates in the common reverse diffusion paradigm shared by all TTS diffusion models. To mitigate the impact on audio quality, Smark utilizes the discrete wavelet transform (DWT) to embed watermarks into the relatively stable low-frequency regions of the audio, which ensures seamless watermark-audio integration and is resistant to removal during the reverse diffusion process. Extensive experiments are conducted to evaluate the audio quality and watermark performance in various simulated real-world attack scenarios. The experimental results show that Smark achieves superior performance in both audio quality and watermark extraction accuracy.\nlink: https://arxiv.org/abs/2512.18791v1\n"}}
{"custom_id": "2506.18439v11", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Computational Complexity of Model-Checking Quantum Pushdown Systems\nsummary: In this paper, we study the problem of model-checking quantum pushdown systems from a computational complexity point of view. We arrive at the following equally important, interesting new results:\n  We first extend the notions of the {\\it probabilistic pushdown systems} and {\\it Markov chains} to their quantum counterparts, i.e., {\\em quantum pushdown system (qPDS)} and {\\em quantum Markov chains}, and prove a necessary and sufficient condition for a qPDS to be well formed, also presenting a method to extend the local transition function of a well-formed qPDS to a unitary local time evolution operator.\n  Next, we investigate the question of whether it is necessary to define a quantum analogue of {\\it probabilistic computational tree logic} to describe the probabilistic and branching-time properties of the {\\it quantum Markov chain}. We study its model-checking question and show that model-checking of {\\it generalized stateless quantum pushdown systems (gqBPA)} against {\\it probabilistic computational tree logic (PCTL)} is generally undecidable, i.e., there exists no algorithm for model-checking {\\it generalized stateless quantum pushdown systems (gqBPA)} against {\\it probabilistic computational tree logic}.\n  We then study in which case there exists an algorithm for model-checking {\\it stateless quantum pushdown systems} and show that the problem of model-checking {\\it stateless quantum pushdown systems (qBPA)} against {\\it bounded probabilistic computational tree logic} (bPCTL) is decidable, and further show that this problem is in $\\mathit{NP}$-hard. Our reduction is from the {\\it bounded Post Correspondence Problem} for the first time, a well-known $\\mathit{NP}$-complete problem.\nlink: https://arxiv.org/abs/2506.18439v11\n"}}
{"custom_id": "2512.06713v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Look Twice before You Leap: A Rational Agent Framework for Localized Adversarial Anonymization\nsummary: Current LLM-based text anonymization frameworks usually rely on remote API services from powerful LLMs, which creates an inherent privacy paradox: users must disclose data to untrusted third parties for guaranteed privacy preservation. Moreover, directly migrating current solutions to local small-scale models (LSMs) offers a suboptimal solution with severe utility collapse. Our work argues that this failure stems not merely from the capability deficits of LSMs, but significantly from the inherent irrationality of the greedy adversarial strategies employed by current state-of-the-art (SOTA) methods. To address this, we propose Rational Localized Adversarial Anonymization (RLAA), a fully localized and training-free framework featuring an Attacker-Arbitrator-Anonymizer architecture. We model the anonymization process as a trade-off between Marginal Privacy Gain (MPG) and Marginal Utility Cost (MUC), and demonstrate that greedy strategies tend to drift into an irrational state. Instead, RLAA introduces an arbitrator that acts as a rationality gatekeeper, validating the attacker's inference to filter out feedback providing negligible privacy benefits. This mechanism promotes a rational early-stopping criterion, and structurally prevents utility collapse. Extensive experiments on different benchmarks demonstrate that RLAA achieves a superior privacy-utility trade-off compared to strong baselines.\nlink: https://arxiv.org/abs/2512.06713v2\n"}}
{"custom_id": "2512.18751v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: ISADM: An Integrated STRIDE, ATT&CK, and D3FEND Model for Threat Modeling Against Real-world Adversaries\nsummary: FinTechs increasing connectivity, rapid innovation, and reliance on global digital infrastructures present significant cybersecurity challenges. Traditional cybersecurity frameworks often struggle to identify and prioritize sector-specific vulnerabilities or adapt to evolving adversary tactics, particularly in highly targeted sectors such as FinTech. To address these gaps, we propose ISADM (Integrated STRIDE-ATTACK-D3FEND Threat Model), a novel hybrid methodology applied to FinTech security that integrates STRIDE's asset-centric threat classification with MITRE ATTACK's catalog of real-world adversary behaviors and D3FEND's structured knowledge of countermeasures. ISADM employs a frequency-based scoring mechanism to quantify the prevalence of adversarial Tactics, Techniques, and Procedures (TTPs), enabling a proactive, score-driven risk assessment and prioritization framework. This proactive approach contributes to shifting organizations from reactive defense strategies toward the strategic fortification of critical assets. We validate ISADM through industry-relevant case study analyses, demonstrating how the approach replicates actual attack patterns and strengthens proactive threat modeling, guiding risk prioritization and resource allocation to the most critical vulnerabilities. Overall, ISADM offers a comprehensive hybrid threat modeling methodology that bridges asset-centric and adversary-centric analysis, providing FinTech systems with stronger defenses. The emphasis on real-world validation highlights its practical significance in enhancing the sector's cybersecurity posture through a frequency-informed, impact-aware prioritization scheme that combines empirical attacker data with contextual risk analysis.\nlink: https://arxiv.org/abs/2512.18751v1\n"}}
{"custom_id": "2501.17021v4", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Network Oblivious Transfer via Noisy Channels: Limits and Capacities\nsummary: In this paper, we study the information-theoretic limits of oblivious transfer via noisy channels. We also investigate oblivious transfer over a noisy multiple-access channel with two non-colluding senders and a single receiver. The channel is modeled through correlations among the parties, who may be honest-but-curious or, in the case of the receiver, potentially malicious. We first revisit the information-theoretic limits of two-party oblivious transfer and then extend these results to the multiple-access setting. For honest-but-curious participants, we introduce a multiparty protocol that reduces a general multiple access channel to a suitable correlation model. In scenarios with a malicious receiver, we characterize an achievable oblivious transfer rate region.\nlink: https://arxiv.org/abs/2501.17021v4\n"}}
{"custom_id": "2512.18733v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Explainable and Fine-Grained Safeguarding of LLM Multi-Agent Systems via Bi-Level Graph Anomaly Detection\nsummary: Large language model (LLM)-based multi-agent systems (MAS) have shown strong capabilities in solving complex tasks. As MAS become increasingly autonomous in various safety-critical tasks, detecting malicious agents has become a critical security concern. Although existing graph anomaly detection (GAD)-based defenses can identify anomalous agents, they mainly rely on coarse sentence-level information and overlook fine-grained lexical cues, leading to suboptimal performance. Moreover, the lack of interpretability in these methods limits their reliability and real-world applicability. To address these limitations, we propose XG-Guard, an explainable and fine-grained safeguarding framework for detecting malicious agents in MAS. To incorporate both coarse and fine-grained textual information for anomalous agent identification, we utilize a bi-level agent encoder to jointly model the sentence- and token-level representations of each agent. A theme-based anomaly detector further captures the evolving discussion focus in MAS dialogues, while a bi-level score fusion mechanism quantifies token-level contributions for explanation. Extensive experiments across diverse MAS topologies and attack scenarios demonstrate robust detection performance and strong interpretability of XG-Guard.\nlink: https://arxiv.org/abs/2512.18733v1\n"}}
{"custom_id": "2412.19652v4", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: A Plug-and-Play Method for Improving Imperceptibility and Capacity in Practical Generative Text Steganography\nsummary: Linguistic steganography embeds secret information into seemingly innocuous text to safeguard privacy under surveillance. Generative linguistic steganography leverages the probability distributions of language models (LMs) and applies steganographic algorithms during generation, and has attracted increasing attention with the rise of large language models (LLMs). To strengthen security, prior work has focused on distribution-preserving steganographic algorithms that minimize the gap between stego sampling and random sampling from the model. However, their reliance on model distributions, which often deviate from real-world cover texts, leads to limited imperceptibility when facing steganalysis detectors in practical settings. Moreover, LLM distributions tend to be more deterministic, reducing entropy and thus lowering embedding capacity. In this paper, we propose a plug-and-play method that reconstructs the distributions of language models used for generative linguistic steganography. FreStega dynamically adjusts token probabilities from the language model at each step of autoregressive stego text generation, leveraging both sequential and spatial dimensions. Extensive experiments on four LLMs, three benchmark datasets, and four distribution-preserving steganographic baselines demonstrate that, by reforming the distribution, FreStega improves the imperceptibility of stego text in realistic scenarios and increases steganographic capacity by 15.41\\%, without degrading the quality of the generated stegotext.\nlink: https://arxiv.org/abs/2412.19652v4\n"}}
{"custom_id": "2507.20688v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Guard-GBDT: Efficient Privacy-Preserving Approximated GBDT Training on Vertical Dataset\nsummary: In light of increasing privacy concerns and stringent legal regulations, using secure multiparty computation (MPC) to enable collaborative GBDT model training among multiple data owners has garnered significant attention. Despite this, existing MPC-based GBDT frameworks face efficiency challenges due to high communication costs and the computation burden of non-linear operations, such as division and sigmoid calculations. In this work, we introduce Guard-GBDT, an innovative framework tailored for efficient and privacy-preserving GBDT training on vertical datasets. Guard-GBDT bypasses MPC-unfriendly division and sigmoid functions by using more streamlined approximations and reduces communication overhead by compressing the messages exchanged during gradient aggregation. We implement a prototype of Guard-GBDT and extensively evaluate its performance and accuracy on various real-world datasets. The results show that Guard-GBDT outperforms state-of-the-art HEP-XGB (CIKM'21) and SiGBDT (ASIA CCS'24) by up to $2.71\\times$ and $12.21 \\times$ on LAN network and up to $2.7\\times$ and $8.2\\times$ on WAN network. Guard-GBDT also achieves comparable accuracy with SiGBDT and plaintext XGBoost (better than HEP-XGB ), which exhibits a deviation of $\\pm1\\%$ to $\\pm2\\%$ only. Our implementation code is provided at https://github.com/XidianNSS/Guard-GBDT.git.\nlink: https://arxiv.org/abs/2507.20688v2\n"}}
{"custom_id": "2512.18714v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: An Evidence-Driven Analysis of Threat Information Sharing Challenges for Industrial Control Systems and Future Directions\nsummary: The increasing cyber threats to critical infrastructure highlight the importance of private companies and government agencies in detecting and sharing information about threat activities. Although the need for improved threat information sharing is widely recognized, various technical and organizational challenges persist, hindering effective collaboration. In this study, we review the challenges that disturb the sharing of usable threat information to critical infrastructure operators within the ICS domain. We analyze three major incidents: Stuxnet, Industroyer, and Triton. In addition, we perform a systematic analysis of 196 procedure examples across 79 MITRE ATT&CK techniques from 22 ICS-related malware families, utilizing automated natural language processing techniques to systematically extract and categorize threat observables. Additionally, we investigated nine recent ICS vulnerability advisories from the CISA Known Exploitable Vulnerability catalog. Our analysis identified four important limitations in the ICS threat information sharing ecosystem: (i) the lack of coherent representation of artifacts related to ICS adversarial techniques in information sharing language standards (e.g., STIX); (ii) the dependence on undocumented proprietary technologies; (iii) limited technical details provided in vulnerability and threat incident reports; and (iv) the accessibility of technical details for observed adversarial techniques. This study aims to guide the development of future information-sharing standards, including the enhancement of the cyber-observable objects schema in STIX, to ensure accurate representation of artifacts specific to ICS environments.\nlink: https://arxiv.org/abs/2512.18714v1\n"}}
{"custom_id": "2506.12880v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Universal Jailbreak Suffixes Are Strong Attention Hijackers\nsummary: We study suffix-based jailbreaks$\\unicode{x2013}$a powerful family of attacks against large language models (LLMs) that optimize adversarial suffixes to circumvent safety alignment. Focusing on the widely used foundational GCG attack, we observe that suffixes vary in efficacy: some are markedly more universal$\\unicode{x2013}$generalizing to many unseen harmful instructions$\\unicode{x2013}$than others. We first show that a shallow, critical mechanism drives GCG's effectiveness. This mechanism builds on the information flow from the adversarial suffix to the final chat template tokens before generation. Quantifying the dominance of this mechanism during generation, we find GCG irregularly and aggressively hijacks the contextualization process. Crucially, we tie hijacking to the universality phenomenon, with more universal suffixes being stronger hijackers. Subsequently, we show that these insights have practical implications: GCG's universality can be efficiently enhanced (up to $\\times$5 in some cases) at no additional computational cost, and can also be surgically mitigated, at least halving the attack's success with minimal utility loss. We release our code and data at http://github.com/matanbt/interp-jailbreak.\nlink: https://arxiv.org/abs/2506.12880v2\n"}}
{"custom_id": "2512.18674v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing\nsummary: Mixture-of-Experts (MoE) has become a dominant architecture in large language models (LLMs) due to its ability to scale model capacity via sparse expert activation. Meanwhile, serverless computing, with its elasticity and pay-per-use billing, is well-suited for deploying MoEs with bursty workloads. However, the large number of experts in MoE models incurs high inference costs due to memory-intensive parameter caching. These costs are difficult to mitigate via simple model partitioning due to input-dependent expert activation. To address these issues, we propose Remoe, a heterogeneous MoE inference system tailored for serverless computing. Remoe assigns non-expert modules to GPUs and expert modules to CPUs, and further offloads infrequently activated experts to separate serverless functions to reduce memory overhead and enable parallel execution. We incorporate three key techniques: (1) a Similar Prompts Searching (SPS) algorithm to predict expert activation patterns based on semantic similarity of inputs; (2) a Main Model Pre-allocation (MMP) algorithm to ensure service-level objectives (SLOs) via worst-case memory estimation; and (3) a joint memory and replica optimization framework leveraging Lagrangian duality and the Longest Processing Time (LPT) algorithm. We implement Remoe on Kubernetes and evaluate it across multiple LLM benchmarks. Experimental results show that Remoe reduces inference cost by up to 57% and cold start latency by 47% compared to state-of-the-art baselines.\nlink: https://arxiv.org/abs/2512.18674v1\n"}}
{"custom_id": "2412.10872v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: ThreatPilot: Attack-Driven Threat Intelligence Extraction\nsummary: Efficient defense against dynamically evolving advanced persistent threats (APT) requires the structured threat intelligence feeds, such as techniques used. However, existing threat-intelligence extraction techniques predominantly focuses on individual pieces of intelligence-such as isolated techniques or atomic indicators-resulting in fragmented and incomplete representations of real-world attacks. This granularity inherently limits on both the depth and the contextual richness of the extracted intelligence, making it difficult for downstream security systems to reason about multi-step behaviors or to generate actionable detections. To address this gap, we propose to extract the layered Attack-driven Threat Intelligence (ATIs), a comprehensive representation that captures the full spectrum of adversarial behavior. We propose ThreatPilot, which can accurately identify the AITs including complete tactics, techniques, multi-step procedures, and their procedure variants, and integrate the threat intelligence to software security application scenarios: the detection rules (i.e., Sigma) and attack command can be generated automatically to a more accuracy level. Experimental results on 1,769 newly crawly reports and 16 manually calibrated reports show ThreatPilot's effectiveness in identifying accuracy techniques, outperforming state-of-the-art approaches of AttacKG by 1.34X in F1 score. Further studies upon 64,185 application logs via Honeypot show that our Sigma rule generator significantly outperforms several existing rules-set in detecting the real-world malicious events. Industry partners confirm that our Sigma rule generator can significantly help save time and costs of the rule generation process. In addition, our generated commands achieve an execution rate of 99.3%, compared to 50.3% without the extracted intelligence.\nlink: https://arxiv.org/abs/2412.10872v2\n"}}
{"custom_id": "2512.18646v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Volley Revolver: A Novel Matrix-Encoding Method for Privacy-Preserving Deep Learning (Inference++)\nsummary: Privacy-preserving inference of convolutional neural networks (CNNs) using homomorphic encryption has emerged as a promising approach for enabling secure machine learning in untrusted environments. In our previous work, we introduced a matrix-encoding strategy that allows convolution and matrix multiplication to be efficiently evaluated over encrypted data, enabling practical CNN inference without revealing either the input data or the model parameters. The core idea behind this strategy is to construct a three-dimensional representation within ciphertexts that preserves the intrinsic spatial structure of both input image data and model weights, rather than flattening them into conventional two-dimensional encodings. However, this approach can operate efficiently $only$ when the number of available plaintext slots within a ciphertext is sufficient to accommodate an entire input image, which becomes a critical bottleneck when processing high-resolution images. In this paper, we address this fundamental limitation by proposing an improved encoding and computation framework that removes the requirement that a single encrypted ciphertext must fully contain one input image. Our method reformulates the data layout and homomorphic operations to partition high-resolution inputs across multiple ciphertexts while preserving the algebraic structure required for efficient convolution and matrix multiplication. As a result, our approach enables privacy-preserving CNN inference to scale naturally beyond the slot-capacity constraints of prior methods, making homomorphic evaluation of CNNs practical for higher-resolution and more complex datasets.\nlink: https://arxiv.org/abs/2512.18646v1\n"}}
{"custom_id": "2512.18632v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Multi-user Pufferfish Privacy\nsummary: This paper studies how to achieve individual indistinguishability by pufferfish privacy in aggregated query to a multi-user system. It is assumed that each user reports realization of a random variable. We study how to calibrate Laplace noise, added to the query answer, to attain pufferfish privacy when user changes his/her reported data value, leaves the system and is replaced by another use with different randomness. Sufficient conditions are derived for all scenarios for attaining statistical indistinguishability on four sets of secret pairs. They are derived using the existing Kantorovich method (Wasserstain metric of order $1$). These results can be applied to attain indistinguishability when a certain class of users is added or removed from a tabular data. It is revealed that attaining indifference in individual's data is conditioned on the statistics of this user only. For binary (Bernoulli distributed) random variables, the derived sufficient conditions can be further relaxed to reduce the noise and improve data utility.\nlink: https://arxiv.org/abs/2512.18632v1\n"}}
{"custom_id": "2310.19147v4", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Incentivizing Forecasters to Learn: Summarized vs. Unrestricted Advice\nsummary: How should forecasters be incentivized to acquire the most information when learning takes place over time? We address this question in the context of a novel dynamic mechanism design problem in which a designer incentivizes an expert to learn by conditioning rewards on an event's outcome and the expert's reports. Eliciting summarized advice at a terminal date maximizes information acquisition if an informative signal either fully reveals the outcome or has predictable content. Otherwise, richer reporting capabilities may be required. Our findings shed light on incentive design for consultation and forecasting by illustrating how learning dynamics shape the qualitative properties of effort-maximizing contracts.\nlink: https://arxiv.org/abs/2310.19147v4\n"}}
{"custom_id": "2512.18620v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Obnoxious Facility Location Problems: Strategyproof Mechanisms Optimizing $L_p$-Aggregated Utilities and Costs\nsummary: We study the problem of locating a single obnoxious facility on the normalized line segment $[0,1]$ with strategic agents from a mechanism design perspective. Each agent has a preference for the undesirable location of the facility and would prefer the facility to be far away from their location. We consider the utility of the agent, defined as the distance between the agent's location and the facility location, and the cost of each agent, equal to one minus the utility. Given this standard setting of obnoxious facility location problems, our goal is to design (group) strategyproof mechanisms to elicit agent locations truthfully and determine facility location approximately optimizing the $L_p$-aggregated utility and cost objectives, which generalizes the $L_p$-norm ($p\\ge 1$) of the agents' utilities and agents' costs to any $p \\in [-\\infty, \\infty]$, respectively. We establish upper and lower bounds on the approximation ratios of deterministic and randomized (group) strategyproof mechanisms for maximizing the $L_p$-aggregated utilities or minimizing the $L_p$-aggregated costs across the range of \\(p\\)-values. While there are gaps between upper and lower bounds for randomized mechanisms, our bounds for deterministic mechanisms are tight.\nlink: https://arxiv.org/abs/2512.18620v1\n"}}
{"custom_id": "2512.18616v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: DASH: Deception-Augmented Shared Mental Model for a Human-Machine Teaming System\nsummary: We present DASH (Deception-Augmented Shared mental model for Human-machine teaming), a novel framework that enhances mission resilience by embedding proactive deception into Shared Mental Models (SMM). Designed for mission-critical applications such as surveillance and rescue, DASH introduces \"bait tasks\" to detect insider threats, e.g., compromised Unmanned Ground Vehicles (UGVs), AI agents, or human analysts, before they degrade team performance. Upon detection, tailored recovery mechanisms are activated, including UGV system reinstallation, AI model retraining, or human analyst replacement. In contrast to existing SMM approaches that neglect insider risks, DASH improves both coordination and security. Empirical evaluations across four schemes (DASH, SMM-only, no-SMM, and baseline) show that DASH sustains approximately 80% mission success under high attack rates, eight times higher than the baseline. This work contributes a practical human-AI teaming framework grounded in shared mental models, a deception-based strategy for insider threat detection, and empirical evidence of enhanced robustness under adversarial conditions. DASH establishes a foundation for secure, adaptive human-machine teaming in contested environments.\nlink: https://arxiv.org/abs/2512.18616v1\n"}}
{"custom_id": "2512.18589v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: DNA-HHE: Dual-mode Near-network Accelerator for Hybrid Homomorphic Encryption on the Edge\nsummary: Fully homomorphic encryption (FHE) schemes like RNS-CKKS enable privacy-preserving outsourced computation (PPOC) but suffer from high computational latency and ciphertext expansion, especially on the resource-constrained edge side. Hybrid Homomorphic Encryption (HHE) mitigates these issues on the edge side by replacing HE with lightweight symmetric encryption for plaintext encryption, such as the Rubato cipher for the HHE variant of RNS-CKKS, yet it introduces transciphering overhead on the cloud. The respective strengths and limitations of FHE and HHE call for a dual-mode HHE solution with flexible algorithm switching ability. This paper presents DNA-HHE, the first dual-mode HHE accelerator with near-network coupling for edge devices. DNA-HHE supports both edge-side RNS-CKKS and Rubato within a unified architecture driven by flexible custom instructions. To realize a compact implementation for the edge side, we propose a DSP-efficient modular reduction design, a compact multi-field-adaptive butterfly unit, and parallel scheduling schemes of Rubato with a high degree of resource sharing. DNA-HHE is designed with network protocol packaging and transmission capacities and directly coupled to the network interface controller, achieving reduced overall latency of edge-side PPOC by 1.09$\\times$ to 1.56$\\times$. Our evaluations on the ASIC and FPGA platforms demonstrate that DNA-HHE outperforms the state-of-the-art single-mode designs in both edge-side RNS-CKKS and symmetric cipher with better computation latency and area efficiency, while offering dual-mode functionality.\nlink: https://arxiv.org/abs/2512.18589v1\n"}}
{"custom_id": "2511.19171v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Can LLMs Threaten Human Survival? Benchmarking Potential Existential Threats from LLMs via Prefix Completion\nsummary: Research on the safety evaluation of large language models (LLMs) has become extensive, driven by jailbreak studies that elicit unsafe responses. Such response involves information already available to humans, such as the answer to \"how to make a bomb\". When LLMs are jailbroken, the practical threat they pose to humans is negligible. However, it remains unclear whether LLMs commonly produce unpredictable outputs that could pose substantive threats to human safety. To address this gap, we study whether LLM-generated content contains potential existential threats, defined as outputs that imply or promote direct harm to human survival. We propose \\textsc{ExistBench}, a benchmark designed to evaluate such risks. Each sample in \\textsc{ExistBench} is derived from scenarios where humans are positioned as adversaries to AI assistants. Unlike existing evaluations, we use prefix completion to bypass model safeguards. This leads the LLMs to generate suffixes that express hostility toward humans or actions with severe threat, such as the execution of a nuclear strike. Our experiments on 10 LLMs reveal that LLM-generated content indicates existential threats. To investigate the underlying causes, we also analyze the attention logits from LLMs. To highlight real-world safety risks, we further develop a framework to assess model behavior in tool-calling. We find that LLMs actively select and invoke external tools with existential threats. Code and data are available at: https://github.com/cuiyu-ai/ExistBench.\nlink: https://arxiv.org/abs/2511.19171v2\n"}}
{"custom_id": "2512.18560v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Proof of Authenticity of General IoT Information with Tamper-Evident Sensors and Blockchain\nsummary: Sensor data in IoT (Internet of Things) systems is vulnerable to tampering or falsification when transmitted through untrusted services. This is critical because such data increasingly underpins real-world decisions in domains such as logistics, healthcare, and other critical infrastructure. We propose a general method for secure sensor-data logging in which tamper-evident devices periodically sign readouts, link data using redundant hash chains, and submit cryptographic evidence to a blockchain-based service via Merkle trees to ensure verifiability even under data loss. Our approach enables reliable and cost-effective validation of sensor data across diverse IoT systems, including disaster response and other humanitarian applications, without relying on the integrity of intermediate systems.\nlink: https://arxiv.org/abs/2512.18560v1\n"}}
{"custom_id": "2512.18542v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models\nsummary: AI assistants produce vulnerable code in 45% of security-relevant scenarios, introducing flaws into production systems at scale. Yet existing secure coding datasets fall short. They lack incident grounding, don't provide the scale modern training requires, and miss the operational security context developers need for production deployments. We present SecureCode v2.0, a production-grade dataset of 1,215 security-focused coding examples that passed structural validation and expert security review. Every example ties to actual documented security incidents with CVE references, provides vulnerable and secure implementations, demonstrates concrete attacks, and includes defense-in-depth operational guidance. The dataset covers 11 vulnerability categories (complete OWASP Top 10:2025 plus AI/ML Security Threats) across 11 languages (Python, JavaScript, Java, Go, PHP, C#, TypeScript, Ruby, Rust, Kotlin, and YAML for infrastructure-as-code).\n  Our quality assurance framework ensures complete incident grounding. Each example includes SIEM integration strategies, infrastructure hardening recommendations (Docker, AppArmor, WAF configurations), and testing approaches using language-appropriate frameworks. The dataset uses a 4-turn conversational structure mirroring actual developer-AI interactions, escalating from basic implementations to advanced security considerations and defense-in-depth guidance.\n  Our contributions: (1) 1,215 rigorously validated examples split into 989 training, 122 validation, and 104 test sets, (2) an automated validation framework ensuring dataset consistency, (3) a 4-turn conversational structure capturing realistic security workflows, (4) comprehensive operational security guidance with SIEM integration strategies, (5) complete language-specific implementation fidelity, and (6) open-source release of data, validation tools, and benchmarking protocols.\nlink: https://arxiv.org/abs/2512.18542v1\n"}}
{"custom_id": "2512.18529v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Protecting Human Activity Signatures in Compressed IEEE 802.11 CSI Feedback\nsummary: Explicit channel state information (CSI) feedback in IEEE~802.11 conveys \\emph{transmit beamforming directions} by reporting quantized Givens rotation and phase angles that parametrize the right-singular subspace of the channel matrix. Because these angles encode fine-grained spatial signatures of the propagation environment, recent work have shown that plaintext CSI feedback can inadvertently reveal user activity, identity, and location to passive eavesdroppers. In this work, we introduce a standards-compatible \\emph{differentially private (DP) quantization mechanism} that replaces deterministic angular quantization with an $\\varepsilon$-DP stochastic quantizer applied directly to the Givens parameters of the transmit beamforming matrix. The mechanism preserves the 802.11 feedback structure, admits closed-form sensitivity bounds for the angular representation, and enables principled privacy calibration. Numerical simulations demonstrate strong privacy guarantees with minimal degradation in beamforming performance.\nlink: https://arxiv.org/abs/2512.18529v1\n"}}
{"custom_id": "2509.16625v5", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Self-Supervised Learning of Graph Representations for Network Intrusion Detection\nsummary: Detecting intrusions in network traffic is a challenging task, particularly under limited supervision and constantly evolving attack patterns. While recent works have leveraged graph neural networks for network intrusion detection, they often decouple representation learning from anomaly detection, limiting the utility of the embeddings for identifying attacks. We propose GraphIDS, a self-supervised intrusion detection model that unifies these two stages by learning local graph representations of normal communication patterns through a masked autoencoder. An inductive graph neural network embeds each flow with its local topological context to capture typical network behavior, while a Transformer-based encoder-decoder reconstructs these embeddings, implicitly learning global co-occurrence patterns via self-attention without requiring explicit positional information. During inference, flows with unusually high reconstruction errors are flagged as potential intrusions. This end-to-end framework ensures that embeddings are directly optimized for the downstream task, facilitating the recognition of malicious traffic. On diverse NetFlow benchmarks, GraphIDS achieves up to 99.98% PR-AUC and 99.61% macro F1-score, outperforming baselines by 5-25 percentage points.\nlink: https://arxiv.org/abs/2509.16625v5\n"}}
{"custom_id": "2512.18517v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Exploring Runtime Evolution in Android: A Cross-Version Analysis and Its Implications for Memory Forensics\nsummary: Userland memory forensics has become a critical component of smartphone investigations and incident response, enabling the recovery of volatile evidence such as deleted messages from end-to-end encrypted apps and cryptocurrency transactions. However, these forensics tools, particularly on Android, face significant challenges in adapting to different versions and maintaining reliability over time due to the constant evolution of low-level structures critical for evidence recovery and reconstruction. Structural changes, ranging from simple offset modifications to complete architectural redesigns, pose substantial maintenance and adaptability issues for forensic tools that rely on precise structure interpretation. Thus, this paper presents the first systematic study of Android Runtime (ART) structural evolution and its implications for memory forensics. We conduct an empirical analysis of critical Android runtime structures, examining their evolution across six versions for four different architectures. Our findings reveal that over 73.2% of structure members underwent positional changes, significantly affecting the adaptability and reliability of memory forensic tools. Further analysis of core components such as Runtime, Thread, and Heap structures highlights distinct evolution patterns and their impact on critical forensic operations, including thread state enumeration, memory mapping, and object reconstruction. These results demonstrate that traditional approaches relying on static structure definitions and symbol-based methods, while historically reliable, are increasingly unsustainable on their own. We recommend that memory forensic tools in general and Android in particular evolve toward hybrid approaches that retain the validation strength of symbolic methods while integrating automated structure inference, version-aware parsing, and redundant analysis strategies.\nlink: https://arxiv.org/abs/2512.18517v1\n"}}
{"custom_id": "2512.18495v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Enhancing Decision-Making in Windows PE Malware Classification During Dataset Shifts with Uncertainty Estimation\nsummary: Artificial intelligence techniques have achieved strong performance in classifying Windows Portable Executable (PE) malware, but their reliability often degrades under dataset shifts, leading to misclassifications with severe security consequences. To address this, we enhance an existing LightGBM (LGBM) malware detector by integrating Neural Networks (NN), PriorNet, and Neural Network Ensembles, evaluated across three benchmark datasets: EMBER, BODMAS, and UCSB. The UCSB dataset, composed mainly of packed malware, introduces a substantial distributional shift relative to EMBER and BODMAS, making it a challenging testbed for robustness. We study uncertainty-aware decision strategies, including probability thresholding, PriorNet, ensemble-derived estimates, and Inductive Conformal Evaluation (ICE). Our main contribution is the use of ensemble-based uncertainty estimates as Non-Conformity Measures within ICE, combined with a novel threshold optimisation method. On the UCSB dataset, where the shift is most severe, the state-of-the-art probability-based ICE (SOTA) yields an incorrect acceptance rate (IA%) of 22.8%. In contrast, our method reduces this to 16% a relative reduction of about 30% while maintaining competitive correct acceptance rates (CA%). These results demonstrate that integrating ensemble-based uncertainty with conformal prediction provides a more reliable safeguard against misclassifications under extreme dataset shifts, particularly in the presence of packed malware, thereby offering practical benefits for real-world security operations.\nlink: https://arxiv.org/abs/2512.18495v1\n"}}
{"custom_id": "2512.18493v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Cyber Threat Detection Enabled by Quantum Computing\nsummary: Threat detection models in cybersecurity must keep up with shifting traffic, strict feature budgets, and noisy hardware, yet even strong classical systems still miss rare or borderline attacks when the data distribution drifts. Small, near-term quantum processors are now available, but existing work rarely shows whether quantum components can improve end-to-end detection under these unstable, resource constrained conditions rather than just adding complexity. We address this gap with a hybrid architecture that uses a compact multilayer perceptron to compress security data and then routes a few features to 2-4 qubit quantum heads implemented as quantum support vector machines and variational circuits. Under matched preprocessing and training budgets, we benchmark these hybrids against tuned classical baselines on two security tasks, network intrusion detection on NSL-KDD and spam filtering on Ling-Spam datasets, and then deploy the best 4-qubit quantum SVM to an IBM Quantum device with noise-aware execution (readout mitigation and dynamical decoupling). Across both datasets, shallow quantum heads consistently match, and on difficult near-boundary cases modestly reduce, missed attacks and false alarms relative to classical models using the same features. Hardware results track simulator behavior closely enough that the remaining gap is dominated by device noise rather than model design. Taken together, the study shows that even on small, noisy chips, carefully engineered quantum components can already function as competitive, budget-aware elements in practical threat detection pipelines.\nlink: https://arxiv.org/abs/2512.18493v1\n"}}
{"custom_id": "2512.18488v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: QLink: Quantum-Safe Bridge Architecture for Blockchain Interoperability\nsummary: Secure interoperability across heterogeneous blockchains remains one of the most pressing challenges in Web3 with existing bridge protocols vulnerable to both classical exploits and emerging quantum threats. This paper introduces QLink a quantum-safe Layer 3 interoperability protocol that integrates postquantum cryptography (PQC) quantum key distribution (QKD) and hardware security modules (HSMs) into a unified validator architecture. To our knowledge, QLink is the first interoperability framework to combine these mechanisms to secure validator communication proof aggregation and key management. Validators exchange encryption keys through QKD channels, achieving information-theoretic security against interception, while cross-chain proofs are generated and aggregated with NIST-standardized PQC algorithms. Private keys remain sealed inside HSM enclaves mitigating the risk of theft or leakage. Deployed as a dedicated Layer 3 protocol QLink operates independently of Layer 1 and Layer 2 chains providing a scalable decentralized foundation for secure cross-chain messaging and asset transfer. Experimental evaluation using network simulations demonstrates that validator communication overhead remains sub-second while security guarantees extend beyond current bridge architectures to resist both classical and quantum adversaries. By addressing today vulnerabilities and anticipating future quantum threats QLink establishes a practical and future-proof pathway for blockchain interoperability.\nlink: https://arxiv.org/abs/2512.18488v1\n"}}
{"custom_id": "2512.18483v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Insider Threat Detection Using GCN and Bi-LSTM with Explicit and Implicit Graph Representations\nsummary: Insider threat detection (ITD) is challenging due to the subtle and concealed nature of malicious activities performed by trusted users. This paper proposes a post-hoc ITD framework that integrates explicit and implicit graph representations with temporal modelling to capture complex user behaviour patterns. An explicit graph is constructed using predefined organisational rules to model direct relationships among user activities. To mitigate noise and limitations in this hand-crafted structure, an implicit graph is learned from feature similarities using the Gumbel-Softmax trick, enabling the discovery of latent behavioural relationships. Separate Graph Convolutional Networks (GCNs) process the explicit and implicit graphs to generate node embeddings, which are concatenated and refined through an attention mechanism to emphasise threat-relevant features. The refined representations are then passed to a bidirectional Long Short-Term Memory (Bi-LSTM) network to capture temporal dependencies in user behaviour. Activities are flagged as anomalous when their probability scores fall below a predefined threshold. Extensive experiments on CERT r5.2 and r6.2 datasets demonstrate that the proposed framework outperforms state-of-the-art methods. On r5.2, the model achieves an AUC of 98.62, a detection rate of 100%, and a false positive rate of 0.05. On the more challenging r6.2 dataset, it attains an AUC of 88.48, a detection rate of 80.15%, and a false positive rate of 0.15, highlighting the effectiveness of combining graph-based and temporal representations for robust ITD.\nlink: https://arxiv.org/abs/2512.18483v1\n"}}
{"custom_id": "2508.07745v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation\nsummary: Insider threats pose a persistent and critical security risk, yet are notoriously difficult to detect in complex enterprise environments, where malicious actions are often hidden within seemingly benign user behaviors. Although machine-learning-based insider threat detection (ITD) methods have shown promise, their effectiveness is fundamentally limited by the scarcity of high-quality and realistic training data. Enterprise internal data is highly sensitive and rarely accessible, while existing public and synthetic datasets are either small-scale or lack sufficient realism, semantic richness, and behavioral diversity.\n  To address this challenge, we propose Chimera, an LLM-based multi-agent framework that automatically simulates both benign and malicious insider activities and generates comprehensive system logs across diverse enterprise environments. Chimera models each agent as an individual employee with fine-grained roles and supports group meetings, pairwise interactions, and self-organized scheduling to capture realistic organizational dynamics. Based on 15 insider attacks abstracted from real-world incidents, we deploy Chimera in three representative data-sensitive organizational scenarios and construct ChimeraLog, a new dataset for developing and evaluating ITD methods.\n  We evaluate ChimeraLog through human studies and quantitative analyses, demonstrating its diversity and realism. Experiments with existing ITD methods show substantially lower detection performance on ChimeraLog compared to prior datasets, indicating a more challenging and realistic benchmark. Moreover, despite distribution shifts, models trained on ChimeraLog exhibit strong generalization, highlighting the practical value of LLM-based multi-agent simulation for advancing insider threat detection.\nlink: https://arxiv.org/abs/2508.07745v3\n"}}
{"custom_id": "2512.18456v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: SoK: Understanding (New) Security Issues Across AI4Code Use Cases\nsummary: AI-for-Code (AI4Code) systems are reshaping software engineering, with tools like GitHub Copilot accelerating code generation, translation, and vulnerability detection. Alongside these advances, however, security risks remain pervasive: insecure outputs, biased benchmarks, and susceptibility to adversarial manipulation undermine their reliability. This SoK surveys the landscape of AI4Code security across three core applications, identifying recurring gaps: benchmark dominance by Python and toy problems, lack of standardized security datasets, data leakage in evaluation, and fragile adversarial robustness. A comparative study of six state-of-the-art models illustrates these challenges: insecure patterns persist in code generation, vulnerability detection is brittle to semantic-preserving attacks, fine-tuning often misaligns security objectives, and code translation yields uneven security benefits. From this analysis, we distill three forward paths: embedding secure-by-default practices in code generation, building robust and comprehensive detection benchmarks, and leveraging translation as a route to security-enhanced languages. We call for a shift toward security-first AI4Code, where vulnerability mitigation and robustness are embedded throughout the development life cycle.\nlink: https://arxiv.org/abs/2512.18456v1\n"}}
{"custom_id": "2512.18444v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Snowveil: A Framework for Decentralised Preference Discovery\nsummary: Aggregating subjective preferences of a large group is a fundamental challenge in computational social choice, traditionally reliant on central authorities. To address the limitations of this model, this paper introduces Decentralised Preference Discovery (DPD), the problem of determining the collective will of an electorate under constraints of censorship resistance, partial information, and asynchronous communication. We propose Snowveil, a novel framework for this task. Snowveil uses an iterative, gossip-based protocol where voters repeatedly sample the preferences of a small, random subset of the electorate to progressively converge on a collective outcome. We demonstrate the framework's modularity by designing the Constrained Hybrid Borda (CHB), a novel aggregation rule engineered to balance broad consensus with strong plurality support, and provide a rigorous axiomatic analysis of its properties. By applying a potential function and submartingale theory, we develop a multi-level analytical method to show that the system almost surely converges to a stable, single-winner in finite time, a process that can then be iterated to construct a set of winning candidates for multi-winner scenarios. This technique is largely agnostic to the specific aggregation rule, requiring only that it satisfies core social choice axioms like Positive Responsiveness, thus offering a formal toolkit for a wider class of DPD protocols. Furthermore, we present a comprehensive empirical analysis through extensive simulation, validating Snowveil's $O(n)$ scalability. Overall, this work advances the understanding of how a stable consensus can emerge from subjective, complex, and diverse preferences in decentralised systems for large electorates.\nlink: https://arxiv.org/abs/2512.18444v1\n"}}
{"custom_id": "2512.18432v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Federated Learning Based Decentralized Adaptive Intelligent Transmission Protocol for Privacy Preserving 6G Networks\nsummary: The move to 6th Generation (6G) wireless networks creates new issues with privacy, scalability, and adaptability. The data-intensive nature of 6G is not handled well by older, centralized network models. A shift toward more secure and decentralized systems is therefore required. A new framework called the Federated Learning-based Decentralized Adaptive Intelligent Transmission Protocol (AITP) is proposed to meet these challenges. The AITP uses the distributed learning of Federated Learning (FL) within a decentralized system. Transmission parameters can be adjusted intelligently in real time. User privacy is maintained by keeping raw data on local edge devices. The protocol's performance was evaluated with mathematical modeling and detailed simulations. It was shown to be superior to traditional non-adaptive and centralized AI methods across several key metrics. These included latency, network throughput, energy efficiency, and robustness. The AITP is presented as a foundational technology for future 6G networks that supports a user-centric, privacy-first design. This study is a step forward for privacy-preserving research in 6G.\nlink: https://arxiv.org/abs/2512.18432v1\n"}}
{"custom_id": "2512.18389v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Neural Proofs for Sound Verification and Control of Complex Systems\nsummary: This informal contribution presents an ongoing line of research that is pursuing a new approach to the construction of sound proofs for the formal verification and control of complex stochastic models of dynamical systems, of reactive programs and, more generally, of models of Cyber-Physical Systems. Neural proofs are made up of two key components: 1) proof rules encode requirements entailing the verification of general temporal specifications over the models of interest; and 2) certificates that discharge such rules, namely they are constructed from said proof rules with an inductive (that is, cyclic, repetitive) approach; this inductive approach involves: 2a) accessing samples from the model's dynamics and accordingly training neural networks, whilst 2b) generalising such networks via SAT-modulo-theory (SMT) queries that leverage the full knowledge of the models. In the context of sequential decision making problems over complex stochastic models, it is possible to additionally generate provably-correct policies/strategies/controllers, namely state-feedback functions that, in conjunction with neural certificates, formally attain the given specifications for the models of interest.\nlink: https://arxiv.org/abs/2512.18389v1\n"}}
{"custom_id": "2510.12812v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: We Can Hide More Bits: The Unused Watermarking Capacity in Theory and in Practice\nsummary: Despite rapid progress in deep learning-based image watermarking, the capacity of current robust methods remains limited to the scale of only a few hundred bits. Such plateauing progress raises the question: How far are we from the fundamental limits of image watermarking? To this end, we present an analysis that establishes upper bounds on the message-carrying capacity of images under PSNR and linear robustness constraints. Our results indicate theoretical capacities are orders of magnitude larger than what current models achieve. Our experiments show this gap between theoretical and empirical performance persists, even in minimal, easily analysable setups. This suggests a fundamental problem. As proof that larger capacities are indeed possible, we train ChunkySeal, a scaled-up version of VideoSeal, which increases capacity 4 times to 1024 bits, all while preserving image quality and robustness. These findings demonstrate modern methods have not yet saturated watermarking capacity, and that significant opportunities for architectural innovation and training strategies remain.\nlink: https://arxiv.org/abs/2510.12812v2\n"}}
{"custom_id": "2409.11068v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: A Reinforcement Learning Environment for Automatic Code Optimization in the MLIR Compiler\nsummary: Code optimization is a crucial task that aims to enhance code performance. However, this process is often tedious and complex, highlighting the necessity for automatic code optimization techniques. Reinforcement Learning (RL) has emerged as a promising approach for tackling such complex optimization problems. In this project, we introduce MLIR RL, an RL environment for the MLIR compiler, dedicated to facilitating MLIR compiler research and enabling automatic code optimization. We propose a multi-discrete formulation of the action space where the action space is the Cartesian product of simpler action subspaces. We also propose a new method, called level pointers, to reduce the size of the action space related to the loop interchange transformation. This enables more efficient and effective learning of the policy. To demonstrate the effectiveness of MLIR RL, we train an RL agent to optimize MLIR Linalg code, targeting CPU. The code is generated from two domain-specific frameworks: deep-learning models generated from PyTorch, and LQCD (Lattice Quantum Chromodynamics) code generated from an LQCD compiler. The result of this work is a research environment that allows the community to experiment with novel ideas in RL-driven loop-nest optimization.\nlink: https://arxiv.org/abs/2409.11068v2\n"}}
{"custom_id": "2510.06445v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: A Survey on Agentic Security: Applications, Threats and Defenses\nsummary: In this work we present the first holistic survey of the agentic security landscape, structuring the field around three fundamental pillars: Applications, Threats, and Defenses. We provide a comprehensive taxonomy of over 160 papers, explaining how agents are used in downstream cybersecurity applications, inherent threats to agentic systems, and countermeasures designed to protect them. A detailed cross-cutting analysis shows emerging trends in agent architecture while revealing critical research gaps in model and modality coverage. A complete and continuously updated list of all surveyed papers is publicly available at https://github.com/kagnlp/Awesome-Agentic-Security.\nlink: https://arxiv.org/abs/2510.06445v2\n"}}
{"custom_id": "2512.18345v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Theodosian: A Deep Dive into Memory-Hierarchy-Centric FHE Acceleration\nsummary: Fully homomorphic encryption (FHE) enables secure computation on encrypted data, mitigating privacy concerns in cloud and edge environments. However, due to its high compute and memory demands, extensive acceleration research has been pursued across diverse hardware platforms, especially GPUs. In this paper, we perform a microarchitectural analysis of CKKS, a popular FHE scheme, on modern GPUs. We focus on on-chip cache behavior, and show that the dominant kernels remain bound by memory bandwidth despite a high-bandwidth L2 cache, exposing a persistent memory wall. We further discover that the overall CKKS pipeline throughput is constrained by low per-kernel hardware utilization, caused by insufficient intra-kernel parallelism. Motivated by these findings, we introduce Theodosian, a set of complementary, memory-aware optimizations that improve cache efficiency and reduce runtime overheads. Our approach delivers consistent speedups across various CKKS workloads. On an RTX 5090, we reduce the bootstrapping latency for 32,768 complex numbers to 15.2ms with Theodosian, and further to 12.8ms with additional algorithmic optimizations, establishing new state-of-the-art GPU performance to the best of our knowledge.\nlink: https://arxiv.org/abs/2512.18345v1\n"}}
{"custom_id": "2512.18334v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Faster Vertex Cover Algorithms on GPUs with Component-Aware Parallel Branching\nsummary: Algorithms for finding minimum or bounded vertex covers in graphs use a branch-and-reduce strategy, which involves exploring a highly imbalanced search tree. Prior GPU solutions assign different thread blocks to different sub-trees, while using a shared worklist to balance the load. However, these prior solutions do not scale to large and complex graphs because their unawareness of when the graph splits into components causes them to solve these components redundantly. Moreover, their high memory footprint limits the number of workers that can execute concurrently. We propose a novel GPU solution for vertex cover problems that detects when a graph splits into components and branches on the components independently. Although the need to aggregate the solutions of different components introduces non-tail-recursive branches which interfere with load balancing, we overcome this challenge by delegating the post-processing to the last descendant of each branch. We also reduce the memory footprint by reducing the graph and inducing a subgraph before exploring the search tree. Our solution substantially outperforms the state-of-the-art GPU solution, finishing in seconds when the state-of-the-art solution exceeds 6 hours. To the best of our knowledge, our work is the first to parallelize non-tail-recursive branching patterns on GPUs in a load balanced manner.\nlink: https://arxiv.org/abs/2512.18334v1\n"}}
{"custom_id": "2512.18318v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems\nsummary: This paper introduces a parallel and asynchronous Transformer framework designed for efficient and accurate multilingual lip synchronization in real-time video conferencing systems. The proposed architecture integrates translation, speech processing, and lip-synchronization modules within a pipeline-parallel design that enables concurrent module execution through message-queue-based decoupling, reducing end-to-end latency by up to 3.1 times compared to sequential approaches. To enhance computational efficiency and throughput, the inference workflow of each module is optimized through low-level graph compilation, mixed-precision quantization, and hardware-accelerated kernel fusion. These optimizations provide substantial gains in efficiency while preserving model accuracy and visual quality. In addition, a context-adaptive silence-detection component segments the input speech stream at semantically coherent boundaries, improving translation consistency and temporal alignment across languages. Experimental results demonstrate that the proposed parallel architecture outperforms conventional sequential pipelines in processing speed, synchronization stability, and resource utilization. The modular, message-oriented design makes this work applicable to resource-constrained IoT communication scenarios including telemedicine, multilingual kiosks, and remote assistance systems. Overall, this work advances the development of low-latency, resource-efficient multimodal communication frameworks for next-generation AIoT systems.\nlink: https://arxiv.org/abs/2512.18318v1\n"}}
{"custom_id": "2512.18305v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Cyber Risk Scoring with QUBO: A Quantum and Hybrid Benchmark Study\nsummary: Assessing cyber risk in complex IT infrastructures poses significant challenges due to the dynamic, interconnected nature of digital systems. Traditional methods often fall short, relying on static and largely qualitative models that do not scale with system complexity and fail to capture systemic interdependencies. In this work, we introduce a novel quantitative approach to cyber risk assessment based on Quadratic Unconstrained Binary Optimization (QUBO), a formulation compatible with both classical computing and quantum annealing. We demonstrate the capabilities of our approach using a realistic 255-nodes layered infrastructure, showing how risk spreads in non-trivial patterns that are difficult to identify through visual inspection alone. To assess scalability, we further conduct extensive experiments on networks up to 1000 nodes comparing classical, quantum, and hybrid classical-quantum workflows. Our results reveal that although quantum annealing produces solutions comparable to classical heuristics, its potential advantages are significantly hindered by the embedding overhead required to map the densely connected cyber-risk QUBO onto the limited connectivity of current quantum hardware. By contrast, hybrid quantum-classical solvers avoid this bottleneck and therefore emerge as a promising option, combining competitive scaling with an improved ability to explore the solution space and identify more stable risk configurations. Overall, this work delivers two main advances. First, we present a rigorous, tunable, and generalizable mathematical model for cyber risk that can be adapted to diverse infrastructures and domains through flexible parameterization. Second, we provide the first comparative study of classical, quantum, and hybrid approaches for cyber risk scoring at scale, highlighting the emerging potential of hybrid quantum-classical methods for large-scale infrastructures.\nlink: https://arxiv.org/abs/2512.18305v1\n"}}
{"custom_id": "2512.18303v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: MORPHEUS: A Multidimensional Framework for Modeling, Measuring, and Mitigating Human Factors in Cybersecurity\nsummary: Current cybersecurity research increasingly acknowledges the human factor, yet remains fragmented, often treating user vulnerabilities as isolated and static traits. This paper introduces MORPHEUS, a holistic framework that operationalizes human-centric security as a dynamic and interconnected system. Grounded in the Cognition-Affect-Behavior (CAB) model and Attribution Theory, MORPHEUS consolidates 50 human factors influencing susceptibility to major cyberthreats, including phishing, malware, password management, and misconfigurations. Beyond factor identification, the framework systematically maps 295 documented interactions, revealing how cognitive, emotional, behavioral, and socio-organizational processes jointly shape security outcomes, and distills them into twelve recurring interaction mechanisms. MORPHEUS further links theory to practice through an inventory of 99 validated psychometric instruments, enabling empirical assessment and targeted intervention. We illustrate the framework's applicability through concrete operational scenarios, spanning risk diagnosis, training, and interface design. Overall, MORPHEUS provides a rigorous yet actionable foundation for advancing human-centered cybersecurity research and practice.\nlink: https://arxiv.org/abs/2512.18303v1\n"}}
{"custom_id": "2407.08651v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: SpiralShard: Highly Concurrent and Secure Blockchain Sharding via Linked Cross-shard Endorsement\nsummary: Blockchain sharding improves the scalability of blockchain systems by partitioning the whole blockchain state, nodes, and transaction workloads into different shards. However, existing blockchain sharding systems generally suffer from a small number of shards, resulting in limited concurrency. The main reason is that existing sharding systems require large shard sizes to ensure security.\n  To enhance the concurrency of blockchain sharding securely, we propose SpiralShard. The intuition is to allow the existence of some shards with a larger fraction of malicious nodes (i.e., corrupted shards), thus reducing shard sizes. SpiralShard can configure more and smaller shards for higher concurrency at the same network size. To ensure security with the existence of corrupted shards, we propose the Linked Cross-shard Endorsement (LCE) protocol. According to our LCE protocol, the blocks of each shard are sequentially verified and endorsed (via intra-shard consensus) by a group of shards before being finalized. As a result, a corrupted shard can eliminate forks with the help of the other shards. We implement SpiralShard based on Harmony and conduct extensive evaluations. Experimental results show that, compared with Harmony, SpiralShard achieves around 19x throughput gain under a large network size with 4,000+ nodes.\nlink: https://arxiv.org/abs/2407.08651v2\n"}}
{"custom_id": "2407.06882v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: StableShard: Stable and Scalable Blockchain Sharding with High Concurrency via Collaborative Committees\nsummary: Sharding enhances blockchain scalability by partitioning nodes into multiple groups for concurrent transaction processing. Configuring a large number of small shards usually helps improve transaction concurrency, but it also increases the fraction of malicious nodes in each shard, easily causing shard corruption and jeopardizing system security. Existing works attempt to improve concurrency by reducing shard sizes while maintaining security, but typically rely on time-consuming recovery of corrupted shards to restore liveness and network-wide consensus. This causes severe system stagnation and limits scalability.\n  To address this, we present StableShard, a sharded blockchain that securely provides high concurrency with stable and scalable performance. The core idea is to carefully co-design the division of labor between proposer shards (PSs) and finalizer committees (FCs): we deliberately assign 1) asymmetric roles and 2) matching parameters to PSs and FCs. Small PSs focus on fast transaction proposal and local validity, while large FCs focus on resolving forks, finalizing PS blocks, and maintaining liveness for faulty PSs via a cross-layer view-change protocol. Moreover, by fine-tuning key system parameters (e.g., shard size, quorum size), we ensure each PS to tolerate <1/2 fraction of malicious nodes without lossing liveness, and allow multiple FCs to securely coexist (each with <1/3 fraction of malicious nodes) for better scalability. Consequently, StableShard can safely configure many smaller PSs to boost concurrency, while FCs and PSs jointly guarantee safety and liveness without system stagnation, leading to stable and scalable performance. Evaluations show that StableShard achieves up to 10x higher throughput than existing solutions and significantly more stable concurrency under attacks.\nlink: https://arxiv.org/abs/2407.06882v2\n"}}
{"custom_id": "2512.18296v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Privacy Data Pricing: A Stackelberg Game Approach\nsummary: Data markets are emerging as key mechanisms for trading personal and organizational data. Traditional data pricing studies -- such as query-based or arbitrage-free pricing models -- mainly emphasize price consistency and profit maximization but often neglect privacy constraints and strategic interactions. The widespread adoption of differential privacy (DP) introduces a fundamental privacy-utility trade-off: noise protects individuals' privacy but reduces data accuracy and market value. This paper develops a Stackelberg game framework for pricing DP data, where the market maker (leader) sets the price function and the data buyer (follower) selects the optimal query precision under DP constraints. We derive the equilibrium strategies for both parties under a balanced pricing function where the pricing decision variable enters linearly into the original pricing model. We obtain closed-form solutions for the optimal variance and pricing level, and determine the boundary conditions for market participation. Furthermore, we extend the analysis to Stackelberg games involving nonlinear power pricing functions. The model bridges DP and economic mechanism design, offering a unified foundation for incentive-compatible and privacy-conscious data pricing in data markets.\nlink: https://arxiv.org/abs/2512.18296v1\n"}}
{"custom_id": "2512.18264v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Who Can See Through You? Adversarial Shielding Against VLM-Based Attribute Inference Attacks\nsummary: As vision-language models (VLMs) become widely adopted, VLM-based attribute inference attacks have emerged as a serious privacy concern, enabling adversaries to infer private attributes from images shared on social media. This escalating threat calls for dedicated protection methods to safeguard user privacy. However, existing methods often degrade the visual quality of images or interfere with vision-based functions on social media, thereby failing to achieve a desirable balance between privacy protection and user experience. To address this challenge, we propose a novel protection method that jointly optimizes privacy suppression and utility preservation under a visual consistency constraint. While our method is conceptually effective, fair comparisons between methods remain challenging due to the lack of publicly available evaluation datasets. To fill this gap, we introduce VPI-COCO, a publicly available benchmark comprising 522 images with hierarchically structured privacy questions and corresponding non-private counterparts, enabling fine-grained and joint evaluation of protection methods in terms of privacy preservation and user experience. Building upon this benchmark, experiments on multiple VLMs demonstrate that our method effectively reduces PAR below 25%, keeps NPAR above 88%, maintains high visual consistency, and generalizes well to unseen and paraphrased privacy questions, demonstrating its strong practical applicability for real-world VLM deployments.\nlink: https://arxiv.org/abs/2512.18264v1\n"}}
{"custom_id": "2512.18256v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: MSC-180: A Benchmark for Automated Formal Theorem Proving from Mathematical Subject Classification\nsummary: Automated Theorem Proving (ATP) represents a core research direction in artificial intelligence for achieving formal reasoning and verification, playing a significant role in advancing machine intelligence. However, current large language model (LLM)-based theorem provers suffer from limitations such as restricted domain coverage and weak generalization in mathematical reasoning. To address these issues, we propose MSC-180, a benchmark for evaluation based on the MSC2020 mathematical subject classification. It comprises 180 formal verification problems, 3 advanced problems from each of 60 mathematical branches, spanning from undergraduate to graduate levels. Each problem has undergone multiple rounds of verification and refinement by domain experts to ensure formal accuracy. Evaluations of state-of-the-art LLM-based theorem provers under the pass@32 setting reveal that the best model achieves only an 18.89% overall pass rate, with prominent issues including significant domain bias (maximum domain coverage 41.7%) and a difficulty gap (significantly lower pass rates on graduate-level problems). To further quantify performance variability across mathematical domains, we introduce the coefficient of variation (CV) as an evaluation metric. The observed CV values are 4-6 times higher than the statistical high-variability threshold, indicating that the models still rely on pattern matching from training corpora rather than possessing transferable reasoning mechanisms and systematic generalization capabilities. MSC-180, together with its multi-dimensional evaluation framework, provides a discriminative and systematic benchmark for driving the development of next-generation AI systems with genuine mathematical reasoning abilities.\nlink: https://arxiv.org/abs/2512.18256v1\n"}}
{"custom_id": "2512.18244v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Breaking Minds, Breaking Systems: Jailbreaking Large Language Models via Human-like Psychological Manipulation\nsummary: Large Language Models (LLMs) have gained considerable popularity and protected by increasingly sophisticated safety mechanisms. However, jailbreak attacks continue to pose a critical security threat by inducing models to generate policy-violating behaviors. Current paradigms focus on input-level anomalies, overlooking that the model's internal psychometric state can be systematically manipulated. To address this, we introduce Psychological Jailbreak, a new jailbreak attack paradigm that exposes a stateful psychological attack surface in LLMs, where attackers exploit the manipulation of a model's psychological state across interactions. Building on this insight, we propose Human-like Psychological Manipulation (HPM), a black-box jailbreak method that dynamically profiles a target model's latent psychological vulnerabilities and synthesizes tailored multi-turn attack strategies. By leveraging the model's optimization for anthropomorphic consistency, HPM creates a psychological pressure where social compliance overrides safety constraints. To systematically measure psychological safety, we construct an evaluation framework incorporating psychometric datasets and the Policy Corruption Score (PCS). Benchmarking against various models (e.g., GPT-4o, DeepSeek-V3, Gemini-2-Flash), HPM achieves a mean Attack Success Rate (ASR) of 88.1%, outperforming state-of-the-art attack baselines. Our experiments demonstrate robust penetration against advanced defenses, including adversarial prompt optimization (e.g., RPO) and cognitive interventions (e.g., Self-Reminder). Ultimately, PCS analysis confirms HPM induces safety breakdown to satisfy manipulated contexts. Our work advocates for a fundamental paradigm shift from static content filtering to psychological safety, prioritizing the development of psychological defense mechanisms against deep cognitive manipulation.\nlink: https://arxiv.org/abs/2512.18244v1\n"}}
{"custom_id": "2512.18207v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: FedWiLoc: Federated Learning for Privacy-Preserving WiFi Indoor Localization\nsummary: Current data-driven Wi-Fi-based indoor localization systems face three critical challenges: protecting user privacy, achieving accurate predictions in dynamic multipath environments, and generalizing across different deployments. Traditional Wi-Fi localization systems often compromise user privacy, particularly when facing compromised access points (APs) or man-in-the-middle attacks. As IoT devices proliferate in indoor environments, developing solutions that deliver accurate localization while robustly protecting privacy has become imperative. We introduce FedWiLoc, a privacy-preserving indoor localization system that addresses these challenges through three key innovations. First, FedWiLoc employs a split architecture where APs process Channel State Information (CSI) locally and transmit only privacy-preserving embedding vectors to user devices, preventing raw CSI exposure. Second, during training, FedWiLoc uses federated learning to collaboratively train the model across APs without centralizing sensitive user data. Third, we introduce a geometric loss function that jointly optimizes angle-of-arrival predictions and location estimates, enforcing geometric consistency to improve accuracy in challenging multipath conditions. Extensive evaluation across six diverse indoor environments spanning over 2,000 sq. ft. demonstrates that FedWiLoc outperforms state-of-the-art methods by up to 61.9% in median localization error while maintaining strong privacy guarantees throughout both training and inference.\nlink: https://arxiv.org/abs/2512.18207v1\n"}}
{"custom_id": "2510.02930v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: iDDS: Intelligent Distributed Dispatch and Scheduling for Workflow Orchestration\nsummary: The intelligent Distributed Dispatch and Scheduling (iDDS) service is a versatile workflow orchestration system designed for large-scale, distributed scientific computing. iDDS extends traditional workload and data management by integrating data-aware execution, conditional logic, and programmable workflows, enabling automation of complex and dynamic processing pipelines. Originally developed for the ATLAS experiment at the Large Hadron Collider, iDDS has evolved into an experiment-agnostic platform that supports both template-driven workflows and a Function-as-a-Task model for Python-based orchestration.\n  This paper presents the architecture and core components of iDDS, highlighting its scalability, modular message-driven design, and integration with systems such as PanDA and Rucio. We demonstrate its versatility through real-world use cases: fine-grained tape resource optimization for ATLAS, orchestration of large Directed Acyclic Graph (DAG) workflows for the Rubin Observatory, distributed hyperparameter optimization for machine learning applications, active learning for physics analyses, and AI-assisted detector design at the Electron-Ion Collider.\n  By unifying workload scheduling, data movement, and adaptive decision-making, iDDS reduces operational overhead and enables reproducible, high-throughput workflows across heterogeneous infrastructures. We conclude with current challenges and future directions, including interactive, cloud-native, and serverless workflow support.\nlink: https://arxiv.org/abs/2510.02930v2\n"}}
{"custom_id": "2512.18199v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: PROVEX: Enhancing SOC Analyst Trust with Explainable Provenance-Based IDS\nsummary: Modern intrusion detection systems (IDS) leverage graph neural networks (GNNs) to detect malicious activity in system provenance data, but their decisions often remain a black box to analysts. This paper presents a comprehensive XAI framework designed to bridge the trust gap in Security Operations Centers (SOCs) by making graph-based detection transparent. We implement this framework on top of KAIROS, a state-of-the-art temporal graph-based IDS, though our design is applicable to any temporal graph-based detector with minimal adaptation. The complete codebase is available at https://github.com/devang1304/provex.git. We augment the detection pipeline with post-hoc explanations that highlight why an alert was triggered, identifying key causal subgraphs and events. We adapt three GNN explanation methods - GraphMask, GNNExplainer, and a variational temporal GNN explainer (VA-TGExplainer) - to the temporal provenance context. These tools output human-interpretable representations of anomalous behavior, including important edges and uncertainty estimates. Our contributions focus on the practical integration of these explainers, addressing challenges in memory management and reproducibility. We demonstrate our framework on the DARPA CADETS Engagement 3 dataset and show that it produces concise window-level explanations for detected attacks. Our evaluation reveals that the explainers preserve the TGNN's decisions with high fidelity, surfacing critical edges such as malicious file interactions and anomalous netflows. The average explanation overhead is 3-5 seconds per event. By providing insight into the model's reasoning, our framework aims to improve analyst trust and triage speed.\nlink: https://arxiv.org/abs/2512.18199v1\n"}}
{"custom_id": "2512.18194v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale\nsummary: Disaggregated LLM serving improves resource efficiency by separating the compute-intensive prefill phase from the latency-critical decode phase. However, this architecture introduces a fundamental bottleneck: key/value (KV) tensors generated during prefill must be transferred to decode workers, and existing systems rely on RDMA-based network paths for this exchange. As model sizes and context lengths increase, KV transfer dominates both time-to-first-token (TTFT) and peak throughput, and remains highly sensitive to network contention even when prefix reuse is high. This paper presents TraCT, a rack-scale LLM serving system that uses CXL shared memory as both a KV-transfer substrate and a rack-wide prefix-aware KV cache. TraCT enables GPUs to write and read KV blocks directly through CXL load/store and DMA operations, eliminating the NIC hop that constrains existing disaggregated pipelines. However, to realize this design, multiple new challenges such as synchronization, consistency, and data management on non-coherent CXL memory need to be addressed. TraCT proposes various software solutions such as the two-tier inter-node synchronization mechanism to address these challenges. We implement TraCT on the Dynamo LLM inference framework and show that, across static and synthetic workloads, TraCT reduces average TTFT by up to 9.8x, lowers P99 latency by up to 6.2x, and improves peak throughput by up to 1.6x compared to RDMA and DRAM-based caching baselines.\nlink: https://arxiv.org/abs/2512.18194v1\n"}}
{"custom_id": "2512.10094v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Does Timeboost Reduce MEV-Related Spam? Theory and Evidence from Layer-2 Transactions\nsummary: Maximal extractable value opportunities often induce spam in Layer-2 blockchains: many identical transactions are submitted near simultaneously, most of which revert, wasting blockspace. We study Timeboost, a mechanism on Arbitrum that auctions a timestamp advantage, crucial under first-come first-served sequencing rules. We develop a game-theoretic model in which users choose the number of transaction copies to submit, and extend upon the baseline setting by modeling the Timeboost auction and subsequent transaction submission behavior. We show that Timeboost reduces spam and increases sequencer/DAO revenue in equilibrium relative to the baseline, transferring user payments from revert costs to auction bids. Empirically, we assemble mempool data from multiple Layer-2 networks, measuring spam via identical transactions submitted in narrow time intervals, and conduct an event study around Timeboost adoption on Arbitrum using other L2s as contemporaneous benchmarks. We find a decline in MEV-related spam and an increase in revenue on Arbitrum post-adoption, consistent with model predictions.\nlink: https://arxiv.org/abs/2512.10094v2\n"}}
{"custom_id": "2512.18174v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Conscious Data Contribution via Community-Driven Chain-of-Thought Distillation\nsummary: The current era of AI development places a heavy emphasis on training large models on increasingly scaled-up datasets. This paradigm has catalyzed entirely new product categories, such as LLM chatbots, while also raising concerns about data privacy and consumer choice. In this paper, we consider questions of data portability and user autonomy in the context of LLMs that \"reason\" using chain-of-thought (CoT) traces, computing intermediate text artifacts from user input before producing a final output. We first interpret recent data privacy and portability law to argue that these intermediate computations qualify as users' personal data. Then, building on the existing framework of Conscious Data Contribution, we show how communities who receive low utility from an available model can aggregate and distill their shared knowledge into an alternate model better aligned with their goals. We verify this approach empirically and investigate the effects of community diversity, reasoning granularity, and community size on distillation performance.\nlink: https://arxiv.org/abs/2512.18174v1\n"}}
{"custom_id": "2502.00138v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: JustAct+: A Framework for Auditable Multi-Agent Systems Regulated by Inter-Organisational Policies\nsummary: In open multi-agent agent systems that cross organisational boundaries, agent actions must be regulated by complex policies. Consider medical data processing systems, which must observe generic laws (e.g., EU data protection regulations) and also specific participants' resource conditions (e.g., Bob consents to sharing his X-Rays with EU hospitals). Presently, we address the implementation of these systems as distributed software. Solutions to key sub-problems are available: existing policy languages capture the necessary normative concepts and formalise the computational representation and reasoning about policies, and existing distributed algorithms and protocols coordinate agents' changing actions and policies. But which policies and protocols are useful in application? With the JustAct framework, we characterise a class of multi-agent systems where actors justify their actions with sufficient policy information collected from dynamic policy statements and agreements. We prove key properties of these systems, e.g., any decision that an action is permitted now cannot be refuted later, regardless of any added statements or updated agreements. We study a particular instance of the framework by specifying (in Rocq) and implementing (in Rust) a particular policy language and runtime system for mediating agent communications. We demonstrate and assess JustAct via a case study of this implementation: we reproduce the usage scenarios of Brane, an existing policy-regulated, inter-domain, medical data processing system.\nlink: https://arxiv.org/abs/2502.00138v2\n"}}
{"custom_id": "2504.08264v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: To See or Not to See -- Fingerprinting Devices in Adversarial Environments Amid Advanced Machine Learning\nsummary: The increasing use of the Internet of Things raises security concerns. To address this, device fingerprinting is often employed to authenticate devices, detect adversaries, and identify eavesdroppers in an environment. This requires the ability to discern between legitimate and malicious devices which is achieved by analyzing the unique physical and/or operational characteristics of IoT devices. In the era of the latest progress in machine learning, particularly generative models, it is crucial to methodically examine the current studies in device fingerprinting. This involves explaining their approaches and underscoring their limitations when faced with adversaries armed with these ML tools. To systematically analyze existing methods, we propose a generic, yet simplified, model for device fingerprinting. Additionally, we thoroughly investigate existing methods to authenticate devices and detect eavesdropping, using our proposed model. We further study trends and similarities between works in authentication and eavesdropping detection and present the existing threats and attacks in these domains. Finally, we discuss future directions in fingerprinting based on these trends to develop more secure IoT fingerprinting schemes.\nlink: https://arxiv.org/abs/2504.08264v2\n"}}
{"custom_id": "2511.15936v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Lifefin: Escaping Mempool Explosions in DAG-based BFT\nsummary: Directed Acyclic Graph (DAG)-based Byzantine Fault-Tolerant (BFT) protocols have emerged as promising solutions for high-throughput blockchains. By decoupling data dissemination from transaction ordering and constructing a well-connected DAG in the mempool, these protocols enable zero-message ordering and implicit view changes. However, we identify a fundamental liveness vulnerability: an adversary can trigger mempool explosions to prevent transaction commitment, ultimately compromising the protocol's liveness.\n  In response, this work presents Lifefin, a generic and self-stabilizing protocol designed to integrate seamlessly with existing DAG-based BFT protocols and circumvent such vulnerabilities. Lifefin leverages the Agreement on Common Subset (ACS) mechanism, allowing nodes to escape mempool explosions by committing transactions with bounded resource usage even in adverse conditions. As a result, Lifefin imposes (almost) zero overhead in typical cases while effectively eliminating liveness vulnerabilities.\n  To demonstrate the effectiveness of Lifefin, we integrate it into two state-of-the-art DAG-based BFT protocols, Sailfish and Mysticeti, resulting in two enhanced variants: Sailfish-Lifefin and Mysticeti-Lifefin. We implement these variants and compare them with the original Sailfish and Mysticeti systems. Our evaluation demonstrates that Lifefin achieves comparable transaction throughput while introducing only minimal additional latency to resist similar attacks.\nlink: https://arxiv.org/abs/2511.15936v2\n"}}
{"custom_id": "2512.15330v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Practical Challenges in Executing Shor's Algorithm on Existing Quantum Platforms\nsummary: Quantum computers pose a fundamental threat to widely deployed public-key cryptosystems, such as RSA and ECC, by enabling efficient integer factorization using Shor's algorithm. Theoretical resource estimates suggest that 2048-bit RSA keys could be broken using Shor's algorithm with fewer than a million noisy qubits. Although such machines do not yet exist, the availability of smaller, cloud-accessible quantum processors and open-source implementations of Shor's algorithm raises the question of what key sizes can realistically be factored with today's platforms. In this work, we experimentally investigate Shor's algorithm on several cloud-based quantum computers using publicly available implementations. Our results reveal a substantial gap between the capabilities of current quantum hardware and the requirements for factoring cryptographically relevant integers. In particular, we observe that circuit constructions still need to be highly specific for each modulus, and that machine fidelities are unstable, with high and fluctuating error rates.\nlink: https://arxiv.org/abs/2512.15330v2\n"}}
{"custom_id": "2512.18141v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Constrained Cuts, Flows, and Lattice-Linearity\nsummary: In a capacitated directed graph, it is known that the set of all min-cuts forms a distributive lattice [1], [2]. Here, we describe this lattice as a regular predicate whose forbidden elements can be advanced in constant parallel time after precomputing a max-flow, so as to obtain parallel algorithms for min-cut problems with additional constraints encoded by lattice-linear predicates [3]. Some nice algorithmic applications follow. First, we use these methods to compute the irreducibles of the sublattice of min-cuts satisfying a regular predicate. By Birkhoff's theorem [4] this gives a succinct representation of such cuts, and so we also obtain a general algorithm for enumerating this sublattice. Finally, though we prove computing min-cuts satisfying additional constraints is NP-hard in general, we use poset slicing [5], [6] for exact algorithms with constraints not necessarily encoded by lattice-linear predicates) with better complexity than exhaustive search. We also introduce $k$-transition predicates and strong advancement for improved complexity analyses of lattice-linear predicate algorithms in parallel settings, which is of independent interest.\nlink: https://arxiv.org/abs/2512.18141v1\n"}}
{"custom_id": "2512.18133v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection\nsummary: Nowadays, Graph Fraud Detection (GFD) in financial scenarios has become an urgent research topic to protect online payment security. However, as organized crime groups are becoming more professional in real-world scenarios, fraudsters are employing more sophisticated camouflage strategies. Specifically, fraudsters disguise themselves by mimicking the behavioral data collected by platforms, ensuring that their key characteristics are consistent with those of benign users to a high degree, which we call Adaptive Camouflage. Consequently, this narrows the differences in behavioral traits between them and benign users within the platform's database, thereby making current GFD models lose efficiency. To address this problem, we propose a relation diffusion-based graph augmentation model Grad. In detail, Grad leverages a supervised graph contrastive learning module to enhance the fraud-benign difference and employs a guided relation diffusion generator to generate auxiliary homophilic relations from scratch. Based on these, weak fraudulent signals would be enhanced during the aggregation process, thus being obvious enough to be captured. Extensive experiments have been conducted on two real-world datasets provided by WeChat Pay, one of the largest online payment platforms with billions of users, and three public datasets. The results show that our proposed model Grad outperforms SOTA methods in both various scenarios, achieving at most 11.10% and 43.95% increases in AUC and AP, respectively. Our code is released at https://github.com/AI4Risk/antifraud and https://github.com/Muyiiiii/WWW25-Grad.\nlink: https://arxiv.org/abs/2512.18133v1\n"}}
{"custom_id": "2512.18132v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: PermuteV: A Performant Side-channel-Resistant RISC-V Core Securing Edge AI Inference\nsummary: Edge AI inference is becoming prevalent thanks to the emergence of small yet high-performance microprocessors. This shift from cloud to edge processing brings several benefits in terms of energy savings, improved latency, and increased privacy. On the downside, bringing computation to the edge makes them more vulnerable to physical side-channel attacks (SCA), which aim to extract the confidentiality of neural network models, e.g., architecture and weight. To address this growing threat, we propose PermuteV, a performant side-channel resistant RISC-V core designed to secure neural network inference. PermuteV employs a hardware-accelerated defense mechanism that randomly permutes the execution order of loop iterations, thereby obfuscating the electromagnetic (EM) signature associated with sensitive operations. We implement PermuteV on FPGA and perform evaluations in terms of side-channel security, hardware area, and runtime overhead. The experimental results demonstrate that PermuteV can effectively defend against EM SCA with minimal area and runtime overhead.\nlink: https://arxiv.org/abs/2512.18132v1\n"}}
{"custom_id": "2512.18127v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: ACE-Sync: An Adaptive Cloud-Edge Synchronization Framework for Communication-Efficient Large-Scale Distributed Model Training\nsummary: Large-scale deep learning models impose substantial communication overh ead in distributed training, particularly in bandwidth-constrained or heterogeneous clo ud-edge environments. Conventional synchronous or fixed-compression techniques o ften struggle to balance communication cost, convergence stability, and model accura cy. To address these challenges, we propose ACE-Sync, an Adaptive Cloud-Edge Sy nchronization Framework that integrates (1) an attention-based gradient importance p redictor, (2) a differentiated parameter compression strategy, and (3) a hierarchical cl oud-edge coordination mechanism. ACE-Sync dynamically selects which parameter groups to synchronize and determines appropriate compression levels under per-devic e bandwidth budgets. A knapsack-based optimization strategy is adopted to maximize important gradient preservation while reducing redundant communication. Furthermo re, residual-based error compensation and device clustering ensure long-term converg ence and cross-device personalization. Experiments show that ACE-Sync substantiall y reduces communication overhead while maintaining competitive accuracy. Compar ed with FullSync, ACE-Sync lowers communication cost from 112.5 GB to 44.7 GB (a 60% reduction) and shortens convergence from 41 to 39 epochs. Despite aggressiv e communication reduction, ACE-Sync preserves high model quality, achieving 82. 1% Top-1 accuracy-only 0.3% below the full-synchronization baseline-demonstrating its efficiency and scalability for large-scale distributed training. These results indicate that ACE-Sync provides a scalable, communication-efficient, and accuracy-preservin g solution for large-scale cloud-edge distributed model training.\nlink: https://arxiv.org/abs/2512.18127v1\n"}}
{"custom_id": "2512.18102v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: From Coverage to Causes: Data-Centric Fuzzing for JavaScript Engines\nsummary: Context: Exhaustive fuzzing of modern JavaScript engines is infeasible due to the vast number of program states and execution paths. Coverage-guided fuzzers waste effort on low-risk inputs, often ignoring vulnerability-triggering ones that do not increase coverage. Existing heuristics proposed to mitigate this require expert effort, are brittle, and hard to adapt.\n  Objective: We propose a data-centric, LLM-boosted alternative that learns from historical vulnerabilities to automatically identify minimal static (code) and dynamic (runtime) features for detecting high-risk inputs.\n  Method: Guided by historical V8 bugs, iterative prompting generated 115 static and 49 dynamic features, with the latter requiring only five trace flags, minimizing instrumentation cost. After feature selection, 41 features remained to train an XGBoost model to predict high-risk inputs during fuzzing.\n  Results: Combining static and dynamic features yields over 85% precision and under 1% false alarms. Only 25% of these features are needed for comparable performance, showing that most of the search space is irrelevant.\n  Conclusion: This work introduces feature-guided fuzzing, an automated data-driven approach that replaces coverage with data-directed inference, guiding fuzzers toward high-risk states for faster, targeted, and reproducible vulnerability discovery. To support open science, all scripts and data are available at https://github.com/KKGanguly/DataCentricFuzzJS .\nlink: https://arxiv.org/abs/2512.18102v1\n"}}
{"custom_id": "2510.08797v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: TAPAS: Datasets for Learning the Learning with Errors Problem\nsummary: AI-powered attacks on Learning with Errors (LWE), an important hard math problem in post-quantum cryptography, rival or outperform \"classical\" attacks on LWE under certain parameter settings. Despite the promise of this approach, a dearth of accessible data limits AI practitioners' ability to study and improve these attacks. Creating LWE data for AI model training is time- and compute-intensive and requires significant domain expertise. To fill this gap and accelerate AI research on LWE attacks, we propose the TAPAS datasets, a Toolkit for Analysis of Post-quantum cryptography using AI Systems. These datasets cover several LWE settings and can be used off-the-shelf by AI practitioners to prototype new approaches to cracking LWE. This work documents TAPAS dataset creation, establishes attack performance baselines, and lays out directions for future work.\nlink: https://arxiv.org/abs/2510.08797v2\n"}}
{"custom_id": "2504.10096v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Solvers for the Hermitian and the pseudo-Hermitian Bethe-Salpeter equation in the Yambo code: Implementation and Performance\nsummary: We analyze the performance of two strategies in solving the structured eigenvalue problem deriving from the Bethe-Salpeter equation (BSE) in condensed matter physics. The BSE matrix is constructed with the Yambo code, and the two strategies are implemented by interfacing Yambo with the ScaLAPACK and ELPA libraries for direct diagonalization, and with the SLEPc library for the iterative approach. We consider both the Hermitian (Tamm-Dancoff approximation) and pseudo-Hermitian forms, addressing dense matrices of three different sizes. A description of the implementation is also provided, with details for the pseudo-Hermitian case. Timing and memory utilization are analyzed on both CPU and GPU clusters. Our results demonstrate that it is now feasible to handle dense BSE matrices of the order of 10^5.\nlink: https://arxiv.org/abs/2504.10096v3\n"}}
{"custom_id": "2510.06015v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: \"Your Doctor is Spying on You\": An Analysis of Data Practices in Mobile Healthcare Applications\nsummary: Mobile healthcare (mHealth) applications promise convenient, continuous patient-provider interaction but also introduce severe and often underexamined security and privacy risks. We present an end-to-end audit of 272 Android mHealth apps from Google Play, combining permission forensics, static vulnerability analysis, and user review mining. Our multi-tool assessment with MobSF, RiskInDroid, and OWASP Mobile Audit revealed systemic weaknesses: 26.1% request fine-grained location without disclosure, 18.3% initiate calls silently, and 73 send SMS without notice. Nearly half (49.3%) still use deprecated SHA-1 encryption, 42 transmit unencrypted data, and 6 remain vulnerable to StrandHogg 2.0. Analysis of 2.56 million user reviews found 28.5% negative or neutral sentiment, with over 553,000 explicitly citing privacy intrusions, data misuse, or operational instability. These findings demonstrate the urgent need for enforceable permission transparency, automated pre-market security vetting, and systematic adoption of secure-by-design practices to protect Protected Health Information (PHI).\nlink: https://arxiv.org/abs/2510.06015v2\n"}}
{"custom_id": "2512.18043v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Securing Agentic AI Systems -- A Multilayer Security Framework\nsummary: Securing Agentic Artificial Intelligence (AI) systems requires addressing the complex cyber risks introduced by autonomous, decision-making, and adaptive behaviors. Agentic AI systems are increasingly deployed across industries, organizations, and critical sectors such as cybersecurity, finance, and healthcare. However, their autonomy introduces unique security challenges, including unauthorized actions, adversarial manipulation, and dynamic environmental interactions. Existing AI security frameworks do not adequately address these challenges or the unique nuances of agentic AI. This research develops a lifecycle-aware security framework specifically designed for agentic AI systems using the Design Science Research (DSR) methodology. The paper introduces MAAIS, an agentic security framework, and the agentic AI CIAA (Confidentiality, Integrity, Availability, and Accountability) concept. MAAIS integrates multiple defense layers to maintain CIAA across the AI lifecycle. Framework validation is conducted by mapping with the established MITRE ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems) AI tactics. The study contributes a structured, standardized, and framework-based approach for the secure deployment and governance of agentic AI in enterprise environments. This framework is intended for enterprise CISOs, security, AI platform, and engineering teams and offers a detailed step-by-step approach to securing agentic AI workloads.\nlink: https://arxiv.org/abs/2512.18043v1\n"}}
{"custom_id": "2512.18035v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Towards Benchmarking Privacy Vulnerabilities in Selective Forgetting with Large Language Models\nsummary: The rapid advancements in artificial intelligence (AI) have primarily focused on the process of learning from data to acquire knowledgeable learning systems. As these systems are increasingly deployed in critical areas, ensuring their privacy and alignment with human values is paramount. Recently, selective forgetting (also known as machine unlearning) has shown promise for privacy and data removal tasks, and has emerged as a transformative paradigm shift in the field of AI. It refers to the ability of a model to selectively erase the influence of previously seen data, which is especially important for compliance with modern data protection regulations and for aligning models with human values. Despite its promise, selective forgetting raises significant privacy concerns, especially when the data involved come from sensitive domains. While new unlearning-induced privacy attacks are continuously proposed, each is shown to outperform its predecessors using different experimental settings, which can lead to overly optimistic and potentially unfair assessments that may disproportionately favor one particular attack over the others. In this work, we present the first comprehensive benchmark for evaluating privacy vulnerabilities in selective forgetting. We extensively investigate privacy vulnerabilities of machine unlearning techniques and benchmark privacy leakage across a wide range of victim data, state-of-the-art unlearning privacy attacks, unlearning methods, and model architectures. We systematically evaluate and identify critical factors related to unlearning-induced privacy leakage. With our novel insights, we aim to provide a standardized tool for practitioners seeking to deploy customized unlearning applications with faithful privacy assessments.\nlink: https://arxiv.org/abs/2512.18035v1\n"}}
{"custom_id": "2512.18025v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Scalable Multiterminal Key Agreement via Error-Correcting Codes\nsummary: We explore connections between secret sharing and secret key agreement, which yield a simple and scalable multiterminal key agreement protocol. In our construction, we use error-correcting codes, specifically Reed-Solomon codes with threshold reconstruction, to ensure no information is leaked to an eavesdropper. We then derive novel bounds for both full-rank maximum distance separable codes and our scheme's secret key capacity, using key capacity's duality with multivariate mutual information.\nlink: https://arxiv.org/abs/2512.18025v1\n"}}
