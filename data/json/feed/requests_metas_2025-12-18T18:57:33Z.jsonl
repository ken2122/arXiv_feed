{"custom_id": "2512.16904v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: How Good is Post-Hoc Watermarking With Language Model Rephrasing?\nsummary: Generation-time text watermarking embeds statistical signals into text for traceability of AI-generated content. We explore *post-hoc watermarking* where an LLM rewrites existing text while applying generation-time watermarking, to protect copyrighted documents, or detect their use in training or RAG via watermark radioactivity. Unlike generation-time approaches, which is constrained by how LLMs are served, this setting offers additional degrees of freedom for both generation and detection. We investigate how allocating compute (through larger rephrasing models, beam search, multi-candidate generation, or entropy filtering at detection) affects the quality-detectability trade-off. Our strategies achieve strong detectability and semantic fidelity on open-ended text such as books. Among our findings, the simple Gumbel-max scheme surprisingly outperforms more recent alternatives under nucleus sampling, and most methods benefit significantly from beam search. However, most approaches struggle when watermarking verifiable text such as code, where we counterintuitively find that smaller models outperform larger ones. This study reveals both the potential and limitations of post-hoc watermarking, laying groundwork for practical applications and future research.\nlink: https://arxiv.org/abs/2512.16904v1\n"}}
{"custom_id": "2512.16897v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Checking the HAL Interface Specification Continuously, Right from the Start\nsummary: The correct use of a Hardware Abstraction Layer (HAL) interface in embedded applications is crucial to prevent malfunctions, crashes, or even hardware damage. Software model checking has been successfully applied to check interface specifications in application programs, but its employment in industrial practice is hindered by its unpredictability (whether it succeeds for a given application program or not). In this paper, we present a novel approach to address this problem by checking the HAL interface specification continuously and right from the start of the development. I.e., we develop an embedded application in several iterations without a formal connection between the steps. The steps start from a program skeleton which does nothing but calling HAL functions. Actual functionality is added consecutively. The HAL interface specification is checked in each step of the sequence. The idea of the approach is to exploit a specific feature of software model checking: Its attempt to compute exactly the abstraction that is needed for the check to succeed may carry over from one step to the next, even if there is no formal connection between the steps. The experience from a preliminary experimental evaluation of our approach in the development of embedded applications is very promising. Following our approach, the check succeeds in each step and in particular in the final application program.\nlink: https://arxiv.org/abs/2512.16897v1\n"}}
{"custom_id": "2512.16876v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies\nsummary: The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.\nlink: https://arxiv.org/abs/2512.16876v1\n"}}
{"custom_id": "2512.16874v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Pixel Seal: Adversarial-only training for invisible image and video watermarking\nsummary: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.\nlink: https://arxiv.org/abs/2512.16874v1\n"}}
{"custom_id": "2512.16851v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy\nsummary: The convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43% and 39%, respectively, for cybersickness tasks while preserving model utility with up to 97% accuracy using Transformer models. Furthermore, it improves inference time by up to ~2x compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay.\nlink: https://arxiv.org/abs/2512.16851v1\n"}}
{"custom_id": "2512.10361v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Bit of a Close Talker: A Practical Guide to Serverless Cloud Co-Location Attacks\nsummary: Serverless computing has revolutionized cloud computing by offering users an efficient, cost-effective way to develop and deploy applications without managing infrastructure details. However, serverless cloud users remain vulnerable to various types of attacks, including micro-architectural side-channel attacks. These attacks typically rely on the physical co-location of victim and attacker instances, and attackers need to exploit cloud schedulers to achieve co-location with victims. Therefore, it is crucial to study vulnerabilities in serverless cloud schedulers and assess the security of different serverless scheduling algorithms. This study addresses the gap in understanding and constructing co-location attacks in serverless clouds. We present a comprehensive methodology to uncover exploitable features in serverless scheduling algorithms and to devise strategies for constructing co-location attacks via normal user interfaces. In our experiments, we successfully reveal exploitable vulnerabilities and achieve instance co-location on prevalent open-source infrastructures and Microsoft Azure Functions. We also present a mitigation strategy, the Double-Dip scheduler, to defend against co-location attacks in serverless clouds. Our work highlights critical areas for security enhancements in current cloud schedulers, offering insights to fortify serverless computing environments against potential co-location attacks.\nlink: https://arxiv.org/abs/2512.10361v2\n"}}
{"custom_id": "2411.14516v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Memory Backdoor Attacks on Neural Networks\nsummary: Neural networks are often trained on proprietary datasets, making them attractive attack targets. We present a novel dataset extraction method leveraging an innovative training time backdoor attack, allowing a malicious federated learning server to systematically and deterministically extract complete client training samples through a simple indexing process. Unlike prior techniques, our approach guarantees exact data recovery rather than probabilistic reconstructions or hallucinations, provides precise control over which samples are memorized and how many, and shows high capacity and robustness. Infected models output data samples when they receive a patternbased index trigger, enabling systematic extraction of meaningful patches from each clients local data without disrupting global model utility. To address small model output sizes, we extract patches and then recombined them. The attack requires only a minor modification to the training code that can easily evade detection during client-side verification. Hence, this vulnerability represents a realistic FL supply-chain threat, where a malicious server can distribute modified training code to clients and later recover private data from their updates. Evaluations across classifiers, segmentation models, and large language models demonstrate that thousands of sensitive training samples can be recovered from client models with minimal impact on task performance, and a clients entire dataset can be stolen after multiple FL rounds. For instance, a medical segmentation dataset can be extracted with only a 3 percent utility drop. These findings expose a critical privacy vulnerability in FL systems, emphasizing the need for stronger integrity and transparency in distributed training pipelines.\nlink: https://arxiv.org/abs/2411.14516v2\n"}}
{"custom_id": "2512.16813v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning\nsummary: Reactive jammers pose a severe security threat to robotic-swarm networks by selectively disrupting inter-agent communications and undermining formation integrity and mission success. Conventional countermeasures such as fixed power control or static channel hopping are largely ineffective against such adaptive adversaries. This paper presents a multi-agent reinforcement learning (MARL) framework based on the QMIX algorithm to improve the resilience of swarm communications under reactive jamming. We consider a network of multiple transmitter-receiver pairs sharing channels while a reactive jammer with Markovian threshold dynamics senses aggregate power and reacts accordingly. Each agent jointly selects transmit frequency (channel) and power, and QMIX learns a centralized but factorizable action-value function that enables coordinated yet decentralized execution. We benchmark QMIX against a genie-aided optimal policy in a no-channel-reuse setting, and against local Upper Confidence Bound (UCB) and a stateless reactive policy in a more general fading regime with channel reuse enabled. Simulation results show that QMIX rapidly converges to cooperative policies that nearly match the genie-aided bound, while achieving higher throughput and lower jamming incidence than the baselines, thereby demonstrating MARL's effectiveness for securing autonomous swarms in contested environments.\nlink: https://arxiv.org/abs/2512.16813v1\n"}}
{"custom_id": "2512.16792v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint\nsummary: In this paper, the Multi-stage Edge Server Upgrade (M-ESU) is proposed as a new network planning problem, involving the upgrading of an existing multi-access edge computing (MEC) system through multiple stages (e.g., over several years). More precisely, the problem considers two key decisions: (i) whether to deploy additional edge servers or upgrade those already installed, and (ii) how tasks should be offloaded so that the average number of tasks that meet their delay requirement is maximized. The framework specifically involves: (i) deployment of new servers combined with capacity upgrades for existing servers, and (ii) the optimal task offloading to maximize the average number of tasks with a delay requirement. It also considers the following constraints: (i) budget per stage, (ii) server deployment and upgrade cost (in $) and cost depreciation rate, (iii) computation resource of servers, (iv) number of tasks and their growth rate (in %), and (v) the increase in task sizes and stricter delay requirements over time. We present two solutions: a Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). MILP yields the optimal solution for small networks, whereas M-ESU/H is used in large-scale networks. For small networks, the simulation results show that the solution computed by M-ESU/H is within 1.25% of the optimal solution while running several orders of magnitude faster. For large networks, M-ESU/H is compared against three alternative heuristic solutions that consider only server deployment, or giving priority to server deployment or upgrade. Our experiments show that M-ESU/H yields up to 21.57% improvement in task satisfaction under identical budget and demand growth conditions, confirming its scalability and practical value for long-term MEC systems.\nlink: https://arxiv.org/abs/2512.16792v1\n"}}
{"custom_id": "2512.16778v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Non-Linear Strong Data-Processing for Quantum Hockey-Stick Divergences\nsummary: Data-processing is a desired property of classical and quantum divergences and information measures. In information theory, the contraction coefficient measures how much the distinguishability of quantum states decreases when they are transmitted through a quantum channel, establishing linear strong data-processing inequalities (SDPI). However, these linear SDPI are not always tight and can be improved in most of the cases. In this work, we establish non-linear SDPI for quantum hockey-stick divergence for noisy channels that satisfy a certain noise criterion. We also note that our results improve upon existing linear SDPI for quantum hockey-stick divergences and also non-linear SDPI for classical hockey-stick divergence. We define $F_\u03b3$ curves generalizing Dobrushin curves for the quantum setting while characterizing SDPI for the sequential composition of heterogeneous channels. In addition, we derive reverse-Pinsker type inequalities for $f$-divergences with additional constraints on hockey-stick divergences. We show that these non-linear SDPI can establish tighter finite mixing times that cannot be achieved through linear SDPI. Furthermore, we find applications of these in establishing stronger privacy guarantees for the composition of sequential private quantum channels when privacy is quantified by quantum local differential privacy.\nlink: https://arxiv.org/abs/2512.16778v1\n"}}
{"custom_id": "2512.16719v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Channel State Information Preprocessing for CSI-based Physical-Layer Authentication Using Reconciliation\nsummary: This paper introduces an adaptive preprocessing technique to enhance the accuracy of channel state information-based physical layer authentication (CSI-PLA) alleviating CSI variations and inconsistencies in the time domain. To this end, we develop an adaptive robust principal component analysis (A-RPCA) preprocessing method based on robust principal component analysis (RPCA). The performance evaluation is then conducted using a PLA framework based on information reconciliation, in which Gaussian approximation (GA) for Polar codes is leveraged for the design of short codelength Slepian Wolf decoders. Furthermore, an analysis of the proposed A-RPCA methods is carried out. Simulation results show that compared to a baseline scheme without preprocessing and without reconciliation, the proposed A-RPCA method substantially reduces the error probability after reconciliation and also substantially increases the detection probabilities that is also 1 in both line-of-sight (LOS) and non-line-of-sight (NLOS) scenarios. We have compared against state-of the-art preprocessing schemes in both synthetic and real datasets, including principal component analysis (PCA) and robust PCA, autoencoders and the recursive projected compressive sensing (ReProCS) framework and we have validated the superior performance of the proposed approach.\nlink: https://arxiv.org/abs/2512.16719v1\n"}}
{"custom_id": "2512.16717v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Phishing Detection System: An Ensemble Approach Using Character-Level CNN and Feature Engineering\nsummary: In actuality, phishing attacks remain one of the most prevalent cybersecurity risks in existence today, with malevolent actors constantly changing their strategies to successfully trick users. This paper presents an AI model for a phishing detection system that uses an ensemble approach to combine character-level Convolutional Neural Networks (CNN) and LightGBM with engineered features. Our system uses a character-level CNN to extract sequential features after extracting 36 lexical, structural, and domain-based features from the URLs. On a test dataset of 19,873 URLs, the ensemble model achieves an accuracy of 99.819 percent, precision of 100 percent, recall of 99.635 percent, and ROC-AUC of 99.947 percent. Through a FastAPI-based service with an intuitive user interface, the suggested system has been utilised to offer real-time detection. In contrast, the results demonstrate that the suggested solution performs better than individual models; LightGBM contributes 40 percent and character-CNN contributes 60 percent to the final prediction. The suggested method maintains extremely low false positive rates while doing a good job of identifying contemporary phishing techniques. Index Terms - Phishing detection, machine learning, deep learning, CNN, ensemble methods, cybersecurity, URL analysis\nlink: https://arxiv.org/abs/2512.16717v1\n"}}
{"custom_id": "2512.16683v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Efficient Bitcoin Meta-Protocol Transaction and Data Discovery Through nLockTime Field Repurposing\nsummary: We describe the Lockchain Protocol, a lightweight Bitcoin meta-protocol that enables highly efficient transaction discovery at zero marginal block space cost, and data verification without introducing any new on-chain storage mechanism. The protocol repurposes the mandatory 4-byte nLockTime field of every Bitcoin transaction as a compact metadata header. By constraining values to an unused range of past Unix timestamps greater than or equal to 500,000,000, the field can encode a protocol signal, type, variant, and sequence identifier while remaining fully valid under Bitcoin consensus and policy rules. The primary contribution of the protocol is an efficient discovery layer. Indexers can filter candidate transactions by examining a fixed-size header field, independent of transaction payload size, and only then selectively inspect heavier data such as OP RETURN outputs or witness fields. The Lockchain Protocol applies established protocol design patterns to an under-optimised problem domain, namely transaction discovery at scale, and does not claim new cryptographic primitives or storage methods.\nlink: https://arxiv.org/abs/2512.16683v1\n"}}
{"custom_id": "2511.07503v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Biologically-Informed Hybrid Membership Inference Attacks on Generative Genomic Models\nsummary: The increased availability of genetic data has transformed genomics research, but raised many privacy concerns regarding its handling due to its sensitive nature. This work explores the use of language models (LMs) for the generation of synthetic genetic mutation profiles, leveraging differential privacy (DP) for the protection of sensitive genetic data. We empirically evaluate the privacy guarantees of our DP modes by introducing a novel Biologically-Informed Hybrid Membership Inference Attack (biHMIA), which combines traditional black box MIA with contextual genomics metrics for enhanced attack power. Our experiments show that both small and large transformer GPT-like models are viable synthetic variant generators for small-scale genomics, and that our hybrid attack leads, on average, to higher adversarial success compared to traditional metric-based MIAs.\nlink: https://arxiv.org/abs/2511.07503v3\n"}}
{"custom_id": "2512.16658v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Protecting Deep Neural Network Intellectual Property with Chaos-Based White-Box Watermarking\nsummary: The rapid proliferation of deep neural networks (DNNs) across several domains has led to increasing concerns regarding intellectual property (IP) protection and model misuse. Trained DNNs represent valuable assets, often developed through significant investments. However, the ease with which models can be copied, redistributed, or repurposed highlights the urgent need for effective mechanisms to assert and verify model ownership. In this work, we propose an efficient and resilient white-box watermarking framework that embeds ownership information into the internal parameters of a DNN using chaotic sequences. The watermark is generated using a logistic map, a well-known chaotic function, producing a sequence that is sensitive to its initialization parameters. This sequence is injected into the weights of a chosen intermediate layer without requiring structural modifications to the model or degradation in predictive performance. To validate ownership, we introduce a verification process based on a genetic algorithm that recovers the original chaotic parameters by optimizing the similarity between the extracted and regenerated sequences. The effectiveness of the proposed approach is demonstrated through extensive experiments on image classification tasks using MNIST and CIFAR-10 datasets. The results show that the embedded watermark remains detectable after fine-tuning, with negligible loss in model accuracy. In addition to numerical recovery of the watermark, we perform visual analyses using weight density plots and construct activation-based classifiers to distinguish between original, watermarked, and tampered models. Overall, the proposed method offers a flexible and scalable solution for embedding and verifying model ownership in white-box settings well-suited for real-world scenarios where IP protection is critical.\nlink: https://arxiv.org/abs/2512.16658v1\n"}}
{"custom_id": "2512.16650v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Prefix Probing: Lightweight Harmful Content Detection for Large Language Models\nsummary: Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.\nlink: https://arxiv.org/abs/2512.16650v1\n"}}
{"custom_id": "2402.12018v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Even-Cycle Detection in the Randomized and Quantum CONGEST Model\nsummary: We show that, for every $k\\geq 2$, $C_{2k}$-freeness can be decided in $O(n^{1-1/k})$ rounds in the \\CONGEST{} model by a randomized Monte-Carlo distributed algorithm with one-sided error probability $1/3$. This matches the best round-complexities of previously known algorithms for $k\\in\\{2,3,4,5\\}$ by Drucker et al. [PODC'14] and Censor-Hillel et al. [DISC'20], but improves the complexities of the known algorithms for $k>5$ by Eden et al. [DISC'19], which were essentially of the form $\\tilde O(n^{1-2/k^2})$. Our algorithm uses colored BFS-explorations with threshold, but with an original \\emph{global} approach that enables to overcome a recent impossibility result by Fraigniaud et al. [SIROCCO'23] about using colored BFS-exploration with \\emph{local} threshold for detecting cycles.\n  We also show how to quantize our algorithm for achieving a round-complexity $\\tilde O(n^{\\frac{1}{2}-\\frac{1}{2k}})$ in the quantum setting for deciding $C_{2k}$ freeness. Furthermore, this allows us to improve the known quantum complexities of the simpler problem of detecting cycles of length \\emph{at most}~$2k$ by van Apeldoorn and de Vos [PODC'22]. Our quantization is in two steps. First, the congestion of our randomized algorithm is reduced, to the cost of reducing its success probability too. Second, the success probability is boosted using a new quantum framework derived from sequential algorithms, namely Monte-Carlo quantum amplification.\nlink: https://arxiv.org/abs/2402.12018v2\n"}}
{"custom_id": "2512.14098v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Cornserve: Efficiently Serving Any-to-Any Multimodal Models\nsummary: We present Cornserve, an efficient online serving system for an emerging class of multimodal models called Any-to-Any models. Any-to-Any models accept combinations of text and multimodal data (e.g., image, video, audio) as input and also generate combinations of text and multimodal data as output, introducing request type, computation path, and computation scaling heterogeneity in model serving.\n  Cornserve allows model developers to describe the computation graph of generic Any-to-Any models, which consists of heterogeneous components such as multimodal encoders, autoregressive models like Large Language Models (LLMs), and multimodal generators like Diffusion Transformers (DiTs). Given this, Cornserve's planner automatically finds an optimized deployment plan for the model, including whether and how to disaggregate the model into smaller components based on model and workload characteristics. Cornserve's distributed runtime then executes the model per the plan, efficiently handling Any-to-Any model heterogeneity during online serving. Evaluations show that Cornserve can efficiently serve diverse Any-to-Any models and workloads, delivering up to 3.81$\\times$ throughput improvement and up to 5.79$\\times$ tail latency reduction over existing solutions.\nlink: https://arxiv.org/abs/2512.14098v2\n"}}
{"custom_id": "2511.04399v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Tight Analysis of a One-Shot Quantum Secret Sharing Scheme\nsummary: Quantum communication protocols can be designed to detect eavesdropping attacks, something that classical technologies are unable to do since classical information can be replicated in a non-destructive manner. Eavesdropping detection is, therefore, a standard feature in all the proposed quantum secret sharing (QSS) protocols. However, detection is often done by a statistical analysis of the outcome of multiple decoy rounds, and this causes a significant communication overhead.\n  In our quest for a QSS protocol that works even in one round, we came across a one-shot secret-sharing framework proposed by Hsu (Phys. Rev. A 2003). The scheme was designed to work over public channels without requiring multiple rounds to detect eavesdropping but it lacked a thorough security analysis. In this work we present a complete characterisation of the correctness and security properties of this framework. Our characterisation allowed us to improve the original protocol to be more resistant towards eavesdropping. However, we prove a couple of impossibility results, including one that dictates that complete security against an eavesdropper is not possible in this framework. Thus, it is not possible to design a perfect QSS using this framework.\nlink: https://arxiv.org/abs/2511.04399v2\n"}}
{"custom_id": "2512.16538v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection\nsummary: As large language models (LLMs) are increasingly adopted for code vulnerability detection, their reliability and robustness across diverse vulnerability types have become a pressing concern. In traditional adversarial settings, code obfuscation has long been used as a general strategy to bypass auditing tools, preserving exploitability without tampering with the tools themselves. Numerous efforts have explored obfuscation methods and tools, yet their capabilities differ in terms of supported techniques, granularity, and programming languages, making it difficult to systematically assess their impact on LLM-based vulnerability detection. To address this gap, we provide a structured systematization of obfuscation techniques and evaluate them under a unified framework. Specifically, we categorize existing obfuscation methods into three major classes (layout, data flow, and control flow) covering 11 subcategories and 19 concrete techniques. We implement these techniques across four programming languages (Solidity, C, C++, and Python) using a consistent LLM-driven approach, and evaluate their effects on 15 LLMs spanning four model families (DeepSeek, OpenAI, Qwen, and LLaMA), as well as on two coding agents (GitHub Copilot and Codex). Our findings reveal both positive and negative impacts of code obfuscation on LLM-based vulnerability detection, highlighting conditions under which obfuscation leads to performance improvements or degradations. We further analyze these outcomes with respect to vulnerability characteristics, code properties, and model attributes. Finally, we outline several open problems and propose future directions to enhance the robustness of LLMs for real-world vulnerability detection.\nlink: https://arxiv.org/abs/2512.16538v1\n"}}
{"custom_id": "2512.16514v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Algorithmic Monetary Policies for Blockchain Participation Games\nsummary: A central challenge in blockchain tokenomics is aligning short-term performance incentives with long-term decentralization goals. We propose a framework for algorithmic monetary policies that navigates this tradeoff in repeated participation games. Agents, characterized by type (capability) and stake, choose to participate or abstain at each round; the policy (probabilistically) selects high-type agents for task execution (maximizing throughput) while distributing rewards to sustain decentralization. We analyze equilibria under two agent behaviors: myopic (short-term utility maximization) and foresighted (multi-round planning). For myopic agents, performance-centric policies risk centralization, but foresight enables stable decentralization with some volatility to the token value. We further discuss virtual stake--a hybrid of type and stake--as an alternative approach. We show that the initial virtual stake distribution critically impacts long-term outcomes, suggesting that policies must indirectly manage decentralization.\nlink: https://arxiv.org/abs/2512.16514v1\n"}}
{"custom_id": "2503.02648v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Practical Unclonable Encryption with Continuous Variables\nsummary: We propose the first continuous-variable (CV) unclonable encryption scheme, extending the paradigm of quantum encryption of classical messages (QECM) to CV systems. In our construction, a classical message is first encrypted classically and then encoded using an errorcorrecting code. Each bit of the codeword is mapped to a CV mode by creating a coherent state which is squeezed in the q or p quadrature direction, with a small displacement that encodes the bit. The squeezing directions are part of the encryption key. We prove unclonability in the framework introduced by Broadbent and Lord, via a reduction of the cloning game to a CV monogamy-of-entanglement game. Furthermore, we demonstrate that our scheme can be readily implemented with current technology. By incorporating realistic imperfections such as channel noise and detector inefficiencies, we show that the protocol remains robust under these conditions.\nlink: https://arxiv.org/abs/2503.02648v2\n"}}
{"custom_id": "2512.16473v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems\nsummary: Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference.\nlink: https://arxiv.org/abs/2512.16473v1\n"}}
{"custom_id": "2508.17361v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias\nsummary: Large Language Models (LLMs) are increasingly trusted to perform automated code review and static analysis at scale, supporting tasks such as vulnerability detection, summarization, and refactoring. In this paper, we identify and exploit a critical vulnerability in LLM-based code analysis: an abstraction bias that causes models to overgeneralize familiar programming patterns and overlook small, meaningful bugs. Adversaries can exploit this blind spot to hijack the control flow of the LLM's interpretation with minimal edits and without affecting actual runtime behavior. We refer to this attack as a Familiar Pattern Attack (FPA).\n  We develop a fully automated, black-box algorithm that discovers and injects FPAs into target code. Our evaluation shows that FPAs are not only effective against basic and reasoning models, but are also transferable across model families (OpenAI, Anthropic, Google), and universal across programming languages (Python, C, Rust, Go). Moreover, FPAs remain effective even when models are explicitly warned about the attack via robust system prompts. Finally, we explore positive, defensive uses of FPAs and discuss their broader implications for the reliability and safety of code-oriented LLMs.\nlink: https://arxiv.org/abs/2508.17361v2\n"}}
{"custom_id": "2512.16455v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research\nsummary: In this paper, we describe a federated compute platform dedicated to support Artificial Intelligence in scientific workloads. Putting the effort into reproducible deployments, it delivers consistent, transparent access to a federation of physically distributed e-Infrastructures. Through a comprehensive service catalogue, the platform is able to offer an integrated user experience covering the full Machine Learning lifecycle, including model development (with dedicated interactive development environments), training (with GPU resources, annotation tools, experiment tracking, and federated learning support) and deployment (covering a wide range of deployment options all along the Cloud Continuum). The platform also provides tools for traceability and reproducibility of AI models, integrates with different Artificial Intelligence model providers, datasets and storage resources, allowing users to interact with the broader Machine Learning ecosystem. Finally, it is easily customizable to lower the adoption barrier by external communities.\nlink: https://arxiv.org/abs/2512.16455v1\n"}}
{"custom_id": "2512.16439v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection\nsummary: Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.\nlink: https://arxiv.org/abs/2512.16439v1\n"}}
{"custom_id": "2512.16419v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Large Language Models as a (Bad) Security Norm in the Context of Regulation and Compliance\nsummary: The use of Large Language Models (LLM) by providers of cybersecurity and digital infrastructures of all kinds is an ongoing development. It is suggested and on an experimental basis used to write the code for the systems, and potentially fed with sensitive data or what would otherwise be considered trade secrets. Outside of these obvious points, this paper asks how AI can negatively affect cybersecurity and law when used for the design and deployment of security infrastructure by its developers.\n  Firstly, the paper discusses the use of LLMs in security, either directly or indirectly, and briefly tackles other types of AI. It then lists norms in cybersecurity, then a range of legal cybersecurity obligations from the European Union, to create a frame of reference. Secondly, the paper describes how LLMs may fail to fulfil both legal obligations and best practice in cybersecurity is given, and the paper ends with some economic and practical consequences for this development, with some notions of solutions as well.\n  The paper finds that using LLMs comes with many risks, many of which are against good security practice, and the legal obligations in security regulation. This is because of the inherent weaknesses of LLMs, most of which are mitigated if replaced with symbolic AI. Both also have issues fulfilling basic traceability obligations and practice. Solutions are secondary systems surrounding LLM based AI, fulfilment of security norms beyond legal requirements and simply not using such technology in certain situations.\nlink: https://arxiv.org/abs/2512.16419v1\n"}}
{"custom_id": "2512.16394v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: SoK: Reviewing Two Decades of Security, Privacy, Accessibility, and Usability Studies on Internet of Things for Older Adults\nsummary: The Internet of Things (IoT) has the potential to enhance older adults' independence and quality of life, but it also exposes them to security, privacy, accessibility, and usability (SPAU) risks. We conducted a systematic review of 44 peer-reviewed studies published between 2004 and 2024 using a five-phase screening pipeline. From each study, we extracted data on study design, IoT type, SPAU measures, and identified research gaps. We introduce the SPAU-IoT Framework, which comprises 27 criteria across four dimensions: security (e.g., resilience to cyber threats, secure authentication, encrypted communication, secure-by-default settings, and guardianship features), privacy (e.g., data minimization, explicit consent, and privacy-preserving analytics), accessibility (e.g., compliance with ADA/WCAG standards and assistive-technology compatibility), and usability (e.g., guided interaction, integrated assistance, and progressive learning). Applying this framework revealed that more than 70% of studies implemented authentication and encryption mechanisms, whereas fewer than 50% addressed accessibility or usability concerns. We further developed a threat model that maps IoT assets, networks, and backend servers to exploit vectors such as phishing, caregiver exploitation, and weak-password attacks, explicitly accounting for age-related vulnerabilities including cognitive decline and sensory impairment. Our results expose a systemic lack of integrated SPAU approaches in existing IoT research and translate these gaps into actionable, standards-aligned design guidelines for IoT systems designed for older adults.\nlink: https://arxiv.org/abs/2512.16394v1\n"}}
{"custom_id": "2511.13654v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning\nsummary: In this paper, we present the first detailed analysis of how training hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the training hyperparameter space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.\nlink: https://arxiv.org/abs/2511.13654v2\n"}}
{"custom_id": "2512.16391v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference\nsummary: Attention is the dominant source of latency during long-context LLM inference, an increasingly popular workload with reasoning models and RAG. We propose Kascade, a training-free sparse attention method that leverages known observations such as 1) post-softmax attention is intrinsically sparse, and 2) the identity of high-weight keys is stable across nearby layers. Kascade computes exact Top-k indices in a small set of anchor layers, then reuses those indices in intermediate reuse layers. The anchor layers are selected algorithmically, via a dynamic-programming objective that maximizes cross-layer similarity over a development set, allowing easy deployment across models. The method incorporates efficient implementation constraints (e.g. tile-level operations), across both prefill and decode attention. The Top-k selection and reuse in Kascade is head-aware and we show in our experiments that this is critical for high accuracy. Kascade achieves up to 4.1x speedup in decode attention and 2.2x speedup in prefill attention over FlashAttention-3 baseline on H100 GPUs while closely matching dense attention accuracy on long-context benchmarks such as LongBench and AIME-24.\nlink: https://arxiv.org/abs/2512.16391v1\n"}}
{"custom_id": "2512.16369v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: A first look at common RPKI publication practices\nsummary: The RPKI is crucial for securing the routing system of the Internet. With the RPKI, owners of Internet resources can make cryptographically backed claims, for example about the legitimate origin of their IP space. Thousands of networks use this information to detect malicious or accidental route hijacks. The RPKI consists out of 100 distributed repositories. However, public reports claim that some of these repositories are unreliable. A current Internet-Draft suggests best practices on how to operate these repositories, with the goal to improve deployment quality.\n  Inspired by this draft, we take a first look at the operational practices of repositories of the RPKI. We mainly focus on the distribution of RPKI information. We find that there is a wide variety in deployment practices, of which some might risk the availability of parts of the information in the RPKI. This study creates a baseline for measuring the maturity of RPKI repositories in the future.\nlink: https://arxiv.org/abs/2512.16369v1\n"}}
{"custom_id": "2512.16310v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation\nsummary: Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.\nlink: https://arxiv.org/abs/2512.16310v1\n"}}
{"custom_id": "2512.16307v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks\nsummary: In this fast-evolving area of LLMs, our paper discusses the significant security risk presented by prompt injection attacks. It focuses on small open-sourced models, specifically the LLaMA family of models. We introduce novel defense mechanisms capable of generating automatic defenses and systematically evaluate said generated defenses against a comprehensive set of benchmarked attacks. Thus, we empirically demonstrated the improvement proposed by our approach in mitigating goal-hijacking vulnerabilities in LLMs. Our work recognizes the increasing relevance of small open-sourced LLMs and their potential for broad deployments on edge devices, aligning with future trends in LLM applications. We contribute to the greater ecosystem of open-source LLMs and their security in the following: (1) assessing present prompt-based defenses against the latest attacks, (2) introducing a new framework using a seed defense (Chain Of Thoughts) to refine the defense prompts iteratively, and (3) showing significant improvements in detecting goal hijacking attacks. Out strategies significantly reduce the success rates of the attacks and false detection rates while at the same time effectively detecting goal-hijacking capabilities, paving the way for more secure and efficient deployments of small and open-source LLMs in resource-constrained environments.\nlink: https://arxiv.org/abs/2512.16307v1\n"}}
{"custom_id": "2511.18723v4", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: N2N: A Parallel Framework for Large-Scale MILP under Distributed Memory\nsummary: Parallelization has emerged as a promising approach for accelerating MILP solving. However, the complexity of the branch-and-bound (B&B) framework and the numerous effective algorithm components in MILP solvers make it difficult to parallelize. In this study, a scalable parallel framework, N2N (a node-to-node framework that maps the B&B nodes to distributed computing nodes), was proposed to solve large-scale problems in a distributed memory computing environment. Both deterministic and nondeterministic modes are supported, and the framework is designed to be easily integrated with existing solvers. Regarding the deterministic mode, a novel sliding-window-based algorithm was designed and implemented to ensure that tasks are generated and solved in a deterministic order. Moreover, several advanced techniques, such as the utilization of CP search and general primal heuristics, have been developed to fully utilize distributed computing resources and capabilities of base solvers. Adaptive solving and data communication optimization were also investigated. A popular open-source MILP solver, SCIP, was integrated into N2N as the base solver, yielding N2N-SCIP. Extensive computational experiments were conducted to evaluate the performance of N2N-SCIP compared to ParaSCIP, which is a state-of-the-art distributed parallel MILP solver under the UG framework. The nondeterministic N2N-SCIP achieves speedups of 22.52 and 12.71 with 1,000 MPI processes on the Kunpeng and x86 computing clusters, which is 1.98 and 2.08 times faster than ParaSCIP, respectively. In the deterministic mode, N2N-SCIP also shows significant performance improvements over ParaSCIP across different process numbers and computing clusters. To validate the generality of N2N, HiGHS, another open-source solver, was integrated into N2N. The related results are analyzed, and the requirements of N2N on base solvers are also concluded.\nlink: https://arxiv.org/abs/2511.18723v4\n"}}
{"custom_id": "2512.16292v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: In-Context Probing for Membership Inference in Fine-Tuned Language Models\nsummary: Membership inference attacks (MIAs) pose a critical privacy threat to fine-tuned large language models (LLMs), especially when models are adapted to domain-specific tasks using sensitive data. While prior black-box MIA techniques rely on confidence scores or token likelihoods, these signals are often entangled with a sample's intrinsic properties - such as content difficulty or rarity - leading to poor generalization and low signal-to-noise ratios. In this paper, we propose ICP-MIA, a novel MIA framework grounded in the theory of training dynamics, particularly the phenomenon of diminishing returns during optimization. We introduce the Optimization Gap as a fundamental signal of membership: at convergence, member samples exhibit minimal remaining loss-reduction potential, while non-members retain significant potential for further optimization. To estimate this gap in a black-box setting, we propose In-Context Probing (ICP), a training-free method that simulates fine-tuning-like behavior via strategically constructed input contexts. We propose two probing strategies: reference-data-based (using semantically similar public samples) and self-perturbation (via masking or generation). Experiments on three tasks and multiple LLMs show that ICP-MIA significantly outperforms prior black-box MIAs, particularly at low false positive rates. We further analyze how reference data alignment, model type, PEFT configurations, and training schedules affect attack effectiveness. Our findings establish ICP-MIA as a practical and theoretically grounded framework for auditing privacy risks in deployed LLMs.\nlink: https://arxiv.org/abs/2512.16292v1\n"}}
{"custom_id": "2512.16284v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Empirical Evaluation of Structured Synthetic Data Privacy Metrics: Novel experimental framework\nsummary: Synthetic data generation is gaining traction as a privacy enhancing technology (PET). When properly generated, synthetic data preserve the analytic utility of real data while avoiding the retention of information that would allow the identification of specific individuals. However, the concept of data privacy remains elusive, making it challenging for practitioners to evaluate and benchmark the degree of privacy protection offered by synthetic data. In this paper, we propose a framework to empirically assess the efficacy of tabular synthetic data privacy quantification methods through controlled, deliberate risk insertion. To demonstrate this framework, we survey existing approaches to synthetic data privacy quantification and the related legal theory. We then apply the framework to the main privacy quantification methods with no-box threat models on publicly available datasets.\nlink: https://arxiv.org/abs/2512.16284v1\n"}}
{"custom_id": "2512.16280v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams\nsummary: Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation.\n  We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion.\nlink: https://arxiv.org/abs/2512.16280v1\n"}}
{"custom_id": "2407.20836v5", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Vulnerabilities in AI-generated Image Detection: The Challenge of Adversarial Attacks\nsummary: Recent advancements in image synthesis, particularly with the advent of GAN and Diffusion models, have amplified public concerns regarding the dissemination of disinformation. To address such concerns, numerous AI-generated Image (AIGI) Detectors have been proposed and achieved promising performance in identifying fake images. However, there still lacks a systematic understanding of the adversarial robustness of AIGI detectors. In this paper, we examine the vulnerability of state-of-the-art AIGI detectors against adversarial attack under white-box and black-box settings, which has been rarely investigated so far. To this end, we propose a new method to attack AIGI detectors. First, inspired by the obvious difference between real images and fake images in the frequency domain, we add perturbations under the frequency domain to push the image away from its original frequency distribution. Second, we explore the full posterior distribution of the surrogate model to further narrow this gap between heterogeneous AIGI detectors, e.g., transferring adversarial examples across CNNs and ViTs. This is achieved by introducing a novel post-train Bayesian strategy that turns a single surrogate into a Bayesian one, capable of simulating diverse victim models using one pre-trained surrogate, without the need for re-training. We name our method as Frequency-based Post-train Bayesian Attack, or FPBA. Through FPBA, we demonstrate that adversarial attacks pose a real threat to AIGI detectors. FPBA can deliver successful black-box attacks across various detectors, generators, defense methods, and even evade cross-generator and compressed image detection, which are crucial real-world detection scenarios. Our code is available at https://github.com/onotoa/fpba.\nlink: https://arxiv.org/abs/2407.20836v5\n"}}
{"custom_id": "2512.16182v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: DualGuard: Dual-stream Large Language Model Watermarking Defense against Paraphrase and Spoofing Attack\nsummary: With the rapid development of cloud-based services, large language models (LLMs) have become increasingly accessible through various web platforms. However, this accessibility has also led to growing risks of model abuse. LLM watermarking has emerged as an effective approach to mitigate such misuse and protect intellectual property. Existing watermarking algorithms, however, primarily focus on defending against paraphrase attacks while overlooking piggyback spoofing attacks, which can inject harmful content, compromise watermark reliability, and undermine trust in attribution. To address this limitation, we propose DualGuard, the first watermarking algorithm capable of defending against both paraphrase and spoofing attacks. DualGuard employs the adaptive dual-stream watermarking mechanism, in which two complementary watermark signals are dynamically injected based on the semantic content. This design enables DualGuard not only to detect but also to trace spoofing attacks, thereby ensuring reliable and trustworthy watermark detection. Extensive experiments conducted across multiple datasets and language models demonstrate that DualGuard achieves excellent detectability, robustness, traceability, and text quality, effectively advancing the state of LLM watermarking for real-world applications.\nlink: https://arxiv.org/abs/2512.16182v1\n"}}
{"custom_id": "2512.15144v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: MCPZoo: A Large-Scale Dataset of Runnable Model Context Protocol Servers for AI Agent\nsummary: Model Context Protocol (MCP) enables agents to interact with external tools, yet empirical research on MCP is hindered by the lack of large-scale, accessible datasets. We present MCPZoo, the largest and most comprehensive dataset of MCP servers collected from multiple public sources, comprising 95,142 servers. MCPZoo includes over ten thousand server instances that have been deployed and verified as runnable and interactable, supporting realistic experimentation beyond static analysis. The dataset provides unified metadata and access interfaces, enabling systematic exploration and interaction without manual deployment effort. MCPZoo is released as an open and accessible resource to support research on MCP-based security analysis.\nlink: https://arxiv.org/abs/2512.15144v2\n"}}
{"custom_id": "2512.05543v4", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Are Bus-Mounted Edge Servers Feasible?\nsummary: Placement of edge servers is the prerequisite of provisioning edge computing services for Internet of Vehicles (IoV). Fixed-site edge servers at Road Side Units (RSUs) or base stations are able to offer basic service coverage for end users, i.e., vehicles on road. However, the server locations and capacity are fixed after deployment, rendering their inefficiency in handling spationtemporal user dynamics. Mobile servers such as buses, on the other hand, have the potential of adding computation elasticity to such system. To this end, this paper studies the feasibility of bus-mounted edge servers based on real traces. First, we investigate the coverage of the buses and base stations using the Shanghai bus/taxi/Telecom datasets, which shows a great potential of bus-based edge servers as they cover a great portion of geographic area and demand points. Next, we build a mathematical model and design a simple greedy heuristic algorithm to select a limited number of buses that maximizes the coverage of demand points, i.e., with a limited purchase budget. We perform trace-driven simulations to verify the performance of the proposed bus selection algorithm. The results show that our approach effectively handles the dynamic user demand under realistic constraints such as server capacity and purchase quantity. Thus, we claim: bus-mounted edge servers for vehicular networks in urban areas are feasible, beneficial, and valuable.\nlink: https://arxiv.org/abs/2512.05543v4\n"}}
{"custom_id": "2512.16148v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: FlexKV: Flexible Index Offloading for Memory-Disaggregated Key-Value Store\nsummary: Disaggregated memory (DM) is a promising data center architecture that decouples CPU and memory into independent resource pools to improve resource utilization. Building on DM, memory-disaggregated key-value (KV) stores are adopted to efficiently manage remote data. Unfortunately, existing approaches suffer from poor performance due to two critical issues: 1) the overdependence on one-sided atomic operations in index processing, and 2) the constrained efficiency in compute-side caches. To address these issues, we propose FlexKV, a memory-disaggregated KV store with index proxying. Our key idea is to dynamically offload the index to compute nodes, leveraging their powerful CPUs to accelerate index processing and maintain high-performance compute-side caches. Three challenges have to be addressed to enable efficient index proxying on DM, i.e., the load imbalance across compute nodes, the limited memory of compute nodes, and the expensive cache coherence overhead. FlexKV proposes: 1) a rank-aware hotness detection algorithm to continuously balance index load across compute nodes, 2) a two-level CN memory optimization scheme to efficiently utilize compute node memory, and 3) an RPC-aggregated cache management mechanism to reduce cache coherence overhead. The experimental results show that FlexKV improves throughput by up to 2.94$\\times$ and reduces latency by up to 85.2%, compared with the state-of-the-art memory-disaggregated KV stores.\nlink: https://arxiv.org/abs/2512.16148v1\n"}}
{"custom_id": "2512.16136v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Lotus: Optimizing Disaggregated Transactions with Disaggregated Locks\nsummary: Disaggregated memory (DM) separates compute and memory resources, allowing flexible scaling to achieve high resource utilization. To ensure atomic and consistent data access on DM, distributed transaction systems have been adapted, where compute nodes (CNs) rely on one-sided RDMA operations to access remote data in memory nodes (MNs). However, we observe that in existing transaction systems, the RDMA network interface cards at MNs become a primary performance bottleneck. This bottleneck arises from the high volume of one-sided atomic operations used for locks, which hinders the system's ability to scale efficiently.\n  To address this issue, this paper presents Lotus, a scalable distributed transaction system with lock disaggregation on DM. The key innovation of Lotus is to disaggregate locks from data and execute all locks on CNs, thus eliminating the bottleneck at MN RNICs. To achieve efficient lock management on CNs, Lotus employs an application-aware lock management mechanism that leverages the locality of the OLTP workloads to shard locks while maintaining load balance. To ensure consistent transaction processing with lock disaggregation, Lotus introduces a lock-first transaction protocol, which separates the locking phase as the first step in each read-write transaction execution. This protocol allows the system to determine the success of lock acquisitions early and proactively abort conflicting transactions, improving overall efficiency. To tolerate lock loss during CN failures, Lotus employs a lock-rebuild-free recovery mechanism that treats locks as ephemeral and avoids their reconstruction, ensuring lightweight recovery for CN failures. Experimental results demonstrate that Lotus improves transaction throughput by up to 2.1$\\times$ and reduces latency by up to 49.4% compared to state-of-the-art transaction systems on DM.\nlink: https://arxiv.org/abs/2512.16136v1\n"}}
{"custom_id": "2512.16134v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference\nsummary: The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.\nlink: https://arxiv.org/abs/2512.16134v1\n"}}
{"custom_id": "2512.16123v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection\nsummary: Deep learning-based object detection models play a critical role in real-world applications such as autonomous driving and security surveillance systems, yet they remain vulnerable to adversarial examples. In this work, we propose an autoencoder-based denoising defense to recover object detection performance degraded by adversarial perturbations. We conduct adversarial attacks using Perlin noise on vehicle-related images from the COCO dataset, apply a single-layer convolutional autoencoder to remove the perturbations, and evaluate detection performance using YOLOv5. Our experiments demonstrate that adversarial attacks reduce bbox mAP from 0.2890 to 0.1640, representing a 43.3% performance degradation. After applying the proposed autoencoder defense, bbox mAP improves to 0.1700 (3.7% recovery) and bbox mAP@50 increases from 0.2780 to 0.3080 (10.8% improvement). These results indicate that autoencoder-based denoising can provide partial defense against adversarial attacks without requiring model retraining.\nlink: https://arxiv.org/abs/2512.16123v1\n"}}
{"custom_id": "2512.16099v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: An Online Fragmentation-Aware Scheduler for Managing GPU-Sharing Workloads on Multi-Instance GPUs\nsummary: Modern GPU workloads increasingly demand efficient resource sharing, as many jobs do not require the full capacity of a GPU. Among sharing techniques, NVIDIA's Multi-Instance GPU (MIG) offers strong resource isolation by enabling hardware-level GPU partitioning. However, leveraging MIG effectively introduces new challenges. First, resource contention persists due to shared components such as PCIe bandwidth. Second, GPU fragmentation becomes a critical issue, which is different from prior fine-grained GPU sharing work due to MIG's limited number of valid MIG configurations. Fragmentation arises not only from spatial discontinuity but also from rigid profile placement constraints, especially after job arrivals and terminations. To address these issues, we propose an online scheduling framework that integrates conditional load balancing, dynamic partitioning, and job migration. Our approach dynamically adapts job placement to minimize contention and reorganizes GPU allocations to combat both internal and external fragmentation. Experimental results show that our method significantly improves system efficiency. When all techniques are applied, the makespan improves by up to 35%.\nlink: https://arxiv.org/abs/2512.16099v1\n"}}
{"custom_id": "2512.16080v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Design of a Decentralized Fixed-Income Lending Automated Market Maker Protocol Supporting Arbitrary Maturities\nsummary: In decentralized finance (DeFi), designing fixed-income lending automated market makers (AMMs) is extremely challenging due to time-related complexities. Moreover, existing protocols only support single-maturity lending. Building upon the BondMM protocol, this paper argues that its mathematical invariants are sufficiently elegant to be generalized to arbitrary maturities. This paper thus propose an improved design, BondMM-A, which supports lending activities of any maturity. By integrating fixed-income instruments of varying maturities into a single smart contract, BondMM-A offers users and liquidity providers (LPs) greater operational freedom and capital efficiency. Experimental results show that BondMM-A performs excellently in terms of interest rate stability and financial robustness.\nlink: https://arxiv.org/abs/2512.16080v1\n"}}
{"custom_id": "2512.16066v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Cold-Start Anti-Patterns and Refactorings in Serverless Systems: An Empirical Study\nsummary: Serverless computing simplifies deployment and scaling, yet cold-start latency remains a major performance bottleneck. Unlike prior work that treats mitigation as a black-box optimization, we study cold starts as a developer-visible design problem. From 81 adjudicated issue reports across open-source serverless systems, we derive taxonomies of initialization anti-patterns, remediation strategies, and diagnostic challenges spanning design, packaging, and runtime layers. Building on these insights, we introduce SCABENCH, a reproducible benchmark, and INITSCOPE, a lightweight analysis framework linking what code is loaded with what is executed. On SCABENCH, INITSCOPE improved localization accuracy by up to 40% and reduced diagnostic effort by 64% compared with prior tools, while a developer study showed higher task accuracy and faster diagnosis. Together, these results advance evidence-driven, performance-aware practices for cold-start mitigation in serverless design. Availability: The research artifact is publicly accessible for future studies and improvements.\nlink: https://arxiv.org/abs/2512.16066v1\n"}}
{"custom_id": "2512.16059v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: ContextLeak: Auditing Leakage in Private In-Context Learning Methods\nsummary: In-Context Learning (ICL) has become a standard technique for adapting Large Language Models (LLMs) to specialized tasks by supplying task-specific exemplars within the prompt. However, when these exemplars contain sensitive information, reliable privacy-preserving mechanisms are essential to prevent unintended leakage through model outputs. Many privacy-preserving methods are proposed to protect the information leakage in the context, but there are less efforts on how to audit those methods. We introduce ContextLeak, the first framework to empirically measure the worst-case information leakage in ICL. ContextLeak uses canary insertion, embedding uniquely identifiable tokens in exemplars and crafting targeted queries to detect their presence. We apply ContextLeak across a range of private ICL techniques, both heuristic such as prompt-based defenses and those with theoretical guarantees such as Embedding Space Aggregation and Report Noisy Max. We find that ContextLeak tightly correlates with the theoretical privacy budget ($\u03b5$) and reliably detects leakage. Our results further reveal that existing methods often strike poor privacy-utility trade-offs, either leaking sensitive information or severely degrading performance.\nlink: https://arxiv.org/abs/2512.16059v1\n"}}
{"custom_id": "2512.16058v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Twinning for Space-Air-Ground-Sea Integrated Networks: Beyond Conventional Digital Twin Towards Goal-Oriented Semantic Twin\nsummary: A space-air-ground-sea integrated network (SAGSIN) has emerged as a cornerstone of 6G systems, establishing a unified global architecture by integrating multi-domain network resources. Motivated by the demand for real-time situational awareness and intelligent operational maintenance, digital twin (DT) technology was initially regarded as a promising solution, owing to its capability to create virtual replicas and emulate physical system behaviors. However, in the context of SAGSIN, the high-fidelity, full-scale modeling paradigm inherent to conventional DTs encounters fundamental limitations, including prohibitive computational overhead, delayed model synchronization, and cross-system semantic gaps. To address these limitations, this survey paper proposes a novel twinning framework: goal-oriented semantic twin (GOST). Unlike DTs that pursue physical mirroring, GOST prioritizes ``utility'' over ``fidelity,'' leveraging semantic technologies and goal-oriented principles to construct lightweight, task-specific representations. This paper systematically articulates the GOST framework through three layers: knowledge-based semantics, data-driven semantics, and goal-oriented principles. Furthermore, we provide a comprehensive tutorial on constructing GOST by detailing its core enabling technologies and introduce a multidimensional evaluation framework for GOST. We present a case study targeting collaborative tracking tasks in remote satellite-UAV networks, demonstrating that GOST significantly outperforms conventional DTs in timeliness of perceptual data and collaborative tracking. Finally, we outline research directions, establishing GOST as a transformative twinning paradigm to guide the development of SAGSIN.\nlink: https://arxiv.org/abs/2512.16058v1\n"}}
{"custom_id": "2512.16056v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services\nsummary: The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.\nlink: https://arxiv.org/abs/2512.16056v1\n"}}
{"custom_id": "2512.16038v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: LOG.io: Unified Rollback Recovery and Data Lineage Capture for Distributed Data Pipelines\nsummary: This paper introduces LOG.io, a comprehensive solution designed for correct rollback recovery and fine-grain data lineage capture in distributed data pipelines. It is tailored for serverless scalable architectures and uses a log-based rollback recovery protocol. LOG.io supports a general programming model, accommodating non-deterministic operators, interactions with external systems, and arbitrary custom code. It is non-blocking, allowing failed operators to recover independently without interrupting other active operators, thereby leveraging data parallelization, and it facilitates dynamic scaling of operators during pipeline execution. Performance evaluations, conducted within the SAP Data Intelligence system, compare LOG.io with the Asynchronous Barrier Snapshotting (ABS) protocol, originally implemented in Flink. Our experiments show that when there are straggler operators in a data pipeline and the throughput of events is moderate (e.g., 1 event every 100 ms), LOG.io performs as well as ABS during normal processing and outperforms ABS during recovery. Otherwise, ABS performs better than LOG.io for both normal processing and recovery. However, we show that in these cases, data parallelization can largely reduce the overhead of LOG.io while ABS does not improve. Finally, we show that the overhead of data lineage capture, at the granularity of the event and between any two operators in a pipeline, is marginal, with less than 1.5% in all our experiments.\nlink: https://arxiv.org/abs/2512.16038v1\n"}}
{"custom_id": "2512.15990v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Random coding for long-range continuous-variable QKD\nsummary: Quantum Key Distribution (QKD) schemes are key exchange protocols based on the physical properties of quantum channels. They avoid the computational-hardness assumptions that underlie the security of classical key exchange. Continuous-Variable QKD (CVQKD), in contrast to qubit-based discrete-variable (DV) schemes, makes use of quadrature measurements of the electromagnetic field. CVQKD has the advantage of being compatible with standard telecom equipment, but at long distances has to deal with very low signal to noise ratios, which necessitates labour-intensive error correction. It is challenging to implement the error correction decoding in realtime.\n  In this paper we introduce a random-codebook error correction method that is suitable for long range Gaussian-modulated CVQKD. We use likelihood ratio scoring with block rejection based on thresholding. For proof-technical reasons, the accept/reject decisions are communicated in encrypted form; in this way we avoid having to deal with non-Gaussian states in the analysis of the leakage. The error correction method is highly parallelisable, which is advantageous for realtime implementation. Under conservative assumptions on the computational resources, we predict a realtime key ratio of at least 8% of the Devetak-Winter value, which outperforms existing reconciliation schemes.\nlink: https://arxiv.org/abs/2512.15990v1\n"}}
{"custom_id": "2504.16112v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing\nsummary: The attention layer, a core component of Transformer-based LLMs, brings out inefficiencies in current GPU systems due to its low operational intensity and the substantial memory requirements of KV caches. We propose a High-bandwidth Processing Unit (HPU), a memoryintensive co-processor that enhances GPU resource utilization during large-batched LLM inference. By offloading memory-bound operations, the HPU allows the GPU to focus on compute-intensive tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales out to accommodate surging memory demands driven by large batch sizes and extended sequence lengths. In this paper, we show the HPU prototype implemented with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy efficiency improvements over a GPUonly system, providing scalability without increasing the number of GPUs.\nlink: https://arxiv.org/abs/2504.16112v2\n"}}
{"custom_id": "2512.15966v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Security Aspects of ISO 15118 Plug and Charge Payment\nsummary: For the rise of electric vehicles, especially for long-distance driving, minimizing charging times is vital. While multiple standards for DC fast charging exist, the leading standard in Europe is ISO 15118. In theory, this standard is accompanied by a variety of security controls, ensuring the authenticity and confidentiality of charging communication, as well as the exchange of payment information. In practice, these security controls are insufficient for effectively securing charging communication. In this paper, we go through all security controls defined in ISO 15118 and demonstrate their shortcomings. Most notably, we present a previously unpublished vulnerability in the plug and charge functionality of ISO 15118. We provide a proof-of-concept implementation of this vulnerability, which, allows a vehicle to be charged while a second, victim vehicle is billed for it. Additionally, we define an alternative plug and charge authentication scheme, which requires fewer efforts towards certificate enrollment and promises to be more resilient and future-proof. Our findings should be considered when implementing and advancing the standard, as the mitigation of the discovered vulnerability is critical for the security of fast charging.\nlink: https://arxiv.org/abs/2512.15966v1\n"}}
{"custom_id": "2512.15919v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Analysing Multidisciplinary Approaches to Fight Large-Scale Digital Influence Operations\nsummary: Crime as a Service (CaaS) has evolved from isolated criminal incidents to a broad spectrum of illicit activities, including social media manipulation, foreign information manipulation and interference (FIMI), and the sale of disinformation toolkits. This article analyses how threat actors exploit specialised infrastructures ranging from proxy and VPN services to AI-driven generative models to orchestrate large-scale opinion manipulation. Moreover, it discusses how these malicious operations monetise the virality of social networks, weaponise dual-use technologies, and leverage user biases to amplify polarising narratives. In parallel, it examines key strategies for detecting, attributing, and mitigating such campaigns by highlighting the roles of blockchain- based content verification, advanced cryptographic proofs, and cross-disciplinary collaboration. Finally, the article highlights that countering disinformation demands an integrated framework that combines legal, tech- nological, and societal efforts to address a rapidly adapting and borderless threat\nlink: https://arxiv.org/abs/2512.15919v1\n"}}
{"custom_id": "2512.15915v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Private Virtual Tree Networks for Secure Multi-Tenant Environments Based on the VIRGO Overlay Network\nsummary: Hierarchical organization is a fundamental structure in real-world society, where authority and responsibility are delegated from managers to subordinates. The VIRGO network (Virtual Hierarchical Overlay Network for scalable grid computing) provides a scalable overlay for organizing distributed systems but lacks intrinsic security and privacy mechanisms. This paper proposes Private Virtual Tree Networks (PVTNs), a cryptographically enforced extension that leverages the VIRGO overlay to mirror real organizational hierarchies. In PVTNs, join requests are encrypted with the manager's public key to ensure confidentiality, while membership authorization is enforced through manager-signed delegation certificates. Public keys are treated as organizational secrets and are disclosed only within direct manager-member relationships, resulting in a private, non-enumerable virtual tree. Our work demonstrates, through the system model, protocols, security analysis, and design rationale, that PVTNs achieve scalability, dynamic management, and strong security guarantees without relying on global public key infrastructures.\nlink: https://arxiv.org/abs/2512.15915v1\n"}}
{"custom_id": "2512.15892v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: VET Your Agent: Towards Host-Independent Autonomy via Verifiable Execution Traces\nsummary: Recent advances in large language models (LLMs) have enabled a new generation of autonomous agents that operate over sustained periods and manage sensitive resources on behalf of users. Trusted for their ability to act without direct oversight, such agents are increasingly considered in high-stakes domains including financial management, dispute resolution, and governance. Yet in practice, agents execute on infrastructure controlled by a host, who can tamper with models, inputs, or outputs, undermining any meaningful notion of autonomy.\n  We address this gap by introducing VET (Verifiable Execution Traces), a formal framework that achieves host-independent authentication of agent outputs and takes a step toward host-independent autonomy. Central to VET is the Agent Identity Document (AID), which specifies an agent's configuration together with the proof systems required for verification. VET is compositional: it supports multiple proof mechanisms, including trusted hardware, succinct cryptographic proofs, and notarized TLS transcripts (Web Proofs).\n  We implement VET for an API-based LLM agent and evaluate our instantiation on realistic workloads. We find that for today's black-box, secret-bearing API calls, Web Proofs appear to be the most practical choice, with overhead typically under 3$\\times$ compared to direct API calls, while for public API calls, a lower-overhead TEE Proxy is often sufficient. As a case study, we deploy a verifiable trading agent that produces proofs for each decision and composes Web Proofs with a TEE Proxy. Our results demonstrate that practical, host-agnostic authentication is already possible with current technology, laying the foundation for future systems that achieve full host-independent autonomy.\nlink: https://arxiv.org/abs/2512.15892v1\n"}}
{"custom_id": "2306.13273v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: A First Order Meta Stackelberg Method for Robust Federated Learning (Technical Report)\nsummary: Recent research efforts indicate that federated learning (FL) systems are vulnerable to a variety of security breaches. While numerous defense strategies have been suggested, they are mainly designed to counter specific attack patterns and lack adaptability, rendering them less effective when facing uncertain or adaptive threats. This work models adversarial FL as a Bayesian Stackelberg Markov game (BSMG) between the defender and the attacker to address the lack of adaptability to uncertain adaptive attacks. We further devise an effective meta-learning technique to solve for the Stackelberg equilibrium, leading to a resilient and adaptable defense. The experiment results suggest that our meta-Stackelberg learning approach excels in combating intense model poisoning and backdoor attacks of indeterminate types.\nlink: https://arxiv.org/abs/2306.13273v3\n"}}
