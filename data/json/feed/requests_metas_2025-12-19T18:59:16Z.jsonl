{"custom_id": "2512.17902v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Adversarial Robustness of Vision in Open Foundation Models\nsummary: With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta's Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.\nlink: https://arxiv.org/abs/2512.17902v1\n"}}
{"custom_id": "2512.17885v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Asymptotic behaviour of galactic small-scale dynamos at modest magnetic Prandtl number\nsummary: Magnetic fields are critical at many scales to galactic dynamics and structure, including multiphase pressure balance, dust processing, and star formation. Dynamo action determines their dynamical structure and strength. Simulations of combined large- and small-scale dynamos have successfully developed mean fields with strength and topology consistent with observations but with turbulent fields much weaker than observed, while simulations of small-scale dynamos with parameters relevant to the interstellar medium yield turbulent fields an order of magnitude below the values observed or expected theoretically. We use the Pencil Code accelerated on GPUs with Astaroth to perform high-resolution simulations of a supernova-driven galactic dynamo including heating and cooling in a periodic domain. Our models show that the strength of the turbulent field produced by the small-scale dynamo approaches an asymptote at only modest magnetic Prandtl numbers. This allows us to use these models to suggest the essential characteristics of this constituent of the magnetic field for inclusion in global galactic models. The asymptotic limit occurs already at magnetic Prandtl number of only a few hundred, many orders of magnitude below physical values in the the interstellar medium and consistent with previous findings for isothermal compressible flows.\nlink: https://arxiv.org/abs/2512.17885v1\n"}}
{"custom_id": "2509.11512v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Machine Learning-Driven Predictive Resource Management in Complex Science Workflows\nsummary: The collaborative efforts of large communities in science experiments, often comprising thousands of global members, reflect a monumental commitment to exploration and discovery. Recently, advanced and complex data processing has gained increasing importance in science experiments. Data processing workflows typically consist of multiple intricate steps, and the precise specification of resource requirements is crucial for each step to allocate optimal resources for effective processing. Estimating resource requirements in advance is challenging due to a wide range of analysis scenarios, varying skill levels among community members, and the continuously increasing spectrum of computing options. One practical approach to mitigate these challenges involves initially processing a subset of each step to measure precise resource utilization from actual processing profiles before completing the entire step. While this two-staged approach enables processing on optimal resources for most of the workflow, it has drawbacks such as initial inaccuracies leading to potential failures and suboptimal resource usage, along with overhead from waiting for initial processing completion, which is critical for fast-turnaround analyses. In this context, our study introduces a novel pipeline of machine learning models within a comprehensive workflow management system, the Production and Distributed Analysis (PanDA) system. These models employ advanced machine learning techniques to predict key resource requirements, overcoming challenges posed by limited upfront knowledge of characteristics at each step. Accurate forecasts of resource requirements enable informed and proactive decision-making in workflow management, enhancing the efficiency of handling diverse, complex workflows across heterogeneous resources.\nlink: https://arxiv.org/abs/2509.11512v2\n"}}
{"custom_id": "2503.10063v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Provably Secure Covert Messaging Using Image-based Diffusion Processes\nsummary: We consider the problem of securely and robustly embedding covert messages into an image-based diffusion model's output. The sender and receiver want to exchange the maximum amount of information possible per diffusion sampled image while remaining undetected. The adversary wants to detect that such communication is taking place by identifying those diffusion samples that contain covert messages. To maximize robustness to transformations of the diffusion sample, a strategy is for the sender and the receiver to embed the message in the initial latents. We first show that prior work that attempted this is easily broken because their embedding technique alters the latents' distribution. We then propose a straightforward method to embed covert messages in the initial latent {\\em without} altering the distribution. We prove that our construction achieves indistinguishability to any probabilistic polynomial time adversary. Finally, we discuss and analyze empirically the tradeoffs between embedding capacity, message recovery rates, and robustness. We find that optimizing the inversion method for error correction is crucial for reliability.\nlink: https://arxiv.org/abs/2503.10063v2\n"}}
{"custom_id": "2505.00554v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Notes on Univariate Sumcheck\nsummary: This note describes a univariate polynomial interactive oracle proof for multilinear extension evaluation. Unlike prior protocols, (1) the verifier here is given a univariate extension oracle for the same vector of which the multilinear extension is getting evaluated and (2) the prover only has linear complexity. For these reasons, the protocol is well-suited for combining multivariate and univariate sumcheck techniques.\nlink: https://arxiv.org/abs/2505.00554v3\n"}}
{"custom_id": "2512.17748v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Methods and Tools for Secure Quantum Clouds with a specific Case Study on Homomorphic Encryption\nsummary: The rise of quantum computing/technology potentially introduces significant security challenges to cloud computing, necessitating quantum-resistant encryption strategies as well as protection schemes and methods for cloud infrastructures offering quantum computing time and services (i.e. quantum clouds). This research explores various options for securing quantum clouds and ensuring privacy, especially focussing on the integration of homomorphic encryption (HE) into Eclipse Qrisp, a high-level quantum computing framework, to enhance the security of quantum cloud platforms. The study addresses the technical feasibility of integrating HE with Qrisp, evaluates performance trade-offs, and assesses the potential impact on future quantum cloud architectures.The successful implementation and Qrisp integration of three post-quantum cryptographic (PQC) algorithms demonstrates the feasibility of integrating HE with quantum computing frameworks. The findings indicate that while the Quantum One-Time Pad (QOTP) offers simplicity and low overhead, other algorithms like Chen and Gentry-Sahai-Waters (GSW) present performance trade-offs in terms of runtime and memory consumption. The study results in an overall set of recommendations for securing quantum clouds, e.g. implementing HE at data storage and processing levels, developing Quantum Key Distribution (QKD), and enforcing stringent access control and authentication mechanisms as well as participating in PQC standardization efforts.\nlink: https://arxiv.org/abs/2512.17748v1\n"}}
{"custom_id": "2410.21041v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Clean Up the Mess: Addressing Data Pollution in Cryptocurrency Abuse Reporting Services\nsummary: Cryptocurrency abuse reporting services are a valuable data source about abusive blockchain addresses, prevalent types of cryptocurrency abuse, and their financial impact on victims. However, they may suffer data pollution due to their crowd-sourced nature. This work analyzes the extent and impact of data pollution in cryptocurrency abuse reporting services and proposes a novel LLM-based defense to address the pollution. We collect 289K abuse reports submitted over 6 years to two popular services and use them to answer three research questions. RQ1 analyzes the extent and impact of pollution. We show that spam reports will eventually flood unchecked abuse reporting services, with BitcoinAbuse receiving 75% of spam before stopping operations. We build a public dataset of 19,443 abuse reports labeled with 19 popular abuse types and use it to reveal the inaccuracy of user-reported abuse types. We identified 91 (0.1%) benign addresses reported, responsible for 60% of all the received funds. RQ2 examines whether we can automate identifying valid reports and their classification into abuse types. We propose an unsupervised LLM-based classifier that achieves an F1 score of 0.95 when classifying reports, an F1 of 0.89 when classifying out-of-distribution data, and an F1 of 0.99 when identifying spam reports. Our unsupervised LLM-based classifier clearly outperforms two baselines: a supervised classifier and a naive usage of the LLM. Finally, RQ3 demonstrates the usefulness of our LLM-based classifier for quantifying the financial impact of different cryptocurrency abuse types. We show that victim-reported losses heavily underestimate cybercriminal revenue by estimating a 29 times higher revenue from deposit transactions. We identified that investment scams have the highest financial impact and that extortions have lower conversion rates but compensate for them with massive email campaigns.\nlink: https://arxiv.org/abs/2410.21041v2\n"}}
{"custom_id": "2512.17722v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Digital and Web Forensics Model Cards, V1\nsummary: This paper introduces a standardized model card framework specifically designed for digital and web forensics. Building upon established model card methodologies and recent work on abstract models for digital forensic analysis, this paper presents a web based framework that generates model cards specifically designed to represent knowledge in the forensic domain. The framework includes controlled vocabularies for classification, reasoning types, bias identification, and error categorization, along with a web-based generator tool to facilitate adoption. The paper describes the model card structure, presents the controlled vocabularies, and introduces the beta version of the generator tool, inviting community feedback to refine this emerging standard. Ultimately, the systemic risk is that that the anti fraud and digital and web forensics processes are controlled by the mobs.\nlink: https://arxiv.org/abs/2512.17722v1\n"}}
{"custom_id": "2512.17710v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: A Practical Solution to Systematically Monitor Inconsistencies in SBOM-based Vulnerability Scanners\nsummary: Software Bill of Materials (SBOM) provides new opportunities for automated vulnerability identification in software products. While the industry is adopting SBOM-based Vulnerability Scanning (SVS) to identify vulnerabilities, we increasingly observe inconsistencies and unexpected behavior, that result in false negatives and silent failures. In this work, we present the background necessary to understand the underlying complexity of SVS and introduce SVS-TEST, a method and tool to analyze the capability, maturity, and failure conditions of SVS-tools in real-world scenarios. We showcase the utility of SVS-TEST in a case study evaluating seven real-world SVS-tools using 16 precisely crafted SBOMs and their respective ground truth. Our results unveil significant differences in the reliability and error handling of SVS-tools; multiple SVS-tools silently fail on valid input SBOMs, creating a false sense of security. We conclude our work by highlighting implications for researchers and practitioners, including how organizations and developers of SVS-tools can utilize SVS-TEST to monitor SVS capability and maturity. All results and research artifacts are made publicly available and all findings were disclosed to the SVS-tool developers ahead of time.\nlink: https://arxiv.org/abs/2512.17710v1\n"}}
{"custom_id": "2409.03735v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Privacy Bias in Language Models: A Contextual Integrity-based Auditing Metric\nsummary: As large language models (LLMs) are integrated into sociotechnical systems, it is crucial to examine the privacy biases they exhibit. We define privacy bias as the appropriateness value of information flows in responses from LLMs. A deviation between privacy biases and expected values, referred to as privacy bias delta, may indicate privacy violations. As an auditing metric, privacy bias can help (a) model trainers evaluate the ethical and societal impact of LLMs, (b) service providers select context-appropriate LLMs, and (c) policymakers assess the appropriateness of privacy biases in deployed LLMs. We formulate and answer a novel research question: how can we reliably examine privacy biases in LLMs and the factors that influence them? We present a novel approach for assessing privacy biases using a contextual integrity-based methodology to evaluate the responses from various LLMs. Our approach accounts for the sensitivity of responses across prompt variations, which hinders the evaluation of privacy biases. Finally, we investigate how privacy biases are affected by model capacities and optimizations.\nlink: https://arxiv.org/abs/2409.03735v3\n"}}
{"custom_id": "2509.24444v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: BugMagnifier: TON Transaction Simulator for Revealing Smart Contract Vulnerabilities\nsummary: The Open Network (TON) blockchain employs an asynchronous execution model that introduces unique security challenges for smart contracts, particularly race conditions arising from unpredictable message processing order. While previous work established vulnerability patterns through static analysis of audit reports, dynamic detection of temporal dependencies through systematic testing remains an open problem. We present BugMagnifier, a transaction simulation framework that systematically reveals vulnerabilities in TON smart contracts through controlled message orchestration. Built atop TON Sandbox and integrated with the TON Virtual Machine (TVM), our tool combines precise message queue manipulation with differential state analysis and probabilistic permutation testing to detect asynchronous execution flaws. Experimental evaluation demonstrates BugMagnifier's effectiveness through extensive parametric studies on purpose-built vulnerable contracts, revealing message ratio-dependent detection complexity that aligns with theoretical predictions. This quantitative model enables predictive vulnerability assessment while shifting discovery from manual expert analysis to automated evidence generation. By providing reproducible test scenarios for temporal vulnerabilities, BugMagnifier addresses a critical gap in the TON security tooling, offering practical support for safer smart contract development in asynchronous blockchain environments.\nlink: https://arxiv.org/abs/2509.24444v2\n"}}
{"custom_id": "2512.12068v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: TreeVQA: A Tree-Structured Execution Framework for Shot Reduction in Variational Quantum Algorithms\nsummary: Variational Quantum Algorithms (VQAs) are promising for near- and intermediate-term quantum computing, but their execution cost is substantial. Each task requires many iterations and numerous circuits per iteration, and real-world applications often involve multiple tasks, scaling with the precision needed to explore the application's energy landscape. This demands an enormous number of execution shots, making practical use prohibitively expensive. We observe that VQA costs can be significantly reduced by exploiting execution similarities across an application's tasks. Based on this insight, we propose TreeVQA, a tree-based execution framework that begins by executing tasks jointly and progressively branches only as their quantum executions diverge. Implemented as a VQA wrapper, TreeVQA integrates with typical VQA applications. Evaluations on scientific and combinatorial benchmarks show shot count reductions of $25.9\\times$ on average and over $100\\times$ for large-scale problems at the same target accuracy. The benefits grow further with increasing problem size and precision requirements.\nlink: https://arxiv.org/abs/2512.12068v2\n"}}
{"custom_id": "2512.17667v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: STAR: Semantic-Traffic Alignment and Retrieval for Zero-Shot HTTPS Website Fingerprinting\nsummary: Modern HTTPS mechanisms such as Encrypted Client Hello (ECH) and encrypted DNS improve privacy but remain vulnerable to website fingerprinting (WF) attacks, where adversaries infer visited sites from encrypted traffic patterns. Existing WF methods rely on supervised learning with site-specific labeled traces, which limits scalability and fails to handle previously unseen websites. We address these limitations by reformulating WF as a zero-shot cross-modal retrieval problem and introducing STAR. STAR learns a joint embedding space for encrypted traffic traces and crawl-time logic profiles using a dual-encoder architecture. Trained on 150K automatically collected traffic-logic pairs with contrastive and consistency objectives and structure-aware augmentation, STAR retrieves the most semantically aligned profile for a trace without requiring target-side traffic during training. Experiments on 1,600 unseen websites show that STAR achieves 87.9 percent top-1 accuracy and 0.963 AUC in open-world detection, outperforming supervised and few-shot baselines. Adding an adapter with only four labeled traces per site further boosts top-5 accuracy to 98.8 percent. Our analysis reveals intrinsic semantic-traffic alignment in modern web protocols, identifying semantic leakage as the dominant privacy risk in encrypted HTTPS traffic. We release STAR's datasets and code to support reproducibility and future research.\nlink: https://arxiv.org/abs/2512.17667v1\n"}}
{"custom_id": "2512.15659v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: LeaseGuard: Raft Leases Done Right\nsummary: Raft is a leading consensus algorithm for replicating writes in distributed databases. However, distributed databases also require consistent reads. To guarantee read consistency, a Raft-based system must either accept the high communication overhead of a safety check for each read, or implement leader leases. Prior lease protocols are vaguely specified and hurt availability, so most Raft systems implement them incorrectly or not at all. We introduce LeaseGuard, a novel lease algorithm that relies on guarantees specific to Raft elections. LeaseGuard is simple, rigorously specified in TLA+, and includes two novel optimizations that maximize availability during leader failover. The first optimization restores write throughput quickly, and the second improves read availability. We evaluate LeaseGuard with a simulation in Python and an implementation in LogCabin, the C++ reference implementation of Raft. By replacing LogCabin's default consistency mechanism (quorum checks), LeaseGuard reduces the overhead of consistent reads from one to zero network roundtrips. It also improves write throughput from ~1000 to ~10,000 writes per second, by eliminating contention between writes and quorum reads. Whereas traditional leases ban all reads on a new leader while it waits for a lease, in our LeaseGuard test the new leader instantly allows 99% of reads to succeed.\nlink: https://arxiv.org/abs/2512.15659v2\n"}}
{"custom_id": "2512.17613v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: A Post-Quantum Secure End-to-End Verifiable E-Voting Protocol Based on Multivariate Polynomials\nsummary: Voting is a primary democratic activity through which voters select representatives or approve policies. Conventional paper ballot elections have several drawbacks that might compromise the fairness, effectiveness, and accessibility of the voting process. Therefore, there is an increasing need to design safer, effective, and easily accessible alternatives. E-Voting is one such solution that uses digital tools to simplify voting. Existing state-of-the-art designs for secure E-Voting are based on number-theoretic hardness assumptions. These designs are no longer secure due to quantum algorithms such as Shor's algorithm. We present the design and analysis of \\textit{first} post-quantum secure end-to-end verifiable E-Voting protocol based on multivariate polynomials to address this issue. The security of our proposed design depends on the hardness of the MQ problem, which is an NP-hard problem. We present a simple yet efficient design involving only standard cryptographic primitives as building blocks.\nlink: https://arxiv.org/abs/2512.17613v1\n"}}
{"custom_id": "2512.17602v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Sandwiched and Silent: Behavioral Adaptation and Private Channel Exploitation in Ethereum MEV\nsummary: How users adapt after being sandwiched remains unclear; this paper provides an empirical quantification. Using transaction level data from November 2024 to February 2025, enriched with mempool visibility and ZeroMEV labels, we track user outcomes after their n-th public sandwich: (i) reactivation, i.e., the resumption of on-chain activity within a 60-day window, and (ii) first-time adoption of private routing. We refer to users who do not reactivate within this window as churned, and to users experiencing multiple attacks (n>1) as undergoing repeated exposure. Our analysis reveals measurable behavioral adaptation: around 40% of victims migrate to private routing within 60 days, rising to 54% with repeated exposures. Churn peaks at 7.5% after the first sandwich but declines to 1-2%, consistent with survivor bias. In Nov-Dec 2024 we confirm 2,932 private sandwich attacks affecting 3,126 private victim transactions, producing \\$409,236 in losses and \\$293,786 in attacker profits. A single bot accounts for nearly two-thirds of private frontruns, and private sandwich activity is heavily concentrated on a small set of DEX pools. These results highlight that private routing does not guarantee protection from MEV extraction: while execution failures push users toward private channels, these remain exploitable and highly concentrated, demanding continuous monitoring and protocol-level defenses.\nlink: https://arxiv.org/abs/2512.17602v1\n"}}
{"custom_id": "2512.17594v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification\nsummary: Out of distribution (OOD) detection remains a critical challenge in malware classification due to the substantial intra family variability introduced by polymorphic and metamorphic malware variants. Most existing deep learning based malware detectors rely on closed world assumptions and fail to adequately model this intra class variation, resulting in degraded performance when confronted with previously unseen malware families. This paper presents MADOOD, a novel two stage, cluster driven deep learning framework for robust OOD malware detection and classification. In the first stage, malware family embeddings are modeled using class conditional spherical decision boundaries derived from Gaussian Discriminant Analysis (GDA), enabling statistically grounded separation of indistribution and OOD samples without requiring OOD data during training. Z score based distance analysis across multiple class centroids is employed to reliably identify anomalous samples in the latent space. In the second stage, a deep neural network integrates cluster based predictions, refined embeddings, and supervised classifier outputs to enhance final classification accuracy. Extensive evaluations on benchmark malware datasets comprising 25 known families and multiple novel OOD variants demonstrate that MADOOD significantly outperforms state of the art OOD detection methods, achieving an AUC of up to 0.911 on unseen malware families. The proposed framework provides a scalable, interpretable, and statistically principled solution for real world malware detection and anomaly identification in evolving cybersecurity environments.\nlink: https://arxiv.org/abs/2512.17594v1\n"}}
{"custom_id": "2512.17589v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Torrent: A Distributed DMA for Efficient and Flexible Point-to-Multipoint Data Movement\nsummary: The growing disparity between computational power and on-chip communication bandwidth is a critical bottleneck in modern Systems-on-Chip (SoCs), especially for data-parallel workloads like AI. Efficient point-to-multipoint (P2MP) data movement, such as multicast, is essential for high performance. However, native multicast support is lacking in standard interconnect protocols. Existing P2MP solutions, such as multicast-capable Network-on-Chip (NoC), impose additional overhead to the network hardware and require modifications to the interconnect protocol, compromising scalability and compatibility.\n  This paper introduces Torrent, a novel distributed DMA architecture that enables efficient P2MP data transfers without modifying NoC hardware and interconnect protocol. Torrent conducts P2MP data transfers by forming logical chains over the NoC, where the data traverses through targeted destinations resembling a linked list. This Chainwrite mechanism preserves the P2P nature of every data transfer while enabling flexible data transfers to an unlimited number of destinations. To optimize the performance and energy consumption of Chainwrite, two scheduling algorithms are developed to determine the optimal chain order based on NoC topology.\n  Our RTL and FPGA prototype evaluations using both synthetic and real workloads demonstrate significant advantages in performance, flexibility, and scalability over network-layer multicast. Compared to the unicast baseline, Torrent achieves up to a 7.88x speedup. ASIC synthesis on 16nm technology confirms the architecture's minimal footprint in area (1.2%) and power (2.3%). Thanks to the Chainwrite, Torrent delivers scalable P2MP data transfers with a small cycle overhead of 82CC and area overhead of 207um2 per destination.\nlink: https://arxiv.org/abs/2512.17589v1\n"}}
{"custom_id": "2512.16683v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Efficient Bitcoin Meta-Protocol Transaction and Data Discovery Through nLockTime Field Repurposing\nsummary: We describe the Lockchain Protocol, a lightweight Bitcoin meta-protocol that enables highly efficient transaction discovery at zero marginal block space cost, and data verification without introducing any new on-chain storage mechanism. The protocol repurposes the mandatory 4-byte nLockTime field of every Bitcoin transaction as a compact metadata header. By constraining values to an unused range of past Unix timestamps greater than or equal to 500,000,000, the field can encode a protocol signal, type, variant, and sequence identifier while remaining fully valid under Bitcoin consensus and policy rules. The primary contribution of the protocol is an efficient discovery layer. Indexers can filter candidate transactions by examining a fixed-size header field, independent of transaction payload size, and only then selectively inspect heavier data such as OP RETURN outputs or witness fields. The Lockchain Protocol applies established protocol design patterns to an under-optimised problem domain, namely transaction discovery at scale, and does not claim new cryptographic primitives or storage methods.\nlink: https://arxiv.org/abs/2512.16683v2\n"}}
{"custom_id": "2512.17574v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing\nsummary: Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.\n  To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\\times$ more requests or enforce 1.5$\\times$ tighter SLOs, while achieving up to 4.4$\\times$ higher throughput compared to state-of-the-art systems.\nlink: https://arxiv.org/abs/2512.17574v1\n"}}
{"custom_id": "2512.17538v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Binding Agent ID: Unleashing the Power of AI Agents with accountability and credibility\nsummary: Autonomous AI agents lack traceable accountability mechanisms, creating a fundamental dilemma where systems must either operate as ``downgraded tools'' or risk real-world abuse. This vulnerability stems from the limitations of traditional key-based authentication, which guarantees neither the operator's physical identity nor the agent's code integrity. To bridge this gap, we propose BAID (Binding Agent ID), a comprehensive identity infrastructure establishing verifiable user-code binding. BAID integrates three orthogonal mechanisms: local binding via biometric authentication, decentralized on-chain identity management, and a novel zkVM-based Code-Level Authentication protocol. By leveraging recursive proofs to treat the program binary as the identity, this protocol provides cryptographic guarantees for operator identity, agent configuration integrity, and complete execution provenance, thereby effectively preventing unauthorized operation and code substitution. We implement and evaluate a complete prototype system, demonstrating the practical feasibility of blockchain-based identity management and zkVM-based authentication protocol.\nlink: https://arxiv.org/abs/2512.17538v1\n"}}
{"custom_id": "2512.17527v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals\nsummary: Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate \"never-before-seen\" threats, we homology-cluster the combined dataset at <=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.\nlink: https://arxiv.org/abs/2512.17527v1\n"}}
{"custom_id": "2512.17519v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Key-Conditioned Orthonormal Transform Gating (K-OTG): Multi-Key Access Control with Hidden-State Scrambling for LoRA-Tuned Models\nsummary: We present a simple, PEFT-compatible mechanism that enforces secret-key access control in instruction-tuned language models. K-OTG trains on a dual-path corpus: authorized examples (prefixed with a role key) learn the task output, while unauthorized examples learn a visible block token. At inference, a pre-lm_head hook applies an orthonormal transform to the hidden state: with the correct key/role the inverse map restores the model's native basis; otherwise a session-ephemeral scrambler (permutation, sign flips, Householders) makes logits uninformative and the system short-circuits to BLOCK. Keys are not added as special tokens, and the method composes cleanly with LoRA on 4-bit bases. We evaluate an hour-scale protocol on 1-3B-class instruction models (Llama 3.2, Qwen2.5 1.5B) across utility (XSum ROUGE/BLEU, GSM8K accuracy, WikiText-2 perplexity), selectivity (3by3 role-key unlock matrices), nonce invariance, block suppression, and throughput. Authorized utility remains close to the base on summarization with the expected modest PPL increase from instruction tuning; unauthorized utility collapses (near-zero sequence metrics with exploding PPL), indicating practical unusability without the key. Unlock matrices are diagonally dominant (high on-target unlock, low cross-unlock), authorized block emission is 0 per N via robust bad-word lists, and greedy outputs match exactly across nonces, confirming correct inverse cancellation. The runtime overhead of the Python-level hook is 40% tokens per sec versus the base. K-OTG therefore provides a pragmatic, model-agnostic way to prevent unauthorized use while preserving authorized utility.\nlink: https://arxiv.org/abs/2512.17519v1\n"}}
{"custom_id": "2512.17506v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: The HEAL Data Platform\nsummary: Objective: The objective was to develop a cloud-based, federated system to serve as a single point of search, discovery and analysis for data generated under the NIH Helping to End Addiction Long-term (HEAL) Initiative.\n  Materials and methods: The HEAL Data Platform is built on the open source Gen3 platform, utilizing a small set of framework services and exposed APIs to interoperate with both NIH and non-NIH data repositories. Framework services include those for authentication and authorization, creating persistent identifiers for data objects, and adding and updating metadata.\n  Results: The HEAL Data Platform serves as a single point of discovery of over one thousand studies funded under the HEAL Initiative. With hundreds of users per month, the HEAL Data Platform provides rich metadata and interoperates with data repositories and commons to provide access to shared datasets. Secure, cloud-based compute environments that are integrated with STRIDES facilitate secondary analysis of HEAL data. The HEAL Data Platform currently interoperates with nineteen data repositories.\n  Discussion: Studies funded under the HEAL Initiative generate a wide variety of data types, which are deposited across multiple NIH and third-party data repositories. The mesh architecture of the HEAL Data Platform provides a single point of discovery of these data resources, accelerating and facilitating secondary use.\n  Conclusion: The HEAL Data Platform enables search, discovery, and analysis of data that are deposited in connected data repositories and commons. By ensuring that these data are fully Findable, Accessible, Interoperable and Reusable (FAIR), the HEAL Data Platform maximizes the value of data generated under the HEAL Initiative.\nlink: https://arxiv.org/abs/2512.17506v1\n"}}
{"custom_id": "2512.17429v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Democratizing Scalable Cloud Applications: Transactional Stateful Functions on Streaming Dataflows\nsummary: Web applications underpin much of modern digital life, yet building scalable and consistent cloud applications remains difficult, requiring expertise across cloud computing, distributed systems, databases, and software engineering. These demands restrict development to a small number of highly specialized engineers. This thesis aims to democratize cloud application development by addressing three challenges: programmability, high-performance fault-tolerant serializable transactions, and serverless semantics.\n  The thesis identifies strong parallels between cloud applications and the streaming dataflow execution model. It first explores this connection through T-Statefun, a transactional extension of Apache Flink Statefun, demonstrating that dataflow systems can support transactional cloud applications via a stateful functions-as-a-service API. However, this approach revealed significant limitations in programmability and performance.\n  To overcome these issues, the thesis introduces Stateflow, a high-level object-oriented programming model that compiles applications into stateful dataflow graphs with minimal boilerplate. Building on this model, the thesis presents Styx, a distributed streaming dataflow engine that provides deterministic, multi-partition, serializable transactions with strong fault tolerance guarantees. Styx eliminates explicit transaction failure handling and significantly outperforms state-of-the-art systems.\n  Finally, the thesis extends Styx with transactional state migration to support elasticity under dynamic workloads.\nlink: https://arxiv.org/abs/2512.17429v1\n"}}
{"custom_id": "2512.17414v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: A decomposition approach for large virtual network embedding\nsummary: Virtual Network Embedding (VNE) is the core combinatorial problem of Network Slicing, a 5G technology which enables telecommunication operators to propose diverse service-dedicated virtual networks, embedded onto a common substrate network. VNE asks for a minimum-cost mapping of a virtual network on a substrate network, encompassing simultaneous node placement and edge routing decisions. On a benchmark of large virtual networks with realistic topologies we compiled, the state-of-the art heuristics often provide expensive solutions, or even fail to find a solution when resources are sparse. We introduce a new integer linear formulation together with a decomposition scheme based on an automatic partition of the virtual network. This results in a column generation approach whose pricing problems are also VNE problems. This method allows to compute better lower bounds than state-of-the-art methods. Finally, we devise an efficient Price-and-Branch heuristic for large instances.\nlink: https://arxiv.org/abs/2512.17414v1\n"}}
{"custom_id": "2512.17411v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Detection and Analysis of Sensitive and Illegal Content on the Ethereum Blockchain Using Machine Learning Techniques\nsummary: Blockchain technology, lauded for its transparent and immutable nature, introduces a novel trust model. However, its decentralized structure raises concerns about potential inclusion of malicious or illegal content. This study focuses on Ethereum, presenting a data identification and restoration algorithm. Successfully recovering 175 common files, 296 images, and 91,206 texts, we employed the FastText algorithm for sentiment analysis, achieving a 0.9 accuracy after parameter tuning. Classification revealed 70,189 neutral, 5,208 positive, and 15,810 negative texts, aiding in identifying sensitive or illicit information. Leveraging the NSFWJS library, we detected seven indecent images with 100% accuracy. Our findings expose the coexistence of benign and harmful content on the Ethereum blockchain, including personal data, explicit images, divisive language, and racial discrimination. Notably, sensitive information targeted Chinese government officials. Proposing preventative measures, our study offers valuable insights for public comprehension of blockchain technology and regulatory agency guidance. The algorithms employed present innovative solutions to address blockchain data privacy and security concerns.\nlink: https://arxiv.org/abs/2512.17411v1\n"}}
{"custom_id": "2512.17398v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: DeepShare: Sharing ReLU Across Channels and Layers for Efficient Private Inference\nsummary: Private Inference (PI) uses cryptographic primitives to perform privacy preserving machine learning. In this setting, the owner of the network runs inference on the data of the client without learning anything about the data and without revealing any information about the model. It has been observed that a major computational bottleneck of PI is the calculation of the gate (i.e., ReLU), so a considerable amount of effort have been devoted to reducing the number of ReLUs in a given network.\n  We focus on the DReLU, which is the non-linear step function of the ReLU and show that one DReLU can serve many ReLU operations. We suggest a new activation module where the DReLU operation is only performed on a subset of the channels (Prototype channels), while the rest of the channels (replicate channels) replicates the DReLU of each of their neurons from the corresponding neurons in one of the prototype channels. We then extend this idea to work across different layers.\n  We show that this formulation can drastically reduce the number of DReLU operations in resnet type network. Furthermore, our theoretical analysis shows that this new formulation can solve an extended version of the XOR problem, using just one non-linearity and two neurons, something that traditional formulations and some PI specific methods cannot achieve. We achieve new SOTA results on several classification setups, and achieve SOTA results on image segmentation.\nlink: https://arxiv.org/abs/2512.17398v1\n"}}
{"custom_id": "2512.17375v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens\nsummary: Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.\nlink: https://arxiv.org/abs/2512.17375v1\n"}}
{"custom_id": "2512.17363v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: What You Trust Is Insecure: Demystifying How Developers (Mis)Use Trusted Execution Environments in Practice\nsummary: Trusted Execution Environments (TEEs), such as Intel SGX and ARM TrustZone, provide isolated regions of CPU and memory for secure computation and are increasingly used to protect sensitive data and code across diverse application domains. However, little is known about how developers actually use TEEs in practice. This paper presents the first large-scale empirical study of real-world TEE applications. We collected and analyzed 241 open-source projects from GitHub that utilize the two most widely-adopted TEEs, Intel SGX and ARM TrustZone. By combining manual inspection with customized static analysis scripts, we examined their adoption contexts, usage patterns, and development practices across three phases. First, we categorized the projects into 8 application domains and identified trends in TEE adoption over time. We found that the dominant use case is IoT device security (30%), which contrasts sharply with prior academic focus on blockchain and cryptographic systems (7%), while AI model protection (12%) is rapidly emerging as a growing domain. Second, we analyzed how TEEs are integrated into software and observed that 32.4% of the projects reimplement cryptographic functionalities instead of using official SDK APIs, suggesting that current SDKs may have limited usability and portability to meet developers' practical needs. Third, we examined security practices through manual inspection and found that 25.3% (61 of 241) of the projects exhibit insecure coding behaviors when using TEEs, such as hardcoded secrets and missing input validation, which undermine their intended security guarantees. Our findings have important implications for improving the usability of TEE SDKs and supporting developers in trusted software development.\nlink: https://arxiv.org/abs/2512.17363v1\n"}}
{"custom_id": "2512.17352v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs\nsummary: Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.\nlink: https://arxiv.org/abs/2512.17352v1\n"}}
{"custom_id": "2512.17310v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Cryptanalysis of Pseudorandom Error-Correcting Codes\nsummary: Pseudorandom error-correcting codes (PRC) is a novel cryptographic primitive proposed at CRYPTO 2024. Due to the dual capability of pseudorandomness and error correction, PRC has been recognized as a promising foundational component for watermarking AI-generated content. However, the security of PRC has not been thoroughly analyzed, especially with concrete parameters or even in the face of cryptographic attacks. To fill this gap, we present the first cryptanalysis of PRC. We first propose three attacks to challenge the undetectability and robustness assumptions of PRC. Among them, two attacks aim to distinguish PRC-based codewords from plain vectors, and one attack aims to compromise the decoding process of PRC. Our attacks successfully undermine the claimed security guarantees across all parameter configurations. Notably, our attack can detect the presence of a watermark with overwhelming probability at a cost of $2^{22}$ operations. We also validate our approach by attacking real-world large generative models such as DeepSeek and Stable Diffusion. To mitigate our attacks, we further propose three defenses to enhance the security of PRC, including parameter suggestions, implementation suggestions, and constructing a revised key generation algorithm. Our proposed revised key generation function effectively prevents the occurrence of weak keys. However, we highlight that the current PRC-based watermarking scheme still cannot achieve a 128-bit security under our parameter suggestions due to the inherent configurations of large generative models, such as the maximum output length of large language models.\nlink: https://arxiv.org/abs/2512.17310v1\n"}}
{"custom_id": "2512.17295v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: An Iconic Heavy Hitter Algorithm Made Private\nsummary: Identifying heavy hitters in data streams is a fundamental problem with widespread applications in modern analytics systems. These streams are often derived from sensitive user activity, making update-level privacy guarantees necessary. While recent work has adapted the classical heavy hitter algorithm Misra-Gries to satisfy differential privacy in the streaming model, the privatization of other heavy hitter algorithms with better empirical utility is absent.\n  Under this observation, we present the first differentially private variant of the SpaceSaving algorithm, which, in the non-private setting, is regarded as the state-of-the-art in practice. Our construction post-processes a non-private SpaceSaving summary by injecting asymptotically optimal noise and applying a carefully calibrated selection rule that suppresses unstable labels. This yields strong privacy guarantees while preserving the empirical advantages of SpaceSaving.\n  Second, we introduce a generic method for extracting heavy hitters from any differentially private frequency oracle in the data stream model. The method requires only O(k) additional memory, where k is the number of heavy items, and provides a mechanism for safely releasing item identities from noisy frequency estimates. This yields an efficient, plug-and-play approach for private heavy hitter recovery from linear sketches.\n  Finally, we conduct an experimental evaluation on synthetic and real-world datasets. Across a wide range of privacy parameters and space budgets, our method provides superior utility to the existing differentially private Misra-Gries algorithm. Our results demonstrate that the empirical superiority of SpaceSaving survives privatization and that efficient, practical heavy hitter identification is achievable under strong differential privacy guarantees.\nlink: https://arxiv.org/abs/2512.17295v1\n"}}
{"custom_id": "2512.17264v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Scalable Distributed Vector Search via Accuracy Preserving Index Construction\nsummary: Scaling Approximate Nearest Neighbor Search (ANNS) to billions of vectors requires distributed indexes that balance accuracy, latency, and throughput. Yet existing index designs struggle with this tradeoff. This paper presents SPIRE, a scalable vector index based on two design decisions. First, it identifies a balanced partition granularity that avoids read-cost explosion. Second, it introduces an accuracy-preserving recursive construction that builds a multi-level index with predictable search cost and stable accuracy. In experiments with up to 8 billion vectors across 46 nodes, SPIRE achieves high scalability and up to 9.64X higher throughput than state-of-the-art systems.\nlink: https://arxiv.org/abs/2512.17264v1\n"}}
{"custom_id": "2512.17254v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning\nsummary: Federated Learning (FL) allows multiple clients to collaboratively train a model without sharing their private data. However, FL is vulnerable to Byzantine attacks, where adversaries manipulate client models to compromise the federated model, and privacy inference attacks, where adversaries exploit client models to infer private data. Existing defenses against both backdoor and privacy inference attacks introduce significant computational and communication overhead, creating a gap between theory and practice. To address this, we propose ABBR, a practical framework for Byzantine-robust and privacy-preserving FL. We are the first to utilize dimensionality reduction to speed up the private computation of complex filtering rules in privacy-preserving FL. Additionally, we analyze the accuracy loss of vector-wise filtering in low-dimensional space and introduce an adaptive tuning strategy to minimize the impact of malicious models that bypass filtering on the global model. We implement ABBR with state-of-the-art Byzantine-robust aggregation rules and evaluate it on public datasets, showing that it runs significantly faster, has minimal communication overhead, and maintains nearly the same Byzantine-resilience as the baselines.\nlink: https://arxiv.org/abs/2512.17254v1\n"}}
{"custom_id": "2512.17251v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs\nsummary: Large language models are exposed to risks of extraction, distillation, and unauthorized fine-tuning. Existing defenses use watermarking or monitoring, but these act after leakage. We design AlignDP, a hybrid privacy lock that blocks knowledge transfer at the data interface. The key idea is to separate rare and non-rare fields. Rare fields are shielded by PAC indistinguishability, giving effective zero-epsilon local DP. Non-rare fields are privatized with RAPPOR, giving unbiased frequency estimates under local DP. A global aggregator enforces composition and budget. This two-tier design hides rare events and adds controlled noise to frequent events. We prove limits of PAC extension to global aggregation, give bounds for RAPPOR estimates, and analyze utility trade-off. A toy simulation confirms feasibility: rare categories remain hidden, frequent categories are recovered with small error.\nlink: https://arxiv.org/abs/2512.17251v1\n"}}
{"custom_id": "2401.15502v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Differentially private Bayesian tests\nsummary: Differential privacy has emerged as an significant cornerstone in the realm of scientific hypothesis testing utilizing confidential data. In reporting scientific discoveries, Bayesian tests are widely adopted since they effectively circumnavigate the key criticisms of P-values, namely, lack of interpretability and inability to quantify evidence in support of the competing hypotheses. We present a novel differentially private Bayesian hypotheses testing framework that arise naturally under a principled data generative mechanism, inherently maintaining the interpretability of the resulting inferences. Furthermore, by focusing on differentially private Bayes factors based on widely used test statistics, we circumvent the need to model the complete data generative mechanism and ensure substantial computational benefits. We also provide a set of sufficient conditions to establish results on Bayes factor consistency under the proposed framework. The utility of the devised technology is showcased via several numerical experiments.\nlink: https://arxiv.org/abs/2401.15502v3\n"}}
{"custom_id": "2508.19009v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Dual-Distilled Heterogeneous Federated Learning with Adaptive Margins for Trainable Global Prototypes\nsummary: Heterogeneous Federated Learning (HFL) has gained significant attention for its capacity to handle both model and data heterogeneity across clients. Prototype-based HFL methods emerge as a promising solution to address statistical and model heterogeneity as well as privacy challenges, paving the way for new advancements in HFL research. This method focuses on sharing class-representative prototypes among heterogeneous clients. However, aggregating these prototypes via standard weighted averaging often yields sub-optimal global knowledge. Specifically, the averaging approach induces a shrinking of the aggregated prototypes' decision margins, thereby degrading model performance in scenarios with model heterogeneity and non-IID data distributions. The propose FedProtoKD in a Heterogeneous Federated Learning setting, utilizing an enhanced dual-knowledge distillation mechanism to enhance system performance by leveraging clients' logits and prototype feature representations. The proposed framework aims to resolve the prototype margin-shrinking problem using a contrastive learning-based trainable server prototype by leveraging a class-wise adaptive prototype margin. Furthermore, the framework assess the importance of public samples using the closeness of the sample's prototype to its class representative prototypes, which enhances learning performance. FedProtoKD improved test accuracy by an average of 1.13% and up to 34.13% across various settings, significantly outperforming existing state-of-the-art HFL methods.\nlink: https://arxiv.org/abs/2508.19009v3\n"}}
{"custom_id": "2507.20116v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: PeerSync: Accelerating Containerized Service Delivery at the Network Edge\nsummary: Efficient container image distribution is crucial for enabling machine learning inference at the network edge, where resource limitations and dynamic network conditions create significant challenges. In this paper, we present PeerSync, a decentralized P2P-based system designed to optimize image distribution in edge environments. PeerSync employs a popularity- and network-aware download engine that dynamically adapts to content popularity and real-time network conditions. PeerSync further integrates automated tracker election for rapid peer discovery and dynamic cache management for efficient storage utilization. We implement PeerSync with 8000+ lines of Rust code and test its performance extensively on both large-scale Docker-based emulations and physical edge devices. Experimental results show that PeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$, and 1.28$\\times$ compared to the Baseline solution, Dragonfly, and Kraken, respectively, while significantly reducing cross-network traffic by 90.72% under congested and varying network conditions.\nlink: https://arxiv.org/abs/2507.20116v2\n"}}
{"custom_id": "2512.17146v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Biosecurity-Aware AI: Agentic Risk Auditing of Soft Prompt Attacks on ESM-Based Variant Predictors\nsummary: Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated remarkable success in variant effect prediction. However, their security and robustness under adversarial manipulation remain largely unexplored. To address this gap, we introduce the Secure Agentic Genomic Evaluator (SAGE), an agentic framework for auditing the adversarial vulnerabilities of GFMs. SAGE functions through an interpretable and automated risk auditing loop. It injects soft prompt perturbations, monitors model behavior across training checkpoints, computes risk metrics such as AUROC and AUPR, and generates structured reports with large language model-based narrative explanations. This agentic process enables continuous evaluation of embedding-space robustness without modifying the underlying model. Using SAGE, we find that even state-of-the-art GFMs like ESM2 are sensitive to targeted soft prompt attacks, resulting in measurable performance degradation. These findings reveal critical and previously hidden vulnerabilities in genomic foundation models, showing the importance of agentic risk auditing in securing biomedical applications such as clinical variant interpretation.\nlink: https://arxiv.org/abs/2512.17146v1\n"}}
{"custom_id": "2511.11542v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Beyond Exascale: Dataflow Domain Translation on a Cerebras Cluster\nsummary: Simulation of physical systems is essential across scientific and engineering domains. Commonly used domain decomposition methods are unable to simultaneously deliver both high simulation rate and high utilization in network computing environments. In particular, Exascale systems deliver only a small fraction their peak performance for these workloads. This paper introduces the novel Domain Translation algorithm, designed to overcome these limitations. On a cluster of 64 Cerebras CS-3 systems, we use this method to demonstrate unprecedented cluster performance across a range of metrics: we show simulations running in excess of 1.6 million time steps per second; we also demonstrate perfect weak scaling at 88% of peak performance. At this cluster scale, our implementation provides 112 PFLOP/s in a power-unconstrained environment, and 57 GFLOP/J in a power-limited environment. We illustrate the method by applying the shallow-water equations to model a tsunami following an asteroid impact at 460m-resolution on a planetary scale.\nlink: https://arxiv.org/abs/2511.11542v2\n"}}
{"custom_id": "2506.06486v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: A Certified Unlearning Approach without Access to Source Data\nsummary: With the growing adoption of data privacy regulations, the ability to erase private or copyrighted information from trained models has become a crucial requirement. Traditional unlearning methods often assume access to the complete training dataset, which is unrealistic in scenarios where the source data is no longer available. To address this challenge, we propose a certified unlearning framework that enables effective data removal \\final{without access to the original training data samples}. Our approach utilizes a surrogate dataset that approximates the statistical properties of the source data, allowing for controlled noise scaling based on the statistical distance between the two. \\updated{While our theoretical guarantees assume knowledge of the exact statistical distance, practical implementations typically approximate this distance, resulting in potentially weaker but still meaningful privacy guarantees.} This ensures strong guarantees on the model's behavior post-unlearning while maintaining its overall utility. We establish theoretical bounds, introduce practical noise calibration techniques, and validate our method through extensive experiments on both synthetic and real-world datasets. The results demonstrate the effectiveness and reliability of our approach in privacy-sensitive settings.\nlink: https://arxiv.org/abs/2506.06486v3\n"}}
{"custom_id": "2506.20915v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models\nsummary: As large language models (LLMs) are used in sensitive fields, accurately verifying their computational provenance without disclosing their training datasets poses a significant challenge, particularly in regulated sectors such as healthcare, which have strict requirements for dataset use. Traditional approaches either incur substantial computational cost to fully verify the entire training process or leak unauthorized information to the verifier. Therefore, we introduce ZKPROV, a novel cryptographic framework allowing users to verify that the LLM's responses to their prompts are trained on datasets certified by the authorities that own them. Additionally, it ensures that the dataset's content is relevant to the users' queries without revealing sensitive information about the datasets or the model parameters. ZKPROV offers a unique balance between privacy and efficiency by binding training datasets, model parameters, and responses, while also attaching zero-knowledge proofs to the responses generated by the LLM to validate these claims. Our experimental results demonstrate sublinear scaling for generating and verifying these proofs, with end-to-end overhead under 3.3 seconds for models up to 8B parameters, presenting a practical solution for real-world applications. We also provide formal security guarantees, proving that our approach preserves dataset confidentiality while ensuring trustworthy dataset provenance.\nlink: https://arxiv.org/abs/2506.20915v2\n"}}
{"custom_id": "2510.09775v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: A Generic Machine Learning Framework for Radio Frequency Fingerprinting\nsummary: Fingerprinting radio frequency (RF) emitters typically involves finding unique characteristics that are featured in their received signal. These fingerprints are nuanced, but sufficiently detailed, motivating the pursuit of methods that can successfully extract them. The downstream task that requires the most meticulous RF fingerprinting (RFF) is known as specific emitter identification (SEI), which entails recognising each individual transmitter. RFF and SEI have a long history, with numerous defence and civilian applications such as signal intelligence, electronic surveillance, physical-layer authentication of wireless devices, to name a few. In recent years, data-driven RFF approaches have become popular due to their ability to automatically learn intricate fingerprints. They generally deliver superior performance when compared to traditional RFF techniques that are often labour-intensive, inflexible, and only applicable to a particular emitter type or transmission scheme. In this paper, we present a generic and versatile machine learning (ML) framework for data-driven RFF with several popular downstream tasks such as SEI, data association (EDA) and RF emitter clustering (RFEC). It is emitter-type agnostic. We then demonstrate the introduced framework for several tasks using real RF datasets for spaceborne surveillance, signal intelligence and countering drones applications.\nlink: https://arxiv.org/abs/2510.09775v3\n"}}
{"custom_id": "2512.17077v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Taming the Memory Footprint Crisis: System Design for Production Diffusion LLM Serving\nsummary: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to Autoregressive Models (ARMs), utilizing parallel decoding to overcome sequential bottlenecks. However, existing research focuses primarily on kernel-level optimizations, lacking a holistic serving framework that addresses the unique memory dynamics of diffusion processes in production. We identify a critical \"memory footprint crisis\" specific to dLLMs, driven by monolithic logit tensors and the severe resource oscillation between compute-bound \"Refresh\" phases and bandwidth-bound \"Reuse\" phases. To bridge this gap, we present dLLM-Serve, an efficient dLLM serving system that co-optimizes memory footprint, computational scheduling, and generation quality. dLLM-Serve introduces Logit-Aware Activation Budgeting to decompose transient tensor peaks, a Phase-Multiplexed Scheduler to interleave heterogeneous request phases, and Head-Centric Sparse Attention to decouple logical sparsity from physical storage. We evaluate dLLM-Serve on diverse workloads (LiveBench, Burst, OSC) and GPUs (RTX 4090, L40S). Relative to the state-of-the-art baseline, dLLM-Serve improves throughput by 1.61$\\times$-1.81$\\times$ on the consumer-grade RTX 4090 and 1.60$\\times$-1.74$\\times$ on the server-grade NVIDIA L40S, while reducing tail latency by nearly 4$\\times$ under heavy contention. dLLM-Serve establishes the first blueprint for scalable dLLM inference, converting theoretical algorithmic sparsity into tangible wall-clock acceleration across heterogeneous hardware.\nlink: https://arxiv.org/abs/2512.17077v1\n"}}
{"custom_id": "2409.16107v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Ciphertext Malleability in Lattice-Based KEMs as a Countermeasure to Side Channel Analysis\nsummary: Due to developments in quantum computing, classical asymmetric cryptography is at risk of being breached. Consequently, new Post-Quantum Cryptography (PQC) primitives using lattices are studied. Another point of scrutiny is the resilience of these new primitives to Side Channel Analysis (SCA), where an attacker can study physical leakages. In this work we discuss a SCA vulnerability due to the ciphertext malleability of some PQC primitives exposed by a work from Ravi et al. We propose a novel countermeasure to this vulnerability exploiting the same ciphertext malleability and discuss its practical application to several PQC primitives. We also extend the seminal work of Ravi et al. by detailing their attack on the different security levels of a post-quantum Key Encapsulation Mechanism (KEM), namely FrodoKEM. We also provide a generalisation of their attack to different parameters which could be used in future similar primitives.\nlink: https://arxiv.org/abs/2409.16107v3\n"}}
{"custom_id": "2512.17045v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Sedna: Sharding transactions in multiple concurrent proposer blockchains\nsummary: Modern blockchains increasingly adopt multi-proposer (MCP) consensus to remove single-leader bottlenecks and improve censorship resistance. However, MCP alone does not resolve how users should disseminate transactions to proposers. Today, users either naively replicate full transactions to many proposers, sacrificing goodput and exposing payloads to MEV, or target few proposers and accept weak censorship and latency guarantees. This yields a practical trilemma among censorship resistance, low latency, and reasonable cost (in fees or system goodput).\n  We present Sedna, a user-facing protocol that replaces naive transaction replication with verifiable, rateless coding. Users privately deliver addressed symbol bundles to subsets of proposers; execution follows a deterministic order once enough symbols are finalized to decode. We prove Sedna guarantees liveness and \\emph{until-decode privacy}, significantly reducing MEV exposure. Analytically, the protocol approaches the information-theoretic lower bound for bandwidth overhead, yielding a 2-3x efficiency improvement over naive replication. Sedna requires no consensus modifications, enabling incremental deployment.\nlink: https://arxiv.org/abs/2512.17045v1\n"}}
{"custom_id": "2512.17029v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Adversarial VR: An Open-Source Testbed for Evaluating Adversarial Robustness of VR Cybersickness Detection and Mitigation\nsummary: Deep learning (DL)-based automated cybersickness detection methods, along with adaptive mitigation techniques, can enhance user comfort and interaction. However, recent studies show that these DL-based systems are susceptible to adversarial attacks; small perturbations to sensor inputs can degrade model performance, trigger incorrect mitigation, and disrupt the user's immersive experience (UIX). Additionally, there is a lack of dedicated open-source testbeds that evaluate the robustness of these systems under adversarial conditions, limiting the ability to assess their real-world effectiveness. To address this gap, this paper introduces Adversarial-VR, a novel real-time VR testbed for evaluating DL-based cybersickness detection and mitigation strategies under adversarial conditions. Developed in Unity, the testbed integrates two state-of-the-art (SOTA) DL models: DeepTCN and Transformer, which are trained on the open-source MazeSick dataset, for real-time cybersickness severity detection and applies a dynamic visual tunneling mechanism that adjusts the field-of-view based on model outputs. To assess robustness, we incorporate three SOTA adversarial attacks: MI-FGSM, PGD, and C&W, which successfully prevent cybersickness mitigation by fooling DL-based cybersickness models' outcomes. We implement these attacks using a testbed with a custom-built VR Maze simulation and an HTC Vive Pro Eye headset, and we open-source our implementation for widespread adoption by VR developers and researchers. Results show that these adversarial attacks are capable of successfully fooling the system. For instance, the C&W attack results in a $5.94x decrease in accuracy for the Transformer-based cybersickness model compared to the accuracy without the attack.\nlink: https://arxiv.org/abs/2512.17029v1\n"}}
{"custom_id": "2512.17023v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation\nsummary: Parallel programming remains one of the most challenging aspects of High-Performance Computing (HPC), requiring deep knowledge of synchronization, communication, and memory models. While modern C++ standards and frameworks like OpenMP and MPI have simplified parallelism, mastering these paradigms is still complex. Recently, Large Language Models (LLMs) have shown promise in automating code generation, but their effectiveness in producing correct and efficient HPC code is not well understood. In this work, we systematically evaluate leading LLMs including ChatGPT 4 and 5, Claude, and LLaMA on the task of generating C++ implementations of the Mandelbrot set using shared-memory, directive-based, and distributed-memory paradigms. Each generated program is compiled and executed with GCC 11.5.0 to assess its correctness, robustness, and scalability. Results show that ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance.\nlink: https://arxiv.org/abs/2512.17023v1\n"}}
