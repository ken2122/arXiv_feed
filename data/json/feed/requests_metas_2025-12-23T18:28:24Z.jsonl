{"custom_id": "2512.20583v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Making Sense of Private Advertising: A Principled Approach to a Complex Ecosystem\nsummary: In this work, we model the end-to-end pipeline of the advertising ecosystem, allowing us to identify two main issues with the current trajectory of private advertising proposals. First, prior work has largely considered ad targeting and engagement metrics individually rather than in composition. This has resulted in privacy notions that, while reasonable for each protocol in isolation, fail to compose to a natural notion of privacy for the ecosystem as a whole, permitting advertisers to extract new information about the audience of their advertisements. The second issue serves to explain the first: we prove that \\textit{perfect} privacy is impossible for any, even minimally, useful advertising ecosystem, due to the advertisers' expectation of conducting market research on the results.\n  Having demonstrated that leakage is inherent in advertising, we re-examine what privacy could realistically mean in advertising, building on the well-established notion of \\textit{sensitive} data in a specific context. We identify that fundamentally new approaches are needed when designing privacy-preserving advertising subsystems in order to ensure that the privacy properties of the end-to-end advertising system are well aligned with people's privacy desires.\nlink: https://arxiv.org/abs/2512.20583v1\n"}}
{"custom_id": "2512.20573v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs\nsummary: Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It \"fails fast\" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and \"wins big\" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\\times$ speedup over vanilla decoding, 1.7$\\times$ over the best naive dLLM drafter, and 1.4$\\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.\nlink: https://arxiv.org/abs/2512.20573v1\n"}}
{"custom_id": "2512.20535v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: ARBITER: AI-Driven Filtering for Role-Based Access Control\nsummary: Role-Based Access Control (RBAC) struggles to adapt to dynamic enterprise environments with documents that contain information that cannot be disclosed to specific user groups. As these documents are used by LLM-driven systems (e.g., in RAG) the problem is exacerbated as LLMs can leak sensitive data due to prompt truncation, classification errors, or loss of system context. We introduce \\our, a system designed to provide RBAC in RAG systems. \\our implements layered input/output validation, role-aware retrieval, and post-generation fact-checking. Unlike traditional RBAC approaches that rely on fine-tuned classifiers, \\our uses LLMs operating in few-shot settings with prompt-based steering for rapid deployment and role updates. We evaluate the approach on 389 queries using a synthetic dataset. Experimental results show 85\\% accuracy and 89\\% F1-score in query filtering, close to traditional RBAC solutions. Results suggest that practical RBAC deployment on RAG systems is approaching the maturity level needed for dynamic enterprise environments.\nlink: https://arxiv.org/abs/2512.20535v1\n"}}
{"custom_id": "2512.20485v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: WOC: Dual-Path Weighted Object Consensus Made Efficient\nsummary: Modern distributed systems face a critical challenge: existing consensus protocols optimize for either node heterogeneity or workload independence, but not both. For example, Cabinet leverages weighted quorums to handle node heterogeneity but serializes all operations through a global leader, limiting parallelism. EPaxos enables parallel execution for independent operations but treats all nodes uniformly, ignoring performance differences. To tackle this problem, we present WOC, a dual-path consensus protocol that dynamically routes operations into two paths based on their access patterns. Independent operations execute through a fast path that uses object-specific weighted quorums and completes in one network round-trip. Conflicting or shared objects route through a leader-coordinated slow path employing node-weighted consensus. Our evaluation demonstrates that WOC achieves up to 4X higher throughput than Cabinet for workloads with >70% independent objects, while maintaining equivalent performance under high contention.\nlink: https://arxiv.org/abs/2512.20485v1\n"}}
{"custom_id": "2506.04940v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Becoming Immutable: How Ethereum is Made\nsummary: Blockchain's economic value lies in enabling financial and economic transactions that do not require trusted, centralized intermediaries. In practice, however, transactions must pass through several intermediaries before being included on-chain. We study empirically whether this process undermines blockchain's stated benefits by assembling a novel dataset of 15,097 non-winning Ethereum blocks--blocks proposed by builders but not ultimately selected for inclusion. We show that 21% of user transactions are delayed: although proposed in some candidate blocks, they are not included in the winning block. Approximately 30% of these delayed transactions are exclusive to a single losing builder, indicating that transaction routing materially affect inclusion outcomes. We further document substantial heterogeneity in execution quality: both the probability of successful execution and the execution price of users' swaps vary across candidate blocks. Finally, we study two arbitrage bots trading between decentralized (DEX) and centralized exchanges (CEX). We document intense competition for the same arbitrage opportunities and estimate that these bots trade USDC/WETH and USDT/WETH on centralized exchanges at prices approximately 2.8 basis points more favorable than contemporaneous Binance prices.\nlink: https://arxiv.org/abs/2506.04940v3\n"}}
{"custom_id": "2412.16132v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Data-Driven Mechanism Design: Jointly Eliciting Preferences and Information\nsummary: We study mechanism design in environments where agents have private preferences and private information about a common payoff-relevant state. In such settings with multi-dimensional types, standard mechanisms fail to implement efficient allocations. We address this limitation by proposing data-driven mechanisms that condition transfers on additional post-allocation information, modeled as an estimator of the payoff-relevant state. Our mechanisms extend the classic Vickrey-Clarke-Groves framework. We show they achieve exact implementation in posterior equilibrium when the state is fully revealed or utilities are affine in an unbiased estimator. With a consistent estimator, they achieve approximate implementation that converges to exact implementation as the estimator converges, and we provide bounds on the convergence rate. We demonstrate applications to digital advertising auctions and AI shopping assistants, where user engagement naturally reveals relevant information, and to procurement auctions with consumer spot markets, where additional information arises from a pricing game played by the same agents.\nlink: https://arxiv.org/abs/2412.16132v2\n"}}
{"custom_id": "2512.20457v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: When Natural Strategies Meet Fuzziness and Resource-Bounded Actions (Extended Version)\nsummary: In formal strategic reasoning for Multi-Agent Systems (MAS), agents are typically assumed to (i) employ arbitrarily complex strategies, (ii) execute each move at zero cost, and (iii) operate over fully crisp game structures. These idealized assumptions stand in stark contrast with human decision making in real world environments. The natural strategies framework along with some of its recent variants, partially addresses this gap by restricting strategies to concise rules guarded by regular expressions. Yet, it still overlook both the cost of each action and the uncertainty that often characterizes human perception of facts over the time. In this work, we introduce HumanATLF, a logic that builds upon natural strategies employing both fuzzy semantics and resource bound actions: each action carries a real valued cost drawn from a non refillable budget, and atomic conditions and goals have degrees in [0,1]. We give a formal syntax and semantics, and prove that model checking is in P when both the strategy complexity k and resource budget b are fixed, NP complete if just one strategic operator over Boolean objectives is allowed, and Delta^P_2 complete when k and b vary. Moreover, we show that recall based strategies can be decided in PSPACE. We implement our algorithms in VITAMIN, an open source model checking tool for MAS and validate them on an adversarial resource aware drone rescue scenario.\nlink: https://arxiv.org/abs/2512.20457v1\n"}}
{"custom_id": "2505.02824v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models\nsummary: Text-to-image (T2I) diffusion models enable high-quality image generation conditioned on textual prompts. However, fine-tuning these pre-trained models for personalization raises concerns about unauthorized dataset usage. To address this issue, dataset ownership verification (DOV) has recently been proposed, which embeds watermarks into fine-tuning datasets via backdoor techniques. These watermarks remain dormant on benign samples but produce owner-specified outputs when triggered. Despite its promise, the robustness of DOV against copyright evasion attacks (CEA) remains unexplored. In this paper, we investigate how adversaries can circumvent these mechanisms, enabling models trained on watermarked datasets to bypass ownership verification. We begin by analyzing the limitations of potential attacks achieved by backdoor removal, including TPD and T2IShield. In practice, TPD suffers from inconsistent effectiveness due to randomness, while T2IShield fails when watermarks are embedded as local image patches. To this end, we introduce CEAT2I, the first CEA specifically targeting DOV in T2I diffusion models. CEAT2I consists of three stages: (1) motivated by the observation that T2I models converge faster on watermarked samples with respect to intermediate features rather than training loss, we reliably detect watermarked samples; (2) we iteratively ablate tokens from the prompts of detected samples and monitor feature shifts to identify trigger tokens; and (3) we apply a closed-form concept erasure method to remove the injected watermarks. Extensive experiments demonstrate that CEAT2I effectively evades state-of-the-art DOV mechanisms while preserving model performance. The code is available at https://github.com/csyufei/CEAT2I.\nlink: https://arxiv.org/abs/2505.02824v2\n"}}
{"custom_id": "2512.20423v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Evasion-Resilient Detection of DNS-over-HTTPS Data Exfiltration: A Practical Evaluation and Toolkit\nsummary: The purpose of this project is to assess how well defenders can detect DNS-over-HTTPS (DoH) file exfiltration, and which evasion strategies can be used by attackers. While providing a reproducible toolkit to generate, intercept and analyze DoH exfiltration, and comparing Machine Learning vs threshold-based detection under adversarial scenarios. The originality of this project is the introduction of an end-to-end, containerized pipeline that generates configurable file exfiltration over DoH using several parameters (e.g., chunking, encoding, padding, resolver rotation). It allows for file reconstruction at the resolver side, while extracting flow-level features using a fork of DoHLyzer. The pipeline contains a prediction side, which allows the training of machine learning models based on public labelled datasets and then evaluates them side-by-side with threshold-based detection methods against malicious and evasive DNS-Over-HTTPS traffic. We train Random Forest, Gradient Boosting and Logistic Regression classifiers on a public DoH dataset and benchmark them against evasive DoH exfiltration scenarios. The toolkit orchestrates traffic generation, file capture, feature extraction, model training and analysis. The toolkit is then encapsulated into several Docker containers for easy setup and full reproducibility regardless of the platform it is run on. Future research regarding this project is directed at validating the results on mixed enterprise traffic, extending the protocol coverage to HTTP/3/QUIC request, adding a benign traffic generation, and working on real-time traffic evaluation. A key objective is to quantify when stealth constraints make DoH exfiltration uneconomical and unworthy for the attacker.\nlink: https://arxiv.org/abs/2512.20423v1\n"}}
{"custom_id": "2512.20405v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: ChatGPT: Excellent Paper! Accept It. Editor: Imposter Found! Review Rejected\nsummary: Large Language Models (LLMs) like ChatGPT are now widely used in writing and reviewing scientific papers. While this trend accelerates publication growth and reduces human workload, it also introduces serious risks. Papers written or reviewed by LLMs may lack real novelty, contain fabricated or biased results, or mislead downstream research that others depend on. Such issues can damage reputations, waste resources, and even endanger lives when flawed studies influence medical or safety-critical systems. This research explores both the offensive and defensive sides of this growing threat. On the attack side, we demonstrate how an author can inject hidden prompts inside a PDF that secretly guide or \"jailbreak\" LLM reviewers into giving overly positive feedback and biased acceptance. On the defense side, we propose an \"inject-and-detect\" strategy for editors, where invisible trigger prompts are embedded into papers; if a review repeats or reacts to these triggers, it reveals that the review was generated by an LLM, not a human. This method turns prompt injections from vulnerability into a verification tool. We outline our design, expected model behaviors, and ethical safeguards for deployment. The goal is to expose how fragile today's peer-review process becomes under LLM influence and how editorial awareness can help restore trust in scientific evaluation.\nlink: https://arxiv.org/abs/2512.20405v1\n"}}
{"custom_id": "2512.20402v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: iblock: Accurate and Scalable Bitcoin Simulations with OMNeT++\nsummary: This paper proposes iblock, a comprehensive C++ library for Bitcoin simulation, designed for OMNeT++. iblock offers superior efficiency and scalability with respect to state-of-the-art simulators, which are typically written in high-level languages. Moreover, the possible integration with other OMNeT++ libraries allows highly detailed simulations. We measure iblock's performance against a state-of-the-art blockchain simulator, proving that it is more efficient at the same level of simulation detail. We also validate iblock by using it to simulate different scenarios such as the normal Bitcoin operation and the selfish mine attack, showing that simulation results are coherent with theoretical expectations.\nlink: https://arxiv.org/abs/2512.20402v1\n"}}
{"custom_id": "2512.20396v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Symmaries: Automatic Inference of Formal Security Summaries for Java Programs\nsummary: We introduce a scalable, modular, and sound approach for automatically constructing formal security specifications for Java bytecode programs in the form of method summaries. A summary provides an abstract representation of a method's security behavior, consisting of the conditions under which the method can be securely invoked, together with specifications of information flows and aliasing updates. Such summaries can be consumed by static code analysis tools and also help developers understand the behavior of code segments, such as libraries, in order to evaluate their security implications when reused in applications. Our approach is implemented in a tool called Symmaries, which automates the generation of security summaries. We applied Symmaries to Java API libraries to extract their security specifications and to large real-world applications to evaluate its scalability. Our results show that the tool successfully scales to analyze applications with hundreds of thousands of lines of code, and that Symmaries achieves a promising precision depending on the heap model used. We prove the soundness of our approach in terms of guaranteeing termination-insensitive non-interference.\nlink: https://arxiv.org/abs/2512.20396v1\n"}}
{"custom_id": "2512.20394v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Resilient Packet Forwarding: A Reinforcement Learning Approach to Routing in Gaussian Interconnected Networks with Clustered Faults\nsummary: As Network-on-Chip (NoC) and Wireless Sensor Network architectures continue to scale, the topology of the underlying network becomes a critical factor in performance. Gaussian Interconnected Networks based on the arithmetic of Gaussian integers, offer attractive properties regarding diameter and symmetry. Despite their attractive theoretical properties, adaptive routing techniques in these networks are vulnerable to node and link faults, leading to rapid degradation in communication reliability. Node failures (particularly those following Gaussian distributions, such as thermal hotspots or physical damage clusters) pose severe challenges to traditional deterministic routing. This paper proposes a fault-aware Reinforcement Learning (RL) routing scheme tailored for Gaussian Interconnected Networks. By utilizing a PPO (Proximal Policy Optimization) agent with a specific reward structure designed to penalize fault proximity, the system dynamically learns to bypass faulty regions. We compare our proposed RL-based routing protocol against a greedy adaptive shortest-path routing algorithm. Experimental results demonstrate that the RL agent significantly outperforms the adaptive routing sustaining a Packet Delivery Ratio (PDR) of 0.95 at 40% fault density compared to 0.66 for the greedy. Furthermore, the RL approach exhibits effective delivery rates compared to the greedy adaptive routing, particularly under low network load of 20% at 0.57 vs. 0.43, showing greater proficiency in managing congestion, validating its efficacy in stochastic, fault-prone topologies\nlink: https://arxiv.org/abs/2512.20394v1\n"}}
{"custom_id": "2512.20363v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning\nsummary: Federated learning (FL) supports privacy-preserving, decentralized machine learning (ML) model training by keeping data on client devices. However, non-independent and identically distributed (non-IID) data across clients biases updates and degrades performance. To alleviate these issues, we propose Clust-PSI-PFL, a clustering-based personalized FL framework that uses the Population Stability Index (PSI) to quantify the level of non-IID data. We compute a weighted PSI metric, $WPSI^L$, which we show to be more informative than common non-IID metrics (Hellinger, Jensen-Shannon, and Earth Mover's distance). Using PSI features, we form distributionally homogeneous groups of clients via K-means++; the number of optimal clusters is chosen by a systematic silhouette-based procedure, typically yielding few clusters with modest overhead. Across six datasets (tabular, image, and text modalities), two partition protocols (Dirichlet with parameter $\u03b1$ and Similarity with parameter S), and multiple client sizes, Clust-PSI-PFL delivers up to 18% higher global accuracy than state-of-the-art baselines and markedly improves client fairness by a relative improvement of 37% under severe non-IID data. These results establish PSI-guided clustering as a principled, lightweight mechanism for robust PFL under label skew.\nlink: https://arxiv.org/abs/2512.20363v1\n"}}
{"custom_id": "2512.20323v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Differentially Private Feature Release for Wireless Sensing: Adaptive Privacy Budget Allocation on CSI Spectrograms\nsummary: Wi-Fi/RF-based human sensing has achieved remarkable progress with deep learning, yet practical deployments increasingly require feature sharing for cloud analytics, collaborative training, or benchmark evaluation. Releasing intermediate representations such as CSI spectrograms can inadvertently expose sensitive information, including user identity, location, and membership, motivating formal privacy guarantees. In this paper, we study differentially private (DP) feature release for wireless sensing and propose an adaptive privacy budget allocation mechanism tailored to the highly non-uniform structure of CSI time-frequency representations. Our pipeline converts CSI to bounded spectrogram features, applies sensitivity control via clipping, estimates task-relevant importance over the time-frequency plane, and allocates a global privacy budget across spectrogram blocks before injecting calibrated Gaussian noise. Experiments on multi-user activity sensing (WiMANS), multi-person 3D pose estimation (Person-in-WiFi 3D), and respiration monitoring (Resp-CSI) show that adaptive allocation consistently improves the privacy-utility frontier over uniform perturbation under the same privacy budget. Our method yields higher accuracy and lower error while substantially reducing empirical leakage in identity and membership inference attacks.\nlink: https://arxiv.org/abs/2512.20323v1\n"}}
{"custom_id": "2512.10652v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: TriDF: Evaluating Perception, Detection, and Hallucination for Interpretable DeepFake Detection\nsummary: Advances in generative modeling have made it increasingly easy to fabricate realistic portrayals of individuals, creating serious risks for security, communication, and public trust. Detecting such person-driven manipulations requires systems that not only distinguish altered content from authentic media but also provide clear and reliable reasoning. In this paper, we introduce TriDF, a comprehensive benchmark for interpretable DeepFake detection. TriDF contains high-quality forgeries from advanced synthesis models, covering 16 DeepFake types across image, video, and audio modalities. The benchmark evaluates three key aspects: Perception, which measures the ability of a model to identify fine-grained manipulation artifacts using human-annotated evidence; Detection, which assesses classification performance across diverse forgery families and generators; and Hallucination, which quantifies the reliability of model-generated explanations. Experiments on state-of-the-art multimodal large language models show that accurate perception is essential for reliable detection, but hallucination can severely disrupt decision-making, revealing the interdependence of these three aspects. TriDF provides a unified framework for understanding the interaction between detection accuracy, evidence identification, and explanation reliability, offering a foundation for building trustworthy systems that address real-world synthetic media threats.\nlink: https://arxiv.org/abs/2512.10652v2\n"}}
{"custom_id": "2512.20303v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: From the Two-Capacitor Paradox to Electromagnetic Side-Channel Mitigation in Digital Circuits\nsummary: The classical two-capacitor paradox of the lost energy is revisited from an electronic circuit security stand-point. The paradox has been solved previously by various researchers, and the energy lost during the charging of capacitors has been primarily attributed to the heat and radiation. We analytically prove this for various standard resistor-capacitor (RC) and resistor-inductor-capacitor (RLC) circuit models. From the perspective of electronic system security, electromagnetic (EM) side-channel analysis (SCA) has recently gained significant prominence with the growth of resource-constrained, internet connected devices. This article connects the energy lost due to capacitor charging to the EM SCA leakage in electronic devices, leading to the recovery of the secret encryption key embedded within the device. Finally, with an understanding of how lost energy relates to EM radiation, we propose adiabatic charging as a solution to minimize EM leakage, thereby paving the way towards low-overhead EM SCA resilience.\nlink: https://arxiv.org/abs/2512.20303v1\n"}}
{"custom_id": "2512.20243v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Post-Quantum Cryptography in the 5G Core\nsummary: In this work, the conventional cryptographic algorithms used in the 5G Core are replaced with post-quantum alternatives and the practical impact of this transition is evaluated. Using a simulation environment, we model the registration and deregistration of varying numbers of user equipments (UEs) and measure the resulting effects on bandwidth consumption and latency.\n  Our results show that the deployment of post-quantum cryptographic algorithms has a measurable effect on performance, but that this effect is small, and perhaps more crucially, that the extra overhead needed in terms of computation and bandwidth does not have any substantial impact on the usability of the network and the efficiency of its network functions.\n  Overall the experimental results in this work corroborate earlier research: the 5G Core is technically able to support post-quantum cryptography without any inherent issues connected to the increased computational overhead or larger message size.\nlink: https://arxiv.org/abs/2512.20243v1\n"}}
{"custom_id": "2512.20234v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Achieving Flexible and Secure Authentication with Strong Privacy in Decentralized Networks\nsummary: Anonymous credentials (ACs) are a crucial cryptographic tool for privacy-preserving authentication in decentralized networks, allowing holders to prove eligibility without revealing their identity. However, a major limitation of standard ACs is the disclosure of the issuer's identity, which can leak sensitive contextual information about the holder. Issuer-hiding ACs address this by making a credential's origin indistinguishable among a set of approved issuers. Despite this advancement, existing solutions suffer from practical limitations that hinder their deployment in decentralized environments: unflexible credential models that restrict issuer and holder autonomy, flawed revocation mechanisms that compromise security, and weak attribute hiding that fails to meet data minimization principles. This paper introduces a new scheme called IRAC to overcome these challenges. We propose a flexible credential model that employs vector commitments with a padding strategy to unify credentials from heterogeneous issuers, enabling privacy-preserving authentication without enforcing a global static attribute set or verifier-defined policies. Furthermore, we design a secure decentralized revocation mechanism where holders prove non-revocation by demonstrating their credential's hash lies within a gap in the issuer's sorted revocation list, effectively decoupling revocation checks from verifier policies while maintaining issuer anonymity. IRAC also strengthens attribute hiding by utilizing zk-SNARKs and vector commitments, allowing holders to prove statements about their attributes without disclosing the attributes themselves or the credential structure. Security analysis and performance evaluations demonstrate its practical feasibility for decentralized networks, where presenting a credential can be finished in 1s.\nlink: https://arxiv.org/abs/2512.20234v1\n"}}
{"custom_id": "2512.20210v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs\nsummary: The serverless computing paradigm offers compelling advantages for deploying Large Language Model (LLM) inference services, including elastic scaling and pay-per-use billing. However, serving multiple fine-tuned LLMs via Low-Rank Adaptation (LoRA) in serverless environments faces critical challenges: reactive adapter loading causes significant cold start latency, and frequent adapter swapping leads to severe GPU memory fragmentation. In this paper, we present Predictive-LoRA (P-LoRA), a proactive and fragmentation-aware serverless inference system for LoRA-based LLMs. P-LoRA introduces two key innovations: (1) a lightweight LSTM-based traffic predictor that forecasts adapter demand and proactively prefetches hot adapters from host memory to GPU, reducing cold start latency by up to 68%; and (2) a page-based adapter memory management mechanism inspired by operating system virtual memory, which keeps GPU memory utilization above 87% even under heterogeneous adapter ranks. We evaluate P-LoRA using production-like workloads derived from the Azure Functions trace. Experimental results demonstrate that P-LoRA achieves 1.52x higher throughput than S-LoRA while reducing the average Time-To-First-Token (TTFT) by 35% under high concurrency scenarios.\nlink: https://arxiv.org/abs/2512.20210v1\n"}}
{"custom_id": "2511.16193v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Fast LLM Post-training via Decoupled and Fastest-of-N Speculation\nsummary: Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. This work, SpecActor, achieves fast rollout with speculative decoding that deploys a fast draft path to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges that hinder speculation efficiency: (1) a Decoupled speculation method that overcomes the computation inefficiency issue when executing speculative decoding with relative large per-worker batch size -- a common configuration in training but unfriendly to speculation, and (2) a Fastest-of-N speculation method that selects and combines different draft methods according to the rollout progress to approximate the optimal draft method even when the best one is unknown a priori. Extensive evaluations on production traces show that SpecActor accelerates mean rollout speed by 2.0--2.4x, with up to 2.7x speedup, over common post-training baselines. The results are consistent across both dense and MoE models and across different RL algorithms. Notably, SpecActor is 1.1--2.6x faster compared to vanilla speculative rollout in different traces. The accelerated rollout achieves 1.4--2.3x faster end-to-end training time.\nlink: https://arxiv.org/abs/2511.16193v3\n"}}
{"custom_id": "2512.20184v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Reaching Agreement Among Reasoning LLM Agents\nsummary: Multi-agent systems have extended the capability of agentic AI. Instead of single inference passes, multiple agents perform collective reasoning to derive high quality answers. However, existing multi-agent orchestration relies on static heuristic workflows such as fixed loop limits and barrier synchronization. These ad-hoc approaches waste computational resources, incur high latency due to stragglers, and risk finalizing transient agreements. We argue that reliable multi-agent reasoning requires a formal foundation analogous to classical distributed consensus problem.\n  To that end, we propose a formal model of the multi-agent refinement problem. The model includes definitions of the correctness guarantees and formal semantics of agent reasoning. We then introduce Aegean, a consensus protocol designed for stochastic reasoning agents that solves multi-agent refinement. We implement the protocol in Aegean-Serve, a consensus-aware serving engine that performs incremental quorum detection across concurrent agent executions, enabling early termination when sufficient agents converge. Evaluation using four mathematical reasoning benchmarks shows that Aegean provides provable safety and liveness guarantees while reducing latency by 1.2--20$\\times$ compared to state-of-the-art baselines, maintaining answer quality within 2.5%. Consistent gains across both local GPU deployments and commercial API providers validate that consensus-based orchestration eliminates straggler delays without sacrificing correctness.\nlink: https://arxiv.org/abs/2512.20184v1\n"}}
{"custom_id": "2508.06244v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Membership Inference Attack with Partial Features\nsummary: Machine learning models are vulnerable to membership inference attack, which can be used to determine whether a given sample appears in the training data. Most existing methods assume the attacker has full access to the features of the target sample. This assumption, however, does not hold in many real-world scenarios where only partial features are available, thereby limiting the applicability of these methods. In this work, we introduce Partial Feature Membership Inference (PFMI), a scenario where the adversary observes only partial features of each sample and aims to infer whether this observed subset was present in the training set. To address this problem, we propose MRAD (Memory-guided Reconstruction and Anomaly Detection), a two-stage attack framework that works in both white-box and black-box settings. In the first stage, MRAD leverages the latent memory of the target model to reconstruct the unknown features of the sample. We observe that when the known features are absent from the training set, the reconstructed sample deviates significantly from the true data distribution. Consequently, in the second stage, we use anomaly detection algorithms to measure the deviation between the reconstructed sample and the training data distribution, thereby determining whether the known features belong to a member of the training set. Empirical results demonstrate that MRAD is effective across various datasets, and maintains compatibility with off-the-shelf anomaly detection techniques. For example, on STL-10, our attack exceeds an AUC of around 0.75 even with 60% of the missing features.\nlink: https://arxiv.org/abs/2508.06244v2\n"}}
{"custom_id": "2512.20178v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication\nsummary: Distributed Sparse Matrix-Matrix Multiplication (SpMM) is a fundamental operation in numerous high-performance computing and deep learning applications. The major performance bottleneck in distributed SpMM lies in the substantial communication overhead, which limits both performance and scalability. In this paper, we identify and analyze sources of inefficient communication in existing distributed SpMM implementations at two levels and address these inefficiencies by proposing: (1) a fine-grained, sparsity-aware communication strategy that reduces communication overhead by exploiting the sparsity pattern of the sparse matrix, and (2) a hierarchical communication strategy that integrates the sparsity-aware strategy with the common two-tier network architectures in GPU-accelerated systems, to reduce redundant communication across slow network links. We implement these optimizations in a comprehensive distributed SpMM framework, \\method{}. Extensive evaluations on real-world datasets show that our framework demonstrates strong scalability up to 128 GPUs, achieving geometric mean speedups of 221.5$\\times$, 56.0$\\times$, 23.4$\\times$, and 8.8$\\times$ over four state-of-the-art baselines (CAGNET, SPA, BCL, and CoLa, respectively) at this scale.\nlink: https://arxiv.org/abs/2512.20178v1\n"}}
{"custom_id": "2512.20176v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Optimistic TEE-Rollups: A Hybrid Architecture for Scalable and Verifiable Generative AI Inference on Blockchain\nsummary: The rapid integration of Large Language Models (LLMs) into decentralized physical infrastructure networks (DePIN) is currently bottlenecked by the Verifiability Trilemma, which posits that a decentralized inference system cannot simultaneously achieve high computational integrity, low latency, and low cost. Existing cryptographic solutions, such as Zero-Knowledge Machine Learning (ZKML), suffer from superlinear proving overheads (O(k NlogN)) that render them infeasible for billionparameter models. Conversely, optimistic approaches (opML) impose prohibitive dispute windows, preventing real-time interactivity, while recent \"Proof of Quality\" (PoQ) paradigms sacrifice cryptographic integrity for subjective semantic evaluation, leaving networks vulnerable to model downgrade attacks and reward hacking. In this paper, we introduce Optimistic TEE-Rollups (OTR), a hybrid verification protocol that harmonizes these constraints. OTR leverages NVIDIA H100 Confidential Computing Trusted Execution Environments (TEEs) to provide sub-second Provisional Finality, underpinned by an optimistic fraud-proof mechanism and stochastic Zero-Knowledge spot-checks to mitigate hardware side-channel risks. We formally define Proof of Efficient Attribution (PoEA), a consensus mechanism that cryptographically binds execution traces to hardware attestations, thereby guaranteeing model authenticity. Extensive simulations demonstrate that OTR achieves 99% of the throughput of centralized baselines with a marginal cost overhead of $0.07 per query, maintaining Byzantine fault tolerance against rational adversaries even in the presence of transient hardware vulnerabilities.\nlink: https://arxiv.org/abs/2512.20176v1\n"}}
{"custom_id": "2512.20168v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography\nsummary: By integrating language understanding with perceptual modalities such as images, multimodal large language models (MLLMs) constitute a critical substrate for modern AI systems, particularly intelligent agents operating in open and interactive environments. However, their increasing accessibility also raises heightened risks of misuse, such as generating harmful or unsafe content. To mitigate these risks, alignment techniques are commonly applied to align model behavior with human values. Despite these efforts, recent studies have shown that jailbreak attacks can circumvent alignment and elicit unsafe outputs. Currently, most existing jailbreak methods are tailored for open-source models and exhibit limited effectiveness against commercial MLLM-integrated systems, which often employ additional filters. These filters can detect and prevent malicious input and output content, significantly reducing jailbreak threats. In this paper, we reveal that the success of these safety filters heavily relies on a critical assumption that malicious content must be explicitly visible in either the input or the output. This assumption, while often valid for traditional LLM-integrated systems, breaks down in MLLM-integrated systems, where attackers can leverage multiple modalities to conceal adversarial intent, leading to a false sense of security in existing MLLM-integrated systems. To challenge this assumption, we propose Odysseus, a novel jailbreak paradigm that introduces dual steganography to covertly embed malicious queries and responses into benign-looking images. Extensive experiments on benchmark datasets demonstrate that our Odysseus successfully jailbreaks several pioneering and realistic MLLM-integrated systems, achieving up to 99% attack success rate. It exposes a fundamental blind spot in existing defenses, and calls for rethinking cross-modal security in MLLM-integrated systems.\nlink: https://arxiv.org/abs/2512.20168v1\n"}}
{"custom_id": "2512.20163v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Population Protocols Revisited: Parity and Beyond\nsummary: For nearly two decades, population protocols have been extensively studied, yielding efficient solutions for central problems in distributed computing, including leader election, and majority computation, a predicate type in Presburger Arithmetic closely tied to population protocols. Surprisingly, no protocols have achieved both time- and space-efficiency for congruency predicates, such as parity computation, which are complementary in this arithmetic framework. This gap highlights a significant challenge in the field. To address this gap, we explore the parity problem, where agents are tasked with computing the parity of the given sub-population size. Then we extend the solution for parity to compute congruences modulo an arbitrary $m$.\n  Previous research on efficient population protocols has focused on protocols that minimise both stabilisation time and state utilisation for specific problems. In contrast, this work slightly relaxes this expectation, permitting protocols to place less emphasis on full optimisation and more on universality, robustness, and probabilistic guarantees. This allows us to propose a novel computing paradigm that integrates population weights (or simply weights), a robust clocking mechanism, and efficient anomaly detection coupled with a switching mechanism (which ensures slow but always correct solutions). This paradigm facilitates universal design of efficient multistage stable population protocols. Specifically, the first efficient parity and congruence protocols introduced here use both $O(\\log^3 n)$ states and achieve silent stabilisation in $O(\\log^3 n)$ time. We conclude by discussing the impact of implicit conversion between unary and binary representations enabled by the weight system, with applications to other problems, including the computation and representation of (sub-)population sizes.\nlink: https://arxiv.org/abs/2512.20163v1\n"}}
{"custom_id": "2506.05594v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: SoK: Are Watermarks in LLMs Ready for Deployment?\nsummary: Large Language Models (LLMs) have transformed natural language processing, demonstrating impressive capabilities across diverse tasks. However, deploying these models introduces critical risks related to intellectual property violations and potential misuse, particularly as adversaries can imitate these models to steal services or generate misleading outputs. We specifically focus on model stealing attacks, as they are highly relevant to proprietary LLMs and pose a serious threat to their security, revenue, and ethical deployment. While various watermarking techniques have emerged to mitigate these risks, it remains unclear how far the community and industry have progressed in developing and deploying watermarks in LLMs.\n  To bridge this gap, we aim to develop a comprehensive systematization for watermarks in LLMs by 1) presenting a detailed taxonomy for watermarks in LLMs, 2) proposing a novel intellectual property classifier to explore the effectiveness and impacts of watermarks on LLMs under both attack and attack-free environments, 3) analyzing the limitations of existing watermarks in LLMs, and 4) discussing practical challenges and potential future directions for watermarks in LLMs. Through extensive experiments, we show that despite promising research outcomes and significant attention from leading companies and community to deploy watermarks, these techniques have yet to reach their full potential in real-world applications due to their unfavorable impacts on model utility of LLMs and downstream tasks. Our findings provide an insightful understanding of watermarks in LLMs, highlighting the need for practical watermarks solutions tailored to LLM deployment.\nlink: https://arxiv.org/abs/2506.05594v3\n"}}
{"custom_id": "2512.20077v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Fault Injection Attacks on Machine Learning-based Quantum Computer Readout Error Correction\nsummary: Machine-learning (ML) classifiers are increasingly used in quantum computing systems to improve multi-qubit readout discrimination and to mitigate correlated readout errors. These ML classifiers are an integral component of today's quantum computer's control and readout stacks. This paper is the first to analyze the susceptibility of such ML classifiers to physical fault-injection which can result in generation of incorrect readout results from quantum computers. The study targets 5-qubit (thus 32-class) readout error-correction model. Using the ChipWhisperer Husky for physical voltage glitching, this work leverages an automated algorithm for scanning the fault injection parameter search space to find various successful faults in all the layers of the target ML model. Across repeated trials, this work finds that fault susceptibility is strongly layer-dependent: early-layers demonstrate higher rates of misprediction when faults are triggered in them, whereas later layers have smaller misprediction rates. This work further characterizes the resulting readout failures at the bitstring level using Hamming-distance and per-bit flip statistics, showing that single-shot glitches can induce structured readout corruption rather than purely random noise. These results motivate treating ML-based quantum computer readout and readout correction as a security-critical component of quantum systems and highlight the need for lightweight, deployment-friendly fault detection and redundancy mechanisms in the quantum computer readout pipelines.\nlink: https://arxiv.org/abs/2512.20077v1\n"}}
{"custom_id": "2503.22161v4", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: AI-based Traffic Modeling for Network Security and Privacy: Challenges Ahead\nsummary: Network traffic analysis using AI (machine learning and deep learning) models made significant progress over the past decades. Traffic analysis addresses various challenging problems in network security, ranging from detection of anomalies and attacks to countering of Internet censorship. AI models are also developed to expose user privacy risks as demonstrated by the research works on fingerprinting of user-visiting websites, IoT devices, and different applications, even when payloads are encrypted.\n  Despite these advancements, significant challenges remain in the domain of network traffic analysis to effectively secure our networks from evolving threats and attacks. After briefly reviewing the relevant tasks and recent AI models for traffic analysis, we discuss the challenges that lie ahead.\nlink: https://arxiv.org/abs/2503.22161v4\n"}}
{"custom_id": "2512.20064v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: FastMPS: Revisit Data Parallel in Large-scale Matrix Product State Sampling\nsummary: Matrix Product State (MPS) is a versatile tensor network representation widely applied in quantum physics, quantum chemistry, and machine learning, etc. MPS sampling serves as a critical fundamental operation in these fields. As the problems become more complex, the scale of MPS is rapidly increasing. Traditional data parallelism is limited by memory and heavy I/O in large-scale MPS. Model parallelism that can handle large-scale MPS imposes rigid process bindings and lacks scalability. This work proposes Fast-MPS, a multi-level parallel framework for scalable MPS sampling. Our design combines data parallelism across samples with tensor parallelism along bond dimensions. We eliminate memory and I/O pressure through compression and overlapping, and revive data parallel in large-scale MPS sampling. We evaluate our approach on Gaussian Boson Sampling, a representative and demanding application. Fast-MPS achieves over 10x speedup compared to existing simulators, scales to thousands of processes, and enables simulations with 8,176 sites and bond dimension chi = 10^4, significantly outperforming the state of the art. Fast-MPS has demonstrated great potential in high-performance tensor network applications.\nlink: https://arxiv.org/abs/2512.20064v1\n"}}
{"custom_id": "2512.20062v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: On the Effectiveness of Instruction-Tuning Local LLMs for Identifying Software Vulnerabilities\nsummary: Large Language Models (LLMs) show significant promise in automating software vulnerability analysis, a critical task given the impact of security failure of modern software systems. However, current approaches in using LLMs to automate vulnerability analysis mostly rely on using online API-based LLM services, requiring the user to disclose the source code in development. Moreover, they predominantly frame the task as a binary classification(vulnerable or not vulnerable), limiting potential practical utility. This paper addresses these limitations by reformulating the problem as Software Vulnerability Identification (SVI), where LLMs are asked to output the type of weakness in Common Weakness Enumeration (CWE) IDs rather than simply indicating the presence or absence of a vulnerability. We also tackle the reliance on large, API-based LLMs by demonstrating that instruction-tuning smaller, locally deployable LLMs can achieve superior identification performance. In our analysis, instruct-tuning a local LLM showed better overall performance and cost trade-off than online API-based LLMs. Our findings indicate that instruct-tuned local models represent a more effective, secure, and practical approach for leveraging LLMs in real-world vulnerability management workflows.\nlink: https://arxiv.org/abs/2512.20062v1\n"}}
{"custom_id": "2508.02115v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Collision-based Watermark for Detecting Backdoor Manipulation in Federated Learning\nsummary: As AI-generated content increasingly underpins real-world applications, its accompanying security risks, including privacy leakage and copyright infringement, have become growing concerns. In this context, Federated Learning (FL) offers a promising foundation for enhancing trustworthiness by enabling privacy-preserving collaborative learning over proprietary data. However, its practical adoption is critically threatened by backdoor-based model manipulation, where a small number of malicious clients can compromise the system and induce harmful content generation and decision-making. Although various detection methods have been proposed to detect such manipulation, we reveal that they are either disrupted by non-i.i.d. data distributions and random client participation, or misled by out-of-distribution (OOD) prediction bias, both of which are unique challenges in FL scenarios. To address these issues, we introduce a novel proactive detection method dubbed Coward, inspired by our discovery of multi-backdoor collision effects, in which consecutively planted, distinct backdoors significantly suppress earlier ones. Correspondingly, we modify the federated global model by injecting a carefully designed backdoor-collided watermark, implemented via regulated dual-mapping learning on OOD data. This design not only enables an inverted detection paradigm compared to existing proactive methods, thereby naturally counteracting the adverse impact of OOD prediction bias, but also introduces a low-disruptive training intervention that inherently limits the strength of OOD bias, leading to significantly fewer misjudgments. Extensive experiments on benchmark datasets show that Coward achieves state-of-the-art detection performance, effectively alleviates OOD prediction bias, and remains robust against potential adaptive manipulations.\nlink: https://arxiv.org/abs/2508.02115v2\n"}}
{"custom_id": "2512.19025v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation\nsummary: Machine unlearning aims to remove specific data influences from trained models, a capability essential for adhering to copyright laws and ensuring AI safety. Current unlearning metrics typically measure success by monitoring the model's performance degradation on the specific unlearning dataset ($D_u$). We argue that for Large Language Models (LLMs), this evaluation paradigm is insufficient and potentially misleading. Many real-world uses of unlearning--motivated by copyright or safety--implicitly target not only verbatim content in $D_u$, but also behaviors influenced by the broader generalizations the model derived from it. We demonstrate that LLMs can pass standard unlearning evaluation and appear to have \"forgotten\" the target knowledge, while simultaneously retaining strong capabilities on content that is semantically adjacent to $D_u$. This phenomenon indicates that erasing exact sentences does not necessarily equate to removing the underlying knowledge. To address this gap, we propose Proximal Surrogate Generation (PSG), an automated stress-testing framework that generates a surrogate dataset, $\\tilde{D}_u$. This surrogate set is constructed to be semantically derived from $D_u$ yet sufficiently distinct in embedding space. By comparing unlearning metric scores between $D_u$ and $\\tilde{D}_u$, we can stress-test the reliability of the metric itself. Our extensive evaluation across three LLM families (Llama-3-8B, Qwen2.5-7B, and Zephyr-7B-$\u03b2$), three distinct datasets, and seven standard metrics reveals widespread inconsistencies. We find that current metrics frequently overestimate unlearning success, failing to detect retained knowledge exposed by our stress-test datasets.\nlink: https://arxiv.org/abs/2512.19025v2\n"}}
{"custom_id": "2512.20017v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Scaling Point-based Differentiable Rendering for Large-scale Reconstruction\nsummary: Point-based Differentiable Rendering (PBDR) enables high-fidelity 3D scene reconstruction, but scaling PBDR to high-resolution and large scenes requires efficient distributed training systems. Existing systems are tightly coupled to a specific PBDR method. And they suffer from severe communication overhead due to poor data locality. In this paper, we present Gaian, a general distributed training system for PBDR. Gaian provides a unified API expressive enough to support existing PBDR methods, while exposing rich data-access information, which Gaian leverages to optimize locality and reduce communication. We evaluated Gaian by implementing 4 PBDR algorithms. Our implementations achieve high performance and resource efficiency: across six datasets and up to 128 GPUs, it reduces communication by up to 91% and improves training throughput by 1.50x-3.71x.\nlink: https://arxiv.org/abs/2512.20017v1\n"}}
{"custom_id": "2505.09343v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures\nsummary: The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model co-design can effectively address these challenges, enabling cost-efficient training and inference at scale. This paper presents an in-depth analysis of the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting key innovations such as Multi-head Latent Attention (MLA) for enhanced memory efficiency, Mixture of Experts (MoE) architectures for optimized computation-communication trade-offs, FP8 mixed-precision training to unlock the full potential of hardware capabilities, and a Multi-Plane Network Topology to minimize cluster-level network overhead. Building on the hardware bottlenecks encountered during DeepSeek-V3's development, we engage in a broader discussion with academic and industry peers on potential future hardware directions, including precise low-precision computation units, scale-up and scale-out convergence, and innovations in low-latency communication fabrics. These insights underscore the critical role of hardware and model co-design in meeting the escalating demands of AI workloads, offering a practical blueprint for innovation in next-generation AI systems.\nlink: https://arxiv.org/abs/2505.09343v2\n"}}
{"custom_id": "2512.20004v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense\nsummary: Since the Internet of Things (IoT) is widely adopted using Android applications, detecting malicious Android apps is essential. In recent years, Android graph-based deep learning research has proposed many approaches to extract relationships from applications as graphs to generate graph embeddings. First, we demonstrate the effectiveness of graph-based classification using a Graph Neural Network (GNN)-based classifier to generate API graph embeddings. The graph embeddings are combined with Permission and Intent features to train multiple machine learning and deep learning models for Android malware detection. The proposed classification approach achieves an accuracy of 98.33 percent on the CICMaldroid dataset and 98.68 percent on the Drebin dataset. However, graph-based deep learning models are vulnerable, as attackers can add fake relationships to evade detection by the classifier. Second, we propose a Generative Adversarial Network (GAN)-based attack algorithm named VGAE-MalGAN targeting graph-based GNN Android malware classifiers. The VGAE-MalGAN generator produces adversarial malware API graphs, while the VGAE-MalGAN substitute detector attempts to mimic the target detector. Experimental results show that VGAE-MalGAN can significantly reduce the detection rate of GNN-based malware classifiers. Although the model initially fails to detect adversarial malware, retraining with generated adversarial samples improves robustness and helps mitigate adversarial attacks.\nlink: https://arxiv.org/abs/2512.20004v1\n"}}
{"custom_id": "2512.19997v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: BacAlarm: Mining and Simulating Composite API Traffic to Prevent Broken Access Control Violations\nsummary: Broken Access Control (BAC) violations, which consistently rank among the top five security risks in the OWASP API Security Top 10, refer to unauthorized access attempts arising from BAC vulnerabilities, whose successful exploitation can impose significant risks on exposed application programming interfaces (APIs). In recent years, learning-based methods have demonstrated promising prospects in detecting various types of malicious activities. However, in real-network operation and maintenance scenarios, leveraging learning-based methods for BAC detection faces two critical challenges. Firstly, under the RESTful API design principles, most systems omit recording composite traffic for performance, and together with ethical and legal bans on directly testing real-world systems, this leads to a critical shortage of training data for detecting BAC violations. Secondly, common malicious behaviors such as SQL injection typically generate individual access traffic that is inherently anomalous. In contrast, BAC is usually composed of multiple correlated access requests that appear normal when examined in isolation. To tackle these problems, we introduce \\BAC, an approach for establishing a BAC violation detection model by generating and utilizing API traffic data. The \\BAC consists of an API Traffic Generator and a BAC Detector. Experimental results show that \\BAC outperforms current state-of-the-art invariant-based and learning-based methods with the $\\text{F}_1$ and MCC improving by 21.2\\% and 24.1\\%.\nlink: https://arxiv.org/abs/2512.19997v1\n"}}
{"custom_id": "2512.19972v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions\nsummary: Collaborative learning has emerged as a key paradigm in large-scale intelligent systems, enabling distributed agents to cooperatively train their models while addressing their privacy concerns. Central to this paradigm is knowledge distillation (KD), a technique that facilitates efficient knowledge transfer among agents. However, the underlying mechanisms by which KD leverages memory and knowledge across agents remain underexplored. This paper aims to bridge this gap by offering a comprehensive review of KD in collaborative learning, with a focus on the roles of memory and knowledge. We define and categorize memory and knowledge within the KD process and explore their interrelationships, providing a clear understanding of how knowledge is extracted, stored, and shared in collaborative settings. We examine various collaborative learning patterns, including distributed, hierarchical, and decentralized structures, and provide insights into how memory and knowledge dynamics shape the effectiveness of KD in collaborative learning. Particularly, we emphasize task heterogeneity in distributed learning pattern covering federated learning (FL), multi-agent domain adaptation (MADA), federated multi-modal learning (FML), federated continual learning (FCL), federated multi-task learning (FMTL), and federated graph knowledge embedding (FKGE). Additionally, we highlight model heterogeneity, data heterogeneity, resource heterogeneity, and privacy concerns of these tasks. Our analysis categorizes existing work based on how they handle memory and knowledge. Finally, we discuss existing challenges and propose future directions for advancing KD techniques in the context of collaborative learning.\nlink: https://arxiv.org/abs/2512.19972v1\n"}}
{"custom_id": "2512.19968v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Fast Deterministically Safe Proof-of-Work Consensus\nsummary: Permissionless blockchains achieve consensus while allowing unknown nodes to join and leave the system at any time. They typically come in two flavors: proof of work (PoW) and proof of stake (PoS), and both are vulnerable to attacks. PoS protocols suffer from long-range attacks, wherein attackers alter execution history at little cost, and PoW protocols are vulnerable to attackers with enough computational power to subvert execution history. PoS protocols respond by relying on external mechanisms like social consensus; PoW protocols either fall back to probabilistic guarantees, or are slow.\n  We present Sieve-MMR, the first fully-permissionless protocol with deterministic security and constant expected latency that does not rely on external mechanisms. We obtain Sieve-MMR by porting a PoS protocol (MMR) to the PoW setting. From MMR we inherit constant expected latency and deterministic security, and proof-of-work gives us resilience against long-range attacks. The main challenge to porting MMR to the PoW setting is what we call time-travel attacks, where attackers use PoWs generated in the distant past to increase their perceived PoW power in the present. We respond by proposing Sieve, a novel algorithm that implements a new broadcast primitive we dub time-travel-resilient broadcast (TTRB). Sieve relies on a black-box, deterministic PoW primitive to implement TTRB, which we use as the messaging layer for MMR.\nlink: https://arxiv.org/abs/2512.19968v1\n"}}
{"custom_id": "2406.15596v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: DiVerify: Hardening Identity-Based Software Signing with Diverse-Context Scopes\nsummary: Identity-based code signing enables software developers to digitally sign their code using cryptographic keys. This key is then linked to an identity (e.g., through an identity provider), allowing signers to verify both the code's origin and integrity. However, this code-identity binding is only as trustworthy as the mechanisms enforcing it. State-of-the-art identity-based code signing schemes present a major shortcoming: these schemes fail to provide verifiable information about the context in which a signature is generated. This verifiability is crucial given that modern attackers have subverted long-established security assumptions, namely, that the identity provider ecosystem, as well as signing software itself, is trusted.\n  To address these issues, this paper introduces a diverse identity verification framework, DiVerify, that distributes identity-based verification across multiple entities and enforces stronger guarantees about the signing context. DiVerify makes it possible to provide end-to-end verifiability of not only a signer's identity (via multiple such signals), but also a signer's software stack (e.g., to verify no malware is present on a system at the time of signing). DiVerify is aimed at deployability, and leverages a meta-protocol to gather various trust signals and a binding mechanism to address the aforementioned, novel software supply chain attack vectors. We evaluate DiVerify's performance and confirm it is cheap to deploy and non-intrusive to developers: it only incurs a few kilobytes of additional storage (less than 0.4 percent of the average package size in widely used ecosystems like PyPI), and signing completes in under 100ms on a server-grade deployment.\nlink: https://arxiv.org/abs/2406.15596v3\n"}}
{"custom_id": "2512.19951v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Efficient Mod Approximation and Its Applications to CKKS Ciphertexts\nsummary: The mod function plays a critical role in numerous data encoding and cryptographic primitives. However, the widely used CKKS homomorphic encryption (HE) scheme supports only arithmetic operations, making it difficult to perform mod computations on encrypted data. Approximating the mod function with polynomials has therefore become an important yet challenging problem. The discontinuous and periodic characteristics of the mod function make it particularly difficult to approximate accurately under HE. Existing homomorphic mod constructions provide accurate results only within limited subranges of the input range, leaving the problem of achieving accurate approximation across the full input range unresolved. In this work, we propose a novel method based on polynomial interpolation and Chebyshev series to accurately approximate the mod function. Building upon this, we design two efficient data packing schemes, BitStack and CRTStack, tailored for small-integer inputs in CKKS. These schemes significantly improve the utilization of the CKKS plaintext space and enable efficient ciphertext uploads. Furthermore, we apply the proposed HE mod function to implement a homomorphic rounding operation and a general transformation from additive secret sharing to CKKS ciphertexts, achieving accurate ciphertext rounding and complete secret-share-to-CKKS conversion. Experimental results demonstrate that our approach achieves high approximation accuracy (up to 1e-8). Overall, our work provides a practical and general solution for performing mod operations under CKKS, extending its applicability to a broader range of privacy-preserving computations.\nlink: https://arxiv.org/abs/2512.19951v1\n"}}
{"custom_id": "2512.19945v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Energy-Efficient Multi-LLM Reasoning for Binary-Free Zero-Day Detection in IoT Firmware\nsummary: Securing Internet of Things (IoT) firmware remains difficult due to proprietary binaries, stripped symbols, heterogeneous architectures, and limited access to executable code. Existing analysis methods, such as static analysis, symbolic execution, and fuzzing, depend on binary visibility and functional emulation, making them unreliable when firmware is encrypted or inaccessible. To address this limitation, we propose a binary-free, architecture-agnostic solution that estimates the likelihood of conceptual zero-day vulnerabilities using only high-level descriptors. The approach integrates a tri-LLM reasoning architecture combining a LLaMA-based configuration interpreter, a DeepSeek-based structural abstraction analyzer, and a GPT-4o semantic fusion model. The solution also incorporates LLM computational signatures, including latency patterns, uncertainty markers, and reasoning depth indicators, as well as an energy-aware symbolic load model, to enhance interpretability and operational feasibility. In addition, we formally derive the mathematical foundations of the reasoning pipeline, establishing monotonicity, divergence, and energy-risk coupling properties that theoretically justify the model's behavior. Simulation-based evaluation reveals that high exposure conditions increase the predicted zero-day likelihood by 20 to 35 percent across models, with GPT-4o demonstrating the strongest cross-layer correlations and the highest sensitivity. Energy and divergence metrics significantly predict elevated risk (p < 0.01), reinforcing the effectiveness of the proposed reasoning framework.\nlink: https://arxiv.org/abs/2512.19945v1\n"}}
{"custom_id": "2512.19935v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress\nsummary: Machine learning models used in financial decision systems operate in nonstationary economic environments, yet adversarial robustness is typically evaluated under static assumptions. This work introduces Conditional Adversarial Fragility, a regime dependent phenomenon in which adversarial vulnerability is systematically amplified during periods of macroeconomic stress. We propose a regime aware evaluation framework for time indexed tabular financial classification tasks that conditions robustness assessment on external indicators of economic stress. Using volatility based regime segmentation as a proxy for macroeconomic conditions, we evaluate model behavior across calm and stress periods while holding model architecture, attack methodology, and evaluation protocols constant. Baseline predictive performance remains comparable across regimes, indicating that economic stress alone does not induce inherent performance degradation. Under adversarial perturbations, however, models operating during stress regimes exhibit substantially greater degradation across predictive accuracy, operational decision thresholds, and risk sensitive outcomes. We further demonstrate that this amplification propagates to increased false negative rates, elevating the risk of missed high risk cases during adverse conditions. To complement numerical robustness metrics, we introduce an interpretive governance layer based on semantic auditing of model explanations using large language models. Together, these results demonstrate that adversarial robustness in financial machine learning is a regime dependent property and motivate stress aware approaches to model risk assessment in high stakes financial deployments.\nlink: https://arxiv.org/abs/2512.19935v1\n"}}
{"custom_id": "2512.15503v2", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection\nsummary: Vehicular platooning promises transformative improvements in transportation efficiency and safety through the coordination of multi-vehicle formations enabled by Vehicle-to-Everything (V2X) communication. However, the distributed nature of platoon coordination creates security vulnerabilities, allowing authenticated vehicles to inject falsified kinematic data, compromise operational stability, and pose a threat to passenger safety. Traditional misbehaviour detection approaches, which rely on plausibility checks and statistical methods, suffer from high False Positive (FP) rates and cannot capture the complex temporal dependencies inherent in multi-vehicle coordination dynamics. We present Attention In Motion (AIMformer), a transformer-based framework specifically tailored for real-time misbehaviour detection in vehicular platoons with edge deployment capabilities. AIMformer leverages multi-head self-attention mechanisms to simultaneously capture intra-vehicle temporal dynamics and inter-vehicle spatial correlations. It incorporates global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers. We propose a Precision-Focused Binary Cross-Entropy (PFBCE) loss function that penalizes FPs to meet the requirements of safety-critical vehicular systems. Extensive evaluation across 4 platoon controllers, multiple attack vectors, and diverse mobility scenarios demonstrates superior performance ($\\geq$ 0.93) compared to state-of-the-art baseline architectures. A comprehensive deployment analysis utilizing TensorFlow Lite (TFLite), Open Neural Network Exchange (ONNX), and TensorRT achieves sub-millisecond inference latency, making it suitable for real-time operation on resource-constrained edge platforms. Hence, validating AIMformer is viable for both in-vehicle and roadside infrastructure deployment.\nlink: https://arxiv.org/abs/2512.15503v2\n"}}
{"custom_id": "2512.06253v4", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Privacy Loss of Noise Perturbation via Concentration Analysis of A Product Measure\nsummary: Noise perturbation is one of the most fundamental approaches for achieving $(\u03b5,\u03b4)$-differential privacy (DP) guarantees when releasing the result of a query or function $f(\\cdot)\\in\\mathbb{R}^M$ evaluated on a sensitive dataset $\\mathbf{x}$. In this approach, calibrated noise $\\mathbf{n}\\in\\mathbb{R}^M$ is used to obscure the difference vector $f(\\mathbf{x})-f(\\mathbf{x}')$, where $\\mathbf{x}'$ is known as a neighboring dataset. A DP guarantee is obtained by studying the tail probability bound of a privacy loss random variable (PLRV), defined as the Radon-Nikodym derivative between two distributions. When $\\mathbf{n}$ follows a multivariate Gaussian distribution, the PLRV is characterized as a specific univariate Gaussian. In this paper, we propose a novel scheme to generate $\\mathbf{n}$ by leveraging the fact that the perturbation noise is typically spherically symmetric (i.e., the distribution is rotationally invariant around the origin). The new noise generation scheme allows us to investigate the privacy loss from a geometric perspective and express the resulting PLRV using a product measure, $W\\times U$; measure $W$ is related to a radius random variable controlling the magnitude of $\\mathbf{n}$, while measure $U$ involves a directional random variable governing the angle between $\\mathbf{n}$ and the difference $f(\\mathbf{x})-f(\\mathbf{x}')$. We derive a closed-form moment bound on the product measure to prove $(\u03b5,\u03b4)$-DP. Under the same $(\u03b5,\u03b4)$-DP guarantee, our mechanism yields a smaller expected noise magnitude than the classic Gaussian noise in high dimensions, thereby significantly improving the utility of the noisy result $f(\\mathbf{x})+\\mathbf{n}$. To validate this, we consider convex and non-convex empirical risk minimization (ERM) problems in high dimensional space and apply the proposed product noise to achieve privacy.\nlink: https://arxiv.org/abs/2512.06253v4\n"}}
{"custom_id": "2512.19851v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: An Adaptive Distributed Stencil Abstraction for GPUs\nsummary: The scientific computing ecosystem in Python is largely confined to single-node parallelism, creating a gap between high-level prototyping in NumPy and high-performance execution on modern supercomputers. The increasing prevalence of hardware accelerators and the need for energy efficiency have made resource adaptivity a critical requirement, yet traditional HPC abstractions remain rigid. To address these challenges, we present an adaptive, distributed abstraction for stencil computations on multi-node GPUs. This abstraction is built using CharmTyles, a framework based on the adaptive Charm++ runtime, and features a familiar NumPy-like syntax to minimize the porting effort from prototype to production code. We showcase the resource elasticity of our abstraction by dynamically rescaling a running application across a different number of nodes and present a performance analysis of the associated overheads. Furthermore, we demonstrate that our abstraction achieves significant performance improvements over both a specialized, high-performance stencil DSL and a generalized NumPy replacement.\nlink: https://arxiv.org/abs/2512.19851v1\n"}}
{"custom_id": "2512.19849v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: UCCL-EP: Portable Expert-Parallel Communication\nsummary: Mixture-of-Experts (MoE) workloads rely on expert parallelism (EP) to achieve high GPU efficiency. State-of-the-art EP communication systems such as DeepEP demonstrate strong performance but exhibit poor portability across heterogeneous GPU and NIC platforms. The poor portability is rooted in architecture: GPU-initiated token-level RDMA communication requires tight vertical integration between GPUs and NICs, e.g., GPU writes to NIC driver/MMIO interfaces.\n  We present UCCL-EP, a portable EP communication system that delivers DeepEP-level performance across heterogeneous GPU and NIC hardware. UCCL-EP replaces GPU-initiated RDMA with a high-throughput GPU-CPU control channel: compact token-routing commands are transferred to multithreaded CPU proxies, which then issue GPUDirect RDMA operations on behalf of GPUs. UCCL-EP further emulates various ordering semantics required by specialized EP communication modes using RDMA immediate data, enabling correctness on NICs that lack such ordering, e.g., AWS EFA. We implement UCCL-EP on NVIDIA and AMD GPUs with EFA and Broadcom NICs. On EFA, it outperforms the best existing EP solution by up to $2.1\\times$ for dispatch and combine throughput. On NVIDIA-only platform, UCCL-EP achieves comparable performance to the original DeepEP. UCCL-EP also improves token throughput on SGLang by up to 40% on the NVIDIA+EFA platform, and improves DeepSeek-V3 training throughput over the AMD Primus/Megatron-LM framework by up to 45% on a 16-node AMD+Broadcom platform.\nlink: https://arxiv.org/abs/2512.19849v1\n"}}
{"custom_id": "2512.19842v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Holoscope: Open and Lightweight Distributed Telescope & Honeypot Platform\nsummary: The complexity and scale of Internet attacks call for distributed, cooperative observatories capable of monitoring malicious traffic across diverse networks. Holoscope is a lightweight, cloud-native platform designed to simplify the deployment and management of distributed telescope (passive) and honeypot (active) sensors, used to collect and analyse attack traffic by exposing or simulating vulnerable systems. Built upon K3s and WireGuard, Holoscope offers secure connectivity, automated node onboarding, and resilient operation even in resource-constrained environments. Through modular design and Infrastructure-as-Code principles, it supports dynamic sensor orchestration, automated recovery and processing. We build, deploy and operate Holoscope across multiple institutions and cloud networks in Europe and Brazil, enabling unified visibility into large-scale attack phenomena while maintaining ease of integration and security compliance.\nlink: https://arxiv.org/abs/2512.19842v1\n"}}
{"custom_id": "2302.06085v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Algorithmic Aspects of the Log-Laplace Transform and a Non-Euclidean Proximal Sampler\nsummary: The development of efficient sampling algorithms catering to non-Euclidean geometries has been a challenging endeavor, as discretization techniques which succeed in the Euclidean setting do not readily carry over to more general settings. We develop a non-Euclidean analog of the recent proximal sampler of [LST21], which naturally induces regularization by an object known as the log-Laplace transform (LLT) of a density. We prove new mathematical properties (with an algorithmic flavor) of the LLT, such as strong convexity-smoothness duality and an isoperimetric inequality, which are used to prove a mixing time on our proximal sampler matching [LST21] under a warm start. As our main application, we show our warm-started sampler improves the value oracle complexity of differentially private convex optimization in $\\ell_p$ and Schatten-$p$ norms for $p \\in [1, 2]$ to match the Euclidean setting [GLL22], while retaining state-of-the-art excess risk bounds [GLLST23]. We find our investigation of the LLT to be a promising proof-of-concept of its utility as a tool for designing samplers, and outline directions for future exploration.\nlink: https://arxiv.org/abs/2302.06085v3\n"}}
{"custom_id": "2512.19838v1", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Equilibrium Liquidity and Risk Offsetting in Decentralised Markets\nsummary: We develop an economic model of decentralised exchanges (DEXs) in which risk-averse liquidity providers (LPs) manage risk in a centralised exchange (CEX) based on preferences, information, and trading costs. Rational, risk-averse LPs anticipate the frictions associated with replication and manage risk primarily by reducing the reserves supplied to the DEX. Greater aversion reduces the equilibrium viability of liquidity provision, resulting in thinner markets and lower trading volumes. Greater uninformed demand supports deeper liquidity, whereas higher fundamental price volatility erodes it. Finally, while moderate anticipated price changes can improve LP performance, larger changes require more intensive trading in the CEX, generate higher replication costs, and induce LPs to reduce liquidity supply.\nlink: https://arxiv.org/abs/2512.19838v1\n"}}
{"custom_id": "2410.10110v3", "method": "POST", "url": "/v1/responses", "body": {"model": "gpt-5-mini", "input": "You are a specialist research analyst in Web3, DeFi, cryptography, and distributed systems.\n\nYour task:\nFrom the paper below (Title + Summary + Link), evaluate how useful each paper is for understanding investment opportunities or technological advantages within the Web3 / crypto / DeFi ecosystem.\nRate each paper on a five-level scale: Highest / High / Medium / Low / Lowest.\n\n\"Useful\" means:\n\n- Introduces or significantly improves cryptographic primitives with clear relevance to blockchain security, performance, or trust minimization.\n- Advances scalability or robustness of decentralized systems, including consensus mechanisms, P2P networking, data availability, or fault tolerance under adversarial or permissionless settings.\n- Contributes to zero-knowledge proofs, MPC, or post-quantum cryptography in ways that are plausibly integrable into future L1/L2 or cross-chain architectures.\n- Provides novel or practically relevant models of DeFi systems, including AMMs, MEV, liquidation dynamics, risk modeling, or liquidity incentives.\n- Improves understanding of smart contract security, economic attack vectors, incentive misalignment, or protocol-level exploits.\n- Advances token economics or mechanism design specifically tailored to decentralized, trust-minimized systems.\n- Analyzes or mitigates quantum-era threats to blockchain or cryptographic assumptions.\n- Represents a non-trivial or step-change improvement (not merely incremental optimization) that could materially enhance decentralization, scalability, privacy, security, or composability.\n- Demonstrates potential for real-world protocol adoption, implementation, or influence on future blockchain designs, rather than being purely theoretical with no clear Web3 applicability.\n\nOutput instructions:\n\n- Output each JSON object in JSON Lines format (one line per paper).\n\n{\n    \"title\": title of the paper (in Japanese),\n    \"summary\": A concise summary of the paper (in Japanese),\n    \"impact_level\": \"Highest|High|Medium|Low|Lowest\",\n    \"why_matters\": An array (JSON array) containing 2\u20134 bullet points explaining why this paper matters for Web3 investment\n}\n\n- Do not output anything except valid JSON object in JSON Lines.\n- \"title\", \"summary\", \"why_matters\" text in the output must be written in Japanese.\n\nPaper:\ntitle: Mastering AI: Big Data, Deep Learning, and the Evolution of Large Language Models -- Blockchain and Applications\nsummary: A detailed exploration of blockchain technology and its applications across various fields is provided, beginning with an introduction to cryptography fundamentals, including symmetric and asymmetric encryption, and their roles in ensuring security and trust within blockchain systems. The structure and mechanics of Bitcoin and Ethereum are then examined, covering topics such as proof-of-work, proof-of-stake, and smart contracts. Practical applications of blockchain in industries like decentralized finance (DeFi), supply chain management, and identity authentication are highlighted. The discussion also extends to consensus mechanisms and scalability challenges in blockchain, offering insights into emerging technologies like Layer 2 solutions and cross-chain interoperability. The current state of academic research on blockchain and its potential future developments are also addressed.\nlink: https://arxiv.org/abs/2410.10110v3\n"}}
